{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-24T16:38:19.938637Z",
     "start_time": "2025-05-24T16:38:18.522998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "from data.q_and_a.train_and_eval import TrainAndEval\n",
    "from data.q_and_a.eval_with_answers import EvalWithAnswers\n",
    "\n",
    "from models_.building.llama_tokenizer import  load_tokenizer\n",
    "\n",
    "from data.pubmed.from_json import FromJsonDataset\n",
    "from data.pubmed.contents import ContentsDataset\n",
    "\n",
    "from storage.faiss_ import FaissStorage\n",
    "\n",
    "from rag.tokenization.llama import build_tokenizer_function\n",
    "from rag.quering import build_querier\n",
    "\n",
    "from q_and_a.forward import build_enhanced_forwarder\n",
    "from q_and_a.prompts import prompt\n",
    "from q_and_a.picking.from_logits import build_from_logits\n",
    "from q_and_a.eval import evaluate"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:38:19.977516Z",
     "start_time": "2025-05-24T16:38:19.943452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ],
   "id": "e0666cbe58586ddf",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Loading data: augmentation and question and answer",
   "id": "655073c16ef21725"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:38:21.264278Z",
     "start_time": "2025-05-24T16:38:20.024507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train = TrainAndEval(\"../../data/pubmed_QA_train.json\")\n",
    "evaluationData = TrainAndEval(\"../../data/pubmed_QA_eval.json\")\n",
    "evaluateWithAnswers = EvalWithAnswers(evaluationData)\n",
    "\n",
    "augmented_data = FromJsonDataset(json_file=\"../../data/pubmed_500K.json\")\n",
    "augmented_data = ContentsDataset(augmented_data)"
   ],
   "id": "77a379766b65d5af",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:38:21.271987Z",
     "start_time": "2025-05-24T16:38:21.270144Z"
    }
   },
   "cell_type": "code",
   "source": "print(evaluateWithAnswers[0])",
   "id": "36a02355f4f6679f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'pubmed23n0012_5208', 'excerpt': 'Temporal changes in medial basal hypothalamic LH-RH correlated with plasma LH during the rat estrous cycle and following electrochemical stimulation of the medial preoptic area in pentobarbital-treated proestrous rats. In the present studies we have simultaneously measured changes in medial basal hypothalamic (MBH) leutenizing hormone-releasing hormone (LH-RH) and in plasma LH by radioimmunoassay in female rats at various hours during the 4-day estrous cycle and under experimental conditions known to alter pituitary LH secretion. In groups of rats decapitated at 12.00 h and 15.00 h on estrus and diestrus, plasma LH remained at basal levels (5-8 ng/ml) and MBH-LH-RH concentrations showed average steady state concentrations of 2231 +/- 205 pg/mg. On the day of proestrus hourly measurements of MBH-LH-RH between 12.00 h and 21.00 h suggested rhythmic rises and falls in the decapeptide concomitant with rises and falls in plasma LH. In a second group of pentobarbital-anesthetized proestrous rats a significant decline in MBH-LH-RH occurred (to 573 +/- 137 pg/mg) which then remained at low concentrations between 14.00 h and 18.00 h proestrus. Following bilateral preoptic area (MPOA) electrochemical stimulation of pentobarbital-treated proestrous rats, LH was significantly increased by 30 min, peaked between 90-120 min and returned to basal levels by 210 min poststimulation. In the same animals within 15 min poststimulation, MBH-LH-RH increased from the basal concentrations noted after pentobarbital anesthesia to elevated levels comparable to those observed throughout estrus, diestrus and on proestrous morning. Further, as plasma LH rose to peak concentrations and declined to basal plasma values, rhythmic rises and falls in MBH-LH-RH were observed with intervals between pulses of approximately 60 min. Seemingly, hypothalamic LH-RH is released as pulsatile pulses from a releasable pool; this pool is replenished and again LH-RH is discharged in response to constant stimulation by the preoptic brain.', 'statement': 'Temporal fluctuations in medial basal hypothalamic LH-RH levels correlate with plasma LH changes during the rat estrous cycle and following stimulation of the medial preoptic area.', 'question': 'What relationship exists between medial basal hypothalamic LH-RH levels and plasma LH changes during the rat estrous cycle and after stimulation of the medial preoptic area?', 'distractors': ['Medial basal hypothalamic LH-RH levels remain constant throughout the rat estrous cycle and do not affect plasma LH changes.', 'Temporal fluctuations in medial basal hypothalamic LH-RH levels correlate with plasma LH changes during the rat estrous cycle and following stimulation of the medial preoptic area.', 'There is no relationship between medial basal hypothalamic LH-RH levels and plasma LH changes during the rat estrous cycle.', 'Increased medial basal hypothalamic LH-RH levels lead to a decrease in plasma LH during the rat estrous cycle.'], 'options': ['Medial basal hypothalamic LH-RH levels remain constant throughout the rat estrous cycle and do not affect plasma LH changes.', 'Temporal fluctuations in medial basal hypothalamic LH-RH levels correlate with plasma LH changes during the rat estrous cycle and following stimulation of the medial preoptic area.', 'There is no relationship between medial basal hypothalamic LH-RH levels and plasma LH changes during the rat estrous cycle.', 'Increased medial basal hypothalamic LH-RH levels lead to a decrease in plasma LH during the rat estrous cycle.'], 'answer_idx': 1}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Building the RAG system",
   "id": "4d765b5efcca19a3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:38:27.964973Z",
     "start_time": "2025-05-24T16:38:21.314766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "storage = FaissStorage(\n",
    "    dimension=800,\n",
    ")\n",
    "\n",
    "storage.load(\"../../outputs/store/pubmed_500K.index\")\n",
    "\n",
    "tokenizer = load_tokenizer()\n",
    "tokenizer_fn = build_tokenizer_function(tokenizer)\n",
    "\n",
    "querier = build_querier(storage, augmented_data, tokenizer_fn)"
   ],
   "id": "147eaaa63c7ef980",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Building question and answer system",
   "id": "6f97c28a829420fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:38:35.869497Z",
     "start_time": "2025-05-24T16:38:27.979939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "# Load the PEFT config to find the base model\n",
    "peft_model_path = \"../train/trainer-combined-logits/trainer\"\n",
    "peft_config = PeftConfig.from_pretrained(peft_model_path)\n",
    "\n",
    "# Load the base model (this must match the model used during fine-tuning)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path)\n",
    "\n",
    "# Load the PEFT model\n",
    "model = PeftModel.from_pretrained(base_model, \"../train/trainer-combined-logits/trainer\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../train/trainer-combined-logits/tokenizer\")"
   ],
   "id": "f0c010ff3d2fca2c",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:38:35.906111Z",
     "start_time": "2025-05-24T16:38:35.903530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "question = train[0][\"question\"]\n",
    "question"
   ],
   "id": "3f67d270e97241c2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What did the study reveal about the role of external calcium concentration in the action potential and contraction recovery time of crayfish muscle fibers?'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:38:35.934809Z",
     "start_time": "2025-05-24T16:38:35.932115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "answer = train[0][\"statement\"]\n",
    "answer"
   ],
   "id": "982905dc87d0f4c4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The study investigated how changes in external calcium concentration affect the action potential and contraction recovery time in crayfish muscle fibers, revealing that calcium entry through TTS membranes is crucial for excitation-contraction coupling.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:38:35.983103Z",
     "start_time": "2025-05-24T16:38:35.980053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "options = train[0][\"distractors\"]\n",
    "# append the answer to the options\n",
    "options.append(answer)\n",
    "options"
   ],
   "id": "5330e63da28421ba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The study found that increasing external calcium concentration had no effect on the action potential or contraction recovery time in crayfish muscle fibers.',\n",
       " 'The research concluded that magnesium ions play a more significant role than calcium in the action potential and contraction recovery of crayfish muscle fibers.',\n",
       " 'The investigation revealed that external calcium concentration only affects the resting potential of crayfish muscle fibers, not the action potential or contraction recovery.',\n",
       " 'The study investigated how changes in external calcium concentration affect the action potential and contraction recovery time in crayfish muscle fibers, revealing that calcium entry through TTS membranes is crucial for excitation-contraction coupling.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:38:36.095850Z",
     "start_time": "2025-05-24T16:38:36.027648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "augmented_data = querier(question, 5)\n",
    "augmented_data"
   ],
   "id": "d713c963695a4145",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([171539365888.0,\n",
       "  224323829760.0,\n",
       "  226616164352.0,\n",
       "  232985919488.0,\n",
       "  234069327872.0],\n",
       " [\"3,3'-Diiodothyronine production, a major pathway of peripheral iodothyronine metabolism in man. 3,3'-Diiodothyronine (3,3'-T(2)) has been detected in human serum and in thyroglobulin. However, no quantitative assessment of its clearance rate (CR), production rate (PR), or of the importance of extrathyroidal sources of 3,3'-T(2) relative to direct thyroidal secretion is yet available. This study examines these parameters in seven euthyroid subjects, and in eight athyreotic subjects (H) eumetabolic due to thyroxine therapy (HT(4)) (n = 5) or triiodothyronine replacement (HT(3)) (n = 3). A highly specific radioimmunoassay for the measurement of 3,3'-T(2) in whole serum was developed. Serum 3,3'-T(2) concentrations were (mean +/- SD) 6.0+/-1.0 ng/100 ml in 13 normal subjects, 9.0+/-4.6 ng/100 ml in 25 hyperthyroid patients, and 2.7+/-1.1 ng/100 ml in 17 hypothyroid patients. The values in each of the latter two groups were significantly different from normal. 3,3'-T(2) was detected regularly in normal concentrations in 11 hypothyroid patients eumetabolic by treatment with synthetic T(4), in 10 eumetabolic patients suffering from nonthyroidal systemic illness, and in 2 subjects with elevated serum T(4)-binding globulin. The 3,3'-T(2) CR was assessed from data acquired from the (125)I-3,3'-T(2) constant infusion technique. The 3,3'-T(2) PR was calculated from CR and serum concentration of 3,3'-T(2) determined by radio-immunoassay. In the HT(4) subjects the 3,3'-T(2) CR averaged 840+/-377 liters/day and 3,3'-T(2) PR 33.9+/-12.5 mug/day. These results were not significantly different from those in the control group: 3,3'-T(2) CR 628+/-218 liters/day and 3,3'-T(2) PR 39.8+/-19.8 mug/day (all corrected to 70 kg body wt). In addition to 3,3'-T(2) PR, T(3), and reverse triiodothyronine (rT(3)) PR were determined in three of the HT(4) subjects. In each case studied, the 3,3'-T(2) PR was close to the combined triiodothyronine (T(3) + rT(3)) PR. The mean molar ratio of T(2) PR/(T(3) + rT(3)) PR was 1.08+/-0.10. The results obtained in the HT(4) subjects indicate that the production of 3,3'-T(2) is a major route of T(4) metabolism. The combined studies of 3,3'-T(2), T(3) and rT(3) PR in the HT(4) subjects indicate that both T(3) and rT(3) are major precursors of 3,3'-T(2). In the HT(3) subjects, the conversion of T(3) to 3,3'-T(2), determined as the molar ratio of 3,3'-T(2) PR to T(3) PR, ranged from 0.36 to 0.92, providing further evidence that T(3) is a precursor of 3,3'-T(2). From the close agreement between the mean values for 3,3'-T(2) PR in the euthyroid and HT(4) group it is concluded that most, if not all of the 3,3'-T(2) produced in normal humans is derived by extrathyroidal conversion from T(3) and rT(3).\",\n",
       "  \"Physiological and pharmacological influences on thyroxine to 3,5,3'-triiodothyronine conversion and nuclear 3,5,3'-triiodothyronine binding in rat anterior pituitary. Our recent in vivo studies have suggested that intrapituitary l-thyroxine (T(4)) to 3,5,3'-triiodo-l-thyronine (T(3)) conversion with subsequent nuclear binding of T(3) is an important pathway by which circulating T(4) can inhibit thyrotropin release. The present studies were performed to evaluate various physiological and pharmacological influences on these two processes in rat anterior pituitary tissue. Intact pituitary fragments were incubated in buffer-1% bovine serum albumin containing 0.14 ng/ml [(131)I]T(3) and 3.8 ng/ml [(125)I]T(4). Nuclei were isolated after 3 h of incubation and the bound iodothyronines identified by paper chromatography. There was 0.3-1% [(125)I]T(3) contaminating the medium [(125)I]T(4), and this did not change during incubation. Nuclear [(125)I]T(4) was not decreased by 650-fold excesses of medium T(3) or T(4), suggesting that it was nonspecifically bound. The ratio of nuclear to medium [(131)I]- and [(125)I]T(3) were expressed as nuclear counts per minute per milligram wet weight of tissue:counts per minute per microliter medium. Intrapituitary T(4) to T(3) conversion was evidenced by the fact that the nuclear:medium (N:M) ratio for [(131)I]T(3) was 0.45+/-0.21, whereas that for [(125)I]T(3) was 2.23+/-1.28 (mean+/-SD, n = 51). A ratio (R), the N:M [(125)I]T(3) divided by the N:M [(131)I]T(3), was used as an index of intrapituitary T(4) to T(3) conversion. Increasing medium T(3) concentrations up to 50 ng/ml caused a progressive decrease in the N:M ratio for both T(3) isotopes, but no change in the value for R, indicating that both competed for the same limited-capacity nuclear receptors. Increasing concentrations of medium T(4) caused no change in the N:M [(131)I]T(3) but did cause a significant decrease in R in three of four experiments. These results suggest saturation of T(4)-5'-monodeiodination occurred at lower T(4) concentrations than saturation of nuclear T(3) binding sites. In hypothyroid rats, the N:M ratios for both [(131)I]T(3) and [(125)I]T(3) were increased (P &lt; 0.005), but R was three-fold higher than in controls (P &lt; 0.005). Animals given 10 mug T(4)/100 g body wt per d for 5 d had significantly decreased N:M ratios for both [(131)I]T(3) and [(125)I]T(3), as well as a decreased value for R. In fasted rats, neither N:M ratio was depressed, although hepatic T(4) to T(3) conversion in the same animals was 50% of control (P &lt; 0.005). Iopanoic acid (13 muM), but not 6-n-propylthiouracil (29 muM), decreased the N:M [(125)I]T(3) with a significant decrease in the value for R (P &lt; 0.025 or less). Neither sodium iodide (6 muM) nor thyrotropin-releasing hormone (7-700 nM) affected the T(3) N:M ratios. These results indicate that intrapituitary T(4) to T(3) conversion is stimulated in hypothyroidism and depressed in T(4)-treated animals, whereas opposite changes occur in hepatic T(4)-5'-monodeiodination. Unlike liver, anterior pituitary T(4)-5'-monodeiodination is not affected by fasting or incubation with 6-n-propyl-2-thiouracil, but T(4) to T(3) conversion is inhibited in both by iopanoic acid. These results indicate that there are important differences between anterior pituitary and other tissues in the regulation of T(4)-5'-monodeiodination.\",\n",
       "  'Metabolism of resorcinylic compounds by bacteria. Purification and properties of orcinol hydroxylase from Pseudomonas putida 01. Orcinol hydroxylase (EC 1.14.13.6), which catalyzes the first reaction of orcinol catabolism in Pseudomonas putida 01, has been purified to homogeneity, and crystallized. Orcinol hydroxylase catalyzes the hydroxylation of orcinol with equimolar consumption of O2 and NADH (or NADPH) to 2, 3, 5-trihydroxytoluene, which is nonenzymically oxidized to a quinone. The visible absorption spectrum of the enzyme shows maxima at 373 and 454 nm and a shoulder at 480 nm. FAD can be dissociated from the protein. Reconstitution of enzymic activity was achieved with FAD, and to a limited extent by FMN. The enzyme has a molecular weight of 63,000 to 68,000 and contains 1 mol of FAD per mol of protein. K-m values for the three substrates orcinol, NADH, and O2 are 0.03, 0.13, and 0.07mM, RESPECTIVELY. The molecular activity of the crystalline enzyme is 1560 min minus 1. In the absence of orcinol, NADH is only slowly oxidized with formation of H2O2. Several analogs of orcinol also serve as substrates for hydroxylation, namely resorcinol, 4-methylresorcinol, and 4-bromoresorcinol. Other analogs, m-cresol, m-ethylphenol, 4-ethylresorcinol, and phloroglucinol, mimic orcinol as effectors, in that they (a) accelerate electron flow from NADH to the flavin and (b) decrease the apparent K-m for NADH but not to the same extent as the substrates that are hydroxylated. The latter compounds are not hydroxylated. Instead H2O2 accumulates as the only product of O2 reduction. The enzyme therefore behaves either as a hydroxylase or an oxidase. The ratio of hydroxylase to oxidase activities of the enzyme is decreased by an increase in the temperature of incubation; at 60 degrees the reaction with orcinol is almost 50% uncoupled from hydroxylation. The apparent K-m values for the effectors are in good agreement with the D-D values obtained for orcinol, resorcinol, and m-cresol. K-D values were obtained by measurement of the effector-induced perturbations of the visible absorption spectrum of the flavoprotein by difference absorption spectroscopy. The circular dichroism spectrum of orcinol hydroxylase is also altered in the presence of orcinol. The participation of the flavin in the over-all reaction is demonstrated by its rapid reduction under anaerobic conditions by NADH in the presence or orcinol, resorcinol, or m-cresol. Subsequent introduction of oxygen restores the oxidized form and yields H2O2 when m-cresol is the effector, but not when orcinol is the effector. Transfer of reducing equivalents from the reduced flavoprotein to free FAD may also occur. Reduction of orcinol hydroxylase by NADH in the absence of an effector is 10-4-fold slower than in the presence of an effector. The minimal structural requirements for effectors appear to be a 1,3-dihydroxy or 1-alkyl-3-hydorxybenzene, but only the former are substrates for hydroxylation.',\n",
       "  \"[Production and absorption rate of cerebrospinal fluid in the spinal subarachnoid space of the dog (author's transl)]. Adult mongrel dogs, weighing 10-17 kg. were anesthetized with Nembutal and cervical and lumbosacral laminectomy was performed. The spinal subarachnoid space was blocked by extradural ligation at the level of the C4 to interrupt CSF communication between the cranial and spinal space. Polyethylene catheters were placed in the cervical and lumbosacral subarachnoid space, and artificial Mock CSF buffer, pH 7.35-7.40, containing inulin of 25 mg/dl or 14C-inulin of 1.5-2 muCi/dl as a tracer was perfused in the sacro-cervical direction through the catheter. After a steady state of perfusion was acommplished, the CSF was collected from the outlet catheter. Production and absorption rate of the CSF were calculated after Pappenheimer and Heisey's equation. 1) Effects of CSF pressure on the rate of production (Vf) and absorption (Va) of CSF and on the difference between outflow fluid rate (Vo) and inflow fluid rate (Vi) were studied within the pressure range of -100 to +600 mmH2O. Then, regression lines were calculated by means of the least square method. See Article. Vf was little affected by changes in CSF pressure, while Va increased linearly as CSF pressure elevated. This suggests that the spinal subarachnoid space plays an important role as a site of CSF absorption when the intracranial pressure increases. Vo-Vi, that is difference between absorption and production rate, decreased linearly as the CSF pressure increased. 2) Under a constant CSF pressure of +200 mmH2O, the effects of glucocorticoids (dexamethasone, 0.25 mg/kg and hydrocortisone, 4.15 mg/kg) and a carbonic anhydrase inhibitor (acetazolamide, 10 mg/kg) upon the production and absorption rate of CSF were determined 1/2, 1, 2 and 3 hours after intravenous administration. a) Effect of dexamethasone: The rate of CSF production was reduced to 60.5 +/- 2.4% (p less than 0.001) of the control level. The absorption rate of CSF also decreased to 59.2 +/- 6.09% (p less than 0.001) of the control. b) Effect of hydrocortisone: The production rate of CSF decreased to 67.4 +/- 6.61% (p less than 0.001), and the absorption rate to 76.5 +/- 3.94% (p less than 0.001) of the control level. c) Effect of acetazolamide: The production and absorption rate also decreased to 57.2 +/- 5.61% (p less than 0.001) and to 56.9 +/- 7.02% (p less than 0.001), respectively. 3) Pentration of tritiated dexamethasone and tritiated hydrocortisone from plasma to CSF. The penetration of tritiated dexamethasone and tritiated hydrocortisone from plasma to CSF in the spinal and cranial subarachnoid space was observed after the intravenous administration. The CSF/Plasma ratio of dexamethasone was 30.9% at 15 minutes and gradually increased to 91.5% and 93.5%, respectively, in the cranial and spinal CSF at 3 hours after the injection.\",\n",
       "  \"The role of sulfhydryl groups on the impaired hepatic 3',3,5-triiodothyronine generation from thyroxine in the hypothyroid, starved, fetal, and neonatal rodent. The role of nonprotein sulfhydryl groups (NPSH) in the decreased in vitro hepatic 3',3,5-triiodothyronine (T(3)) generation from thyroxine (T(4)) in the starved, hypothyroid, fetal and 1- to 4-d-old neonatal rat and dwarf mouse was assessed. NPSH were measured in fresh 25% liver homogenates prepared in 0.1 M PO(4)/10 mM EDTA buffer. As compared with values in adult male rats, NPSH concentration was decreased in the 2-d-starved (1.1+/-0.04 (mean+/-SE) vs. 2.2+/-0.15 mmol/250 g wet liver weight, P &lt; 0.001), fetal (1.0+/-0.04 vs. 3.2+/-0.08, P &lt; 0.001), 1-d-old neonatal (1.1+/-0.03 vs. 2.1+/-0.04, P &lt; 0.001), and hypothyroid (thyroidectomized 60 d) (1.4+/-0.06 vs. 2.2+/-0.15 P &lt; 0.001) rat. NPSH were also decreased in the hypothyroid, hypopituitary dwarf mouse as compared with values in their normal litter mates (1.3+/-0.03 vs. 2.0+/-0.2, P &lt; 0.01). Chronic administration of T(3) (0.5 mug/100 g body wt per d) markedly increased hepatic T(3) generation from T(4) in the thyroidectomized rat and in the dwarf mouse to values similar to those observed in the normal rodent without affecting NPSH concentration. In contrast, T(3) administration to the starved rat did not alter either hepatic T(3) generation from T(4) or NPSH. Reduced glutathione concentration was also markedly decreased in the starved rat (fed; 1.05+/-0.075 mmol/250 g wet tissue vs. starved 0.38+/-0.02, P &lt; 0.001). Dithiothreitol (DTT), a thiol reducing agent, increased hepatic T(3) generation from T(4) in the normal adult male rat by 45+/-5% in six experiments. When compared to DTT-stimulated control homogenates, the addition of DTT completely restored hepatic T(3) generation in starved rats, partially restored T(3) generation in 1- and 4-d-old neonates, but had little or no effect in the fetal and hypothyroid rat and dwarf mouse. Liver homogenates stored for 6 mo at -20 degrees C lost their capacity to generate T(3) from T(4). NPSH concentrations in the frozen homogenates decreased progressively with increasing storage and were absent by 6 mo. 5'-Deiodinase activity correlated with NPSH concentration in the stored homogenates (r = 0.95, P &lt; 0.005). Addition of DTT partially restored hepatic T(3) generation in the frozen homogenate. It is concluded that NPSH are important for the action of the liver 5'-deiodinase. The decreased hepatic T(3) generation in the starved rat is associated with decreased NPSH but not with a decrease in the absolute quantity of 5'-deiodinase because provision of sulfhydryl groups restored hepatic T(3) generation to normal. In contrast, the decreased hepatic T(3) generation in the adult hypothyroid rodent and in the fetal rat is probably due to a decrease in the enzyme concentration per se. In the 1- and 4-d neonatal rat, the decrease in hepatic T(3) generation is secondary to a decrease in NPSH and the deiodinating enzyme.\"])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:38:36.116959Z",
     "start_time": "2025-05-24T16:38:36.111374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "\n",
    "from typing import Callable, List, Tuple\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "ForwardType = Callable[[str, List[str]], Any]\n",
    "\n",
    "def enhanced_forward(\n",
    "        llm,\n",
    "        tokenizer,\n",
    "        augmenter: Callable[[str, int], Tuple[List[int], List[str]]],\n",
    "        k_augmentations: int,\n",
    "        prompt_builder: Callable[[str, List[str], List[str]], str],\n",
    "        question: str,\n",
    "        options: List[str],\n",
    "        device: str,\n",
    "        num_iterations: int = 10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs multiple forward passes, appending the most probable token each time,\n",
    "    and returns average probabilities across all tokens.\n",
    "\n",
    "    Args:\n",
    "        llm: The language model to use for generating the response.\n",
    "        tokenizer: The tokenizer associated with the language model.\n",
    "        augmenter (Callable): A function that takes a query string and returns the first k_augmentations in a tuple of\n",
    "            distances and items.\n",
    "        k_augmentations (int): The number of augmentations to generate.\n",
    "        prompt_builder (Callable): A function that builds the prompt for the language model.\n",
    "            It takes the augmented information, question and options, and returns a formatted string.\n",
    "        question (str): The question to ask.\n",
    "        options (List[str]): The list of options to choose from.\n",
    "        device (str): The device to run the model on ('cpu' or 'cuda').\n",
    "        num_iterations (int): Number of forward passes to perform (default: 3).\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - List of generated tokens\n",
    "            - Average probabilities tensor across all tokens\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # Get augmented items\n",
    "    _, items = augmenter(question, k_augmentations)\n",
    "\n",
    "    # Generate the initial prompt\n",
    "    prompt = prompt_builder(question, options, items)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "\n",
    "    generated_token_ids =  input_ids.clone()\n",
    "    probss = []\n",
    "    generated_tokens = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_iterations):\n",
    "            # Prepare model inputs\n",
    "            # For most autoregressive models, only input_ids are strictly necessary for the forward pass\n",
    "            # if attention_mask is not explicitly handled or modified in the loop.\n",
    "            # However, it's good practice to pass it if the model uses it.\n",
    "            # As we append tokens, the attention_mask also needs to be extended.\n",
    "            #attention_mask = torch.ones_like(generated_token_ids).to(device)\n",
    "\n",
    "            # Get model outputs\n",
    "            with torch.no_grad():\n",
    "                outputs = model(generated_token_ids)\n",
    "\n",
    "            # Get the logits for the last token in the sequence\n",
    "            # outputs.logits is typically of shape (batch_size, sequence_length, vocab_size)\n",
    "            next_token_logits = outputs.logits[:, -1, :] # Get logits for the very last token\n",
    "\n",
    "            # Apply softmax to get probabilities (optional, as argmax works on logits directly)\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            probss.append(probs)\n",
    "            # Get the predicted token ID (greedy decoding)\n",
    "            next_token_id = torch.argmax(probs, dim=-1).unsqueeze(-1)\n",
    "\n",
    "            # Append the predicted token ID to the generated sequence\n",
    "            generated_token_ids = torch.cat([generated_token_ids, next_token_id], dim=1)\n",
    "\n",
    "            # Check if the generated token is an end-of-sequence (EOS) token\n",
    "            if tokenizer.eos_token_id is not None and next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "            # Append the generated token ID to the list\n",
    "            generated_tokens.append(next_token_id.item())\n",
    "\n",
    "    # Concatenate all probabilities\n",
    "    probs = torch.cat(probss, dim=0)\n",
    "    # Average the probabilities across all iterations\n",
    "    avg_probs = torch.mean(probs, dim=0)\n",
    "\n",
    "    return generated_tokens, avg_probs\n",
    "\n",
    "def build_enhanced_forwarder(\n",
    "    llm,\n",
    "    tokenizer,\n",
    "    augmenter: Callable[[str, int], Tuple[List[int], List[str]]],\n",
    "    k_augmentations: int,\n",
    "    prompt_builder: Callable[[str, List[str], List[str]], str],\n",
    "    num_iterations: int,\n",
    "    device: str,\n",
    ") -> Callable[[str, List[str]], Tuple[List[int], torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Builds an enhanced forward function that can be used to generate responses from the language model.\n",
    "\n",
    "    Returns:\n",
    "        Callable: A function that takes a question and a list of options and returns the generated response.\n",
    "    \"\"\"\n",
    "    def forward_fn(question: str, options: List[str]) -> Tuple[List[int], torch.Tensor]:\n",
    "        return enhanced_forward(\n",
    "            llm=llm,\n",
    "            tokenizer=tokenizer,\n",
    "            augmenter=augmenter,\n",
    "            k_augmentations=k_augmentations,\n",
    "            prompt_builder=prompt_builder,\n",
    "            question=question,\n",
    "            options=options,\n",
    "            num_iterations=num_iterations,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    return forward_fn"
   ],
   "id": "aef014196c8df6f4",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:38:36.154672Z",
     "start_time": "2025-05-24T16:38:36.152267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prompt(\n",
    "    question: str,\n",
    "    options: List[str],\n",
    "    augmented_items: List[str],\n",
    ") -> str:\n",
    "    context = \"\\n\".join(augmented_items)\n",
    "\n",
    "    options_str = \"\\n\".join(\n",
    "        [f\"{chr(65 + i)}. {option}\" for i, option in enumerate(options)]\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    Generates a prompt for the language model based on the question, options, and augmented items.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to ask.\n",
    "        options (List[str]): The list of options to choose from.\n",
    "        augmented_items (List[str]): The augmented items to include in the prompt.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted prompt string.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are an expert in multiple-choice questions. Your task is to select the best answer from the given options based on the provided context.\n",
    "Context :  {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Options:\n",
    "{options_str}\n",
    "\n",
    "Between A, B, C and D the best option is\"\"\"\n",
    "    return prompt"
   ],
   "id": "193b2a6da63886ec",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:38:36.704420Z",
     "start_time": "2025-05-24T16:38:36.197946Z"
    }
   },
   "cell_type": "code",
   "source": "base_model.to(device)",
   "id": "251cc6b0f6e4e74",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): lora.Linear(\n",
       "            (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
       "            (lora_dropout): ModuleDict(\n",
       "              (default): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (lora_A): ModuleDict(\n",
       "              (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "            )\n",
       "            (lora_B): ModuleDict(\n",
       "              (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "            )\n",
       "            (lora_embedding_A): ParameterDict()\n",
       "            (lora_embedding_B): ParameterDict()\n",
       "            (lora_magnitude_vector): ModuleDict()\n",
       "          )\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:38:38.437613Z",
     "start_time": "2025-05-24T16:38:36.737566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "forward = build_enhanced_forwarder(\n",
    "    base_model,\n",
    "    tokenizer,\n",
    "    querier,\n",
    "    k_augmentations=1,\n",
    "    prompt_builder=prompt,\n",
    "    num_iterations=10,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "def forward_and_get_last_logit(\n",
    "    question,\n",
    "    options,\n",
    "):\n",
    "    tokens, logits =  forward(\n",
    "        question,\n",
    "        options=options,\n",
    "    )\n",
    "\n",
    "    return logits\n",
    "\n",
    "result = forward_and_get_last_logit(\n",
    "    question,\n",
    "    options=options,\n",
    ")"
   ],
   "id": "6eda71576945843",
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 540.00 MiB. GPU 0 has a total capacity of 5.65 GiB of which 200.00 MiB is free. Including non-PyTorch memory, this process has 5.43 GiB memory in use. Of the allocated memory 5.29 GiB is allocated by PyTorch, and 47.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 22\u001B[0m\n\u001B[1;32m     15\u001B[0m     tokens, logits \u001B[38;5;241m=\u001B[39m  forward(\n\u001B[1;32m     16\u001B[0m         question,\n\u001B[1;32m     17\u001B[0m         options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m     18\u001B[0m     )\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m logits\n\u001B[0;32m---> 22\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mforward_and_get_last_logit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[14], line 15\u001B[0m, in \u001B[0;36mforward_and_get_last_logit\u001B[0;34m(question, options)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward_and_get_last_logit\u001B[39m(\n\u001B[1;32m     12\u001B[0m     question,\n\u001B[1;32m     13\u001B[0m     options,\n\u001B[1;32m     14\u001B[0m ):\n\u001B[0;32m---> 15\u001B[0m     tokens, logits \u001B[38;5;241m=\u001B[39m  \u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m logits\n",
      "Cell \u001B[0;32mIn[11], line 111\u001B[0m, in \u001B[0;36mbuild_enhanced_forwarder.<locals>.forward_fn\u001B[0;34m(question, options)\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward_fn\u001B[39m(question: \u001B[38;5;28mstr\u001B[39m, options: List[\u001B[38;5;28mstr\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[List[\u001B[38;5;28mint\u001B[39m], torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m--> 111\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43menhanced_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    112\u001B[0m \u001B[43m        \u001B[49m\u001B[43mllm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mllm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    113\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    114\u001B[0m \u001B[43m        \u001B[49m\u001B[43maugmenter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maugmenter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    115\u001B[0m \u001B[43m        \u001B[49m\u001B[43mk_augmentations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mk_augmentations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    116\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprompt_builder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprompt_builder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    117\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquestion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    118\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    119\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_iterations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_iterations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    120\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    121\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[11], line 66\u001B[0m, in \u001B[0;36menhanced_forward\u001B[0;34m(llm, tokenizer, augmenter, k_augmentations, prompt_builder, question, options, device, num_iterations)\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_iterations):\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;66;03m# Prepare model inputs\u001B[39;00m\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;66;03m# For most autoregressive models, only input_ids are strictly necessary for the forward pass\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     63\u001B[0m \n\u001B[1;32m     64\u001B[0m     \u001B[38;5;66;03m# Get model outputs\u001B[39;00m\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 66\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgenerated_token_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# Get the logits for the last token in the sequence\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# outputs.logits is typically of shape (batch_size, sequence_length, vocab_size)\u001B[39;00m\n\u001B[1;32m     70\u001B[0m     next_token_logits \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mlogits[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :] \u001B[38;5;66;03m# Get logits for the very last token\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/peft/peft_model.py:1757\u001B[0m, in \u001B[0;36mPeftModelForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001B[0m\n\u001B[1;32m   1755\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_enable_peft_forward_hooks(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   1756\u001B[0m         kwargs \u001B[38;5;241m=\u001B[39m {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspecial_peft_forward_args}\n\u001B[0;32m-> 1757\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1758\u001B[0m \u001B[43m            \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1759\u001B[0m \u001B[43m            \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1760\u001B[0m \u001B[43m            \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1761\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1762\u001B[0m \u001B[43m            \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1763\u001B[0m \u001B[43m            \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1764\u001B[0m \u001B[43m            \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1765\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1766\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1768\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m _get_batch_size(input_ids, inputs_embeds)\n\u001B[1;32m   1769\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1770\u001B[0m     \u001B[38;5;66;03m# concat prompt attention mask\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/peft/tuners/tuners_utils.py:193\u001B[0m, in \u001B[0;36mBaseTuner.forward\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    192\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any):\n\u001B[0;32m--> 193\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:965\u001B[0m, in \u001B[0;36mcan_return_tuple.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    962\u001B[0m     set_attribute_for_modules(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_is_top_level_module\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    964\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 965\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    966\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_requested_to_return_tuple \u001B[38;5;129;01mor\u001B[39;00m (is_configured_to_return_tuple \u001B[38;5;129;01mand\u001B[39;00m is_top_level_module):\n\u001B[1;32m    967\u001B[0m         output \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mto_tuple()\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/deprecation.py:172\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action\u001B[38;5;241m.\u001B[39mNOTIFY, Action\u001B[38;5;241m.\u001B[39mNOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m--> 172\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:837\u001B[0m, in \u001B[0;36mLlamaForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001B[0m\n\u001B[1;32m    835\u001B[0m \u001B[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001B[39;00m\n\u001B[1;32m    836\u001B[0m slice_indices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mslice\u001B[39m(\u001B[38;5;241m-\u001B[39mlogits_to_keep, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(logits_to_keep, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m logits_to_keep\n\u001B[0;32m--> 837\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlm_head\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mslice_indices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    839\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    840\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 540.00 MiB. GPU 0 has a total capacity of 5.65 GiB of which 200.00 MiB is free. Including non-PyTorch memory, this process has 5.43 GiB memory in use. Of the allocated memory 5.29 GiB is allocated by PyTorch, and 47.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T16:38:38.498497007Z",
     "start_time": "2025-05-18T04:48:56.003253Z"
    }
   },
   "cell_type": "code",
   "source": "result",
   "id": "a3dac6ce96dff89a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.5858e-04, 1.0133e-05, 3.3714e-07,  ..., 1.3006e-10, 1.3006e-10,\n",
       "        1.3006e-10], device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T04:48:56.055589Z",
     "start_time": "2025-05-18T04:48:56.051793Z"
    }
   },
   "cell_type": "code",
   "source": "possible_answers = [\" A\", \" B\", \" C\", \" D\"]",
   "id": "c808e5e62e917869",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T04:48:56.116948Z",
     "start_time": "2025-05-18T04:48:56.105443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "picker = build_from_logits(\n",
    "    tokenizer,\n",
    "    options=possible_answers,\n",
    ")\n",
    "\n",
    "selected_option = picker(result)"
   ],
   "id": "4aa6e4444a98b39c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Options: [362, 426, 356, 423]\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T04:48:56.166377Z",
     "start_time": "2025-05-18T04:48:56.161125Z"
    }
   },
   "cell_type": "code",
   "source": "selected_option",
   "id": "e59921bab48e1e24",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Lets evaluate the model ",
   "id": "2a4db915fcf2598"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T04:51:57.640577Z",
     "start_time": "2025-05-18T04:48:56.216896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "accuracy = evaluate(\n",
    "    forward_fn=forward_and_get_last_logit,\n",
    "    picker_fn=picker,\n",
    "    eval_dataset=evaluateWithAnswers,\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ],
   "id": "a30eb71e7d3e46b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right answer: 2, picked: 2\n",
      "Accuracy at 0: 1.00\n",
      "Right answer: 3, picked: 3\n",
      "Right answer: 2, picked: 0\n",
      "Right answer: 3, picked: 3\n",
      "Right answer: 0, picked: 3\n",
      "Right answer: 3, picked: 2\n",
      "Right answer: 1, picked: 3\n",
      "Right answer: 2, picked: 2\n",
      "Right answer: 3, picked: 3\n",
      "Right answer: 3, picked: 3\n",
      "Right answer: 1, picked: 3\n",
      "Accuracy at 100: 0.48\n",
      "Right answer: 2, picked: 2\n",
      "Right answer: 2, picked: 2\n",
      "Right answer: 0, picked: 2\n",
      "Right answer: 2, picked: 2\n",
      "Right answer: 0, picked: 0\n",
      "Right answer: 1, picked: 1\n",
      "Right answer: 2, picked: 3\n",
      "Right answer: 1, picked: 3\n",
      "Right answer: 2, picked: 2\n",
      "Right answer: 2, picked: 2\n",
      "Accuracy at 200: 0.50\n",
      "Right answer: 3, picked: 0\n",
      "Right answer: 2, picked: 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[40], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforward_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforward_and_get_last_logit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpicker_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpicker\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevaluateWithAnswers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Try/pytorch_training/10_rag/src/q_and_a/eval.py:35\u001B[0m, in \u001B[0;36mevaluate\u001B[0;34m(forward_fn, picker_fn, eval_dataset, log_each)\u001B[0m\n\u001B[1;32m     32\u001B[0m answer_idx \u001B[38;5;241m=\u001B[39m item[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124manswer_idx\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# Get the model's response\u001B[39;00m\n\u001B[0;32m---> 35\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mforward_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# Pick the best option\u001B[39;00m\n\u001B[1;32m     38\u001B[0m picked_idx \u001B[38;5;241m=\u001B[39m picker_fn(response)\n",
      "Cell \u001B[0;32mIn[35], line 15\u001B[0m, in \u001B[0;36mforward_and_get_last_logit\u001B[0;34m(question, options)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward_and_get_last_logit\u001B[39m(\n\u001B[1;32m     12\u001B[0m     question,\n\u001B[1;32m     13\u001B[0m     options,\n\u001B[1;32m     14\u001B[0m ):\n\u001B[0;32m---> 15\u001B[0m     tokens, logits \u001B[38;5;241m=\u001B[39m  \u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m logits\n",
      "Cell \u001B[0;32mIn[33], line 111\u001B[0m, in \u001B[0;36mbuild_enhanced_forwarder.<locals>.forward_fn\u001B[0;34m(question, options)\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward_fn\u001B[39m(question: \u001B[38;5;28mstr\u001B[39m, options: List[\u001B[38;5;28mstr\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[List[\u001B[38;5;28mint\u001B[39m], torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m--> 111\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43menhanced_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    112\u001B[0m \u001B[43m        \u001B[49m\u001B[43mllm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mllm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    113\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    114\u001B[0m \u001B[43m        \u001B[49m\u001B[43maugmenter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maugmenter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    115\u001B[0m \u001B[43m        \u001B[49m\u001B[43mk_augmentations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mk_augmentations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    116\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprompt_builder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprompt_builder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    117\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquestion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    118\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    119\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_iterations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_iterations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    120\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    121\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[33], line 66\u001B[0m, in \u001B[0;36menhanced_forward\u001B[0;34m(llm, tokenizer, augmenter, k_augmentations, prompt_builder, question, options, device, num_iterations)\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_iterations):\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;66;03m# Prepare model inputs\u001B[39;00m\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;66;03m# For most autoregressive models, only input_ids are strictly necessary for the forward pass\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     63\u001B[0m \n\u001B[1;32m     64\u001B[0m     \u001B[38;5;66;03m# Get model outputs\u001B[39;00m\n\u001B[1;32m     65\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 66\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgenerated_token_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;66;03m# Get the logits for the last token in the sequence\u001B[39;00m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;66;03m# outputs.logits is typically of shape (batch_size, sequence_length, vocab_size)\u001B[39;00m\n\u001B[1;32m     70\u001B[0m     next_token_logits \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mlogits[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :] \u001B[38;5;66;03m# Get logits for the very last token\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/accelerate/hooks.py:175\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    173\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 175\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:965\u001B[0m, in \u001B[0;36mcan_return_tuple.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    962\u001B[0m     set_attribute_for_modules(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_is_top_level_module\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    964\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 965\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    966\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_requested_to_return_tuple \u001B[38;5;129;01mor\u001B[39;00m (is_configured_to_return_tuple \u001B[38;5;129;01mand\u001B[39;00m is_top_level_module):\n\u001B[1;32m    967\u001B[0m         output \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mto_tuple()\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/deprecation.py:172\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action\u001B[38;5;241m.\u001B[39mNOTIFY, Action\u001B[38;5;241m.\u001B[39mNOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m--> 172\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:821\u001B[0m, in \u001B[0;36mLlamaForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001B[0m\n\u001B[1;32m    816\u001B[0m output_hidden_states \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    817\u001B[0m     output_hidden_states \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39moutput_hidden_states\n\u001B[1;32m    818\u001B[0m )\n\u001B[1;32m    820\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[0;32m--> 821\u001B[0m outputs: BaseModelOutputWithPast \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    822\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    823\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    824\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    825\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    826\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    827\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    828\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    829\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    830\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    831\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    832\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    834\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mlast_hidden_state\n\u001B[1;32m    835\u001B[0m \u001B[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:965\u001B[0m, in \u001B[0;36mcan_return_tuple.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    962\u001B[0m     set_attribute_for_modules(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_is_top_level_module\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    964\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 965\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    966\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_requested_to_return_tuple \u001B[38;5;129;01mor\u001B[39;00m (is_configured_to_return_tuple \u001B[38;5;129;01mand\u001B[39;00m is_top_level_module):\n\u001B[1;32m    967\u001B[0m         output \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mto_tuple()\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:571\u001B[0m, in \u001B[0;36mLlamaModel.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001B[0m\n\u001B[1;32m    559\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m    560\u001B[0m         partial(decoder_layer\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mflash_attn_kwargs),\n\u001B[1;32m    561\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    568\u001B[0m         position_embeddings,\n\u001B[1;32m    569\u001B[0m     )\n\u001B[1;32m    570\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 571\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    572\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    573\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    574\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    575\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    576\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    577\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    578\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    579\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    580\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mflash_attn_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    581\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    583\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    585\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/accelerate/hooks.py:175\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    173\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 175\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:315\u001B[0m, in \u001B[0;36mLlamaDecoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001B[0m\n\u001B[1;32m    301\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\n\u001B[1;32m    302\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    303\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    311\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Unpack[FlashAttentionKwargs],\n\u001B[1;32m    312\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mFloatTensor, Optional[Tuple[torch\u001B[38;5;241m.\u001B[39mFloatTensor, torch\u001B[38;5;241m.\u001B[39mFloatTensor]]]:\n\u001B[1;32m    313\u001B[0m     residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[0;32m--> 315\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput_layernorm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    317\u001B[0m     \u001B[38;5;66;03m# Self Attention\u001B[39;00m\n\u001B[1;32m    318\u001B[0m     hidden_states, self_attn_weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mself_attn(\n\u001B[1;32m    319\u001B[0m         hidden_states\u001B[38;5;241m=\u001B[39mhidden_states,\n\u001B[1;32m    320\u001B[0m         attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    327\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    328\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/accelerate/hooks.py:170\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mnew_forward\u001B[39m(module, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 170\u001B[0m     args, kwargs \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_hf_hook\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpre_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    171\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mno_grad:\n\u001B[1;32m    172\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/accelerate/hooks.py:360\u001B[0m, in \u001B[0;36mAlignDevicesHook.pre_forward\u001B[0;34m(self, module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    352\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    353\u001B[0m             value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    354\u001B[0m             \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtied_params_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    355\u001B[0m             \u001B[38;5;129;01mand\u001B[39;00m value\u001B[38;5;241m.\u001B[39mdata_ptr() \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtied_params_map\n\u001B[1;32m    356\u001B[0m             \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecution_device \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtied_params_map[value\u001B[38;5;241m.\u001B[39mdata_ptr()]\n\u001B[1;32m    357\u001B[0m         ):\n\u001B[1;32m    358\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtied_pointers_to_remove\u001B[38;5;241m.\u001B[39madd((value\u001B[38;5;241m.\u001B[39mdata_ptr(), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecution_device))\n\u001B[0;32m--> 360\u001B[0m         \u001B[43mset_module_tensor_to_device\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    361\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    362\u001B[0m \u001B[43m            \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    363\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecution_device\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    364\u001B[0m \u001B[43m            \u001B[49m\u001B[43mvalue\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    365\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfp16_statistics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfp16_statistics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    366\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtied_params_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtied_params_map\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    367\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    369\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m send_to_device(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecution_device), send_to_device(\n\u001B[1;32m    370\u001B[0m     kwargs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecution_device, skip_keys\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mskip_keys\n\u001B[1;32m    371\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/accelerate/utils/modeling.py:337\u001B[0m, in \u001B[0;36mset_module_tensor_to_device\u001B[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001B[0m\n\u001B[1;32m    335\u001B[0m             module\u001B[38;5;241m.\u001B[39m_parameters[tensor_name] \u001B[38;5;241m=\u001B[39m param_cls(new_value, requires_grad\u001B[38;5;241m=\u001B[39mold_value\u001B[38;5;241m.\u001B[39mrequires_grad)\n\u001B[1;32m    336\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m--> 337\u001B[0m     new_value \u001B[38;5;241m=\u001B[39m \u001B[43mvalue\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    338\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    339\u001B[0m     new_value \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(value, device\u001B[38;5;241m=\u001B[39mdevice)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "79530456d332b0a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c6d3d1188d5b88c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "89a95d5b71902270"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e28f1c1215ec7170"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
