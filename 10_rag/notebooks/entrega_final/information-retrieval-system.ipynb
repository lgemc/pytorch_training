{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Lets join all together: retrieval system, fine-tuned model and a query pipeline.",
   "id": "15c5951b1f603960"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:22:40.627377Z",
     "start_time": "2025-05-26T02:22:35.709250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from typing import List, Tuple, Callable, Any\n",
    "import random\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel, PeftConfig"
   ],
   "id": "d56378f021711c02",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-26T02:22:40.641950Z",
     "start_time": "2025-05-26T02:22:40.632808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FaissStorage:\n",
    "    \"\"\"\n",
    "    FaissStorage is a concrete implementation of the Storage abstract base class.\n",
    "    It uses the Faiss library to store and query vectors efficiently.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dimension: int = 800,\n",
    "            index = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the FaissStorage with a specified vector dimension.\n",
    "\n",
    "        Args:\n",
    "            dimension: The dimensionality of the vectors to be stored.\n",
    "        \"\"\"\n",
    "        self.dimension = dimension\n",
    "        if index is None:\n",
    "            index = faiss.IndexFlatL2(dimension)\n",
    "        self.index = index\n",
    "\n",
    "    def store(self, key: str, data):\n",
    "        \"\"\"\n",
    "        Stores a vector associated with a given key.\n",
    "\n",
    "        Args:\n",
    "            key: A unique identifier for the vector (e.g., a string).\n",
    "            data: The vector to be stored. Must be a list of floats with length equal to 'dimension'.\n",
    "        \"\"\"\n",
    "        if len(data[0]) != self.dimension:\n",
    "            raise ValueError(f\"Data must have {self.dimension} dimensions.\")\n",
    "\n",
    "        # Convert the data to a numpy array and ensure it's in the correct format\n",
    "        data = np.array(data, dtype='float32')\n",
    "\n",
    "        self.index.add(data)  # Add the vector to the index\n",
    "\n",
    "    def query(self, key: str, k: int) -> (List, List):\n",
    "        \"\"\"\n",
    "        Retrieves the vector associated with a given key.\n",
    "\n",
    "        Args:\n",
    "            key: The identifier of the vector to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the distances and indices of the nearest vectors.\n",
    "        \"\"\"\n",
    "        # In this simple implementation, we don't actually use the key for retrieval.\n",
    "        # In a real-world scenario, you would need to maintain a mapping from keys to indices.\n",
    "\n",
    "        # For demonstration purposes, we'll return the first vector in the index\n",
    "        if self.index.ntotal == 0:\n",
    "            return None\n",
    "\n",
    "        distances, indices = self.index.search(np.array([[0] * self.dimension], dtype='float32'), k)\n",
    "\n",
    "        return distances[0].tolist(), indices[0].tolist()\n",
    "\n",
    "    def export(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Exports the Faiss index to a file.\n",
    "\n",
    "        Args:\n",
    "            file_path: The path where the index will be saved.\n",
    "        \"\"\"\n",
    "        faiss.write_index(self.index, file_path)\n",
    "\n",
    "    def load(self, file_path: str):\n",
    "        \"\"\"\n",
    "        Loads a Faiss index from a file.\n",
    "\n",
    "        Args:\n",
    "            file_path: The path from where the index will be loaded.\n",
    "        \"\"\"\n",
    "        self.index = faiss.read_index(file_path)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Load original dataset\n",
    "As faiss index is built on top of a dataset, (without store the dataset contents in the index) we need to load the original dataset to be able to query it."
   ],
   "id": "21a254f141f6b912"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:22:40.695637Z",
     "start_time": "2025-05-26T02:22:40.688055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FromJsonDataset(Dataset):\n",
    "    def __init__(self, json_file):\n",
    "        self.raw_content = \"\"\n",
    "        with open(json_file, \"r\") as f:\n",
    "            self.raw_content = f.read()\n",
    "\n",
    "        self.data = json.loads(self.raw_content)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[\"id\"])\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return  {\n",
    "            \"title\": self.data[\"title\"][idx],\n",
    "            \"content\": self.data[\"content\"][idx],\n",
    "            \"contents\": self.data[\"contents\"][idx],\n",
    "            \"PMID\": self.data[\"PMID\"][idx],\n",
    "            \"id\": self.data[\"id\"][idx],\n",
    "        }\n",
    "\n",
    "class ContentsDataset(Dataset):\n",
    "    def __init__(self, dataset: FromJsonDataset):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        item = self.dataset[idx]\n",
    "        return item[\"contents\"]"
   ],
   "id": "749ff51a1ed0132f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:22:42.225843Z",
     "start_time": "2025-05-26T02:22:40.742199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "STORAGE_FILE_PATH = \"./outputs/pubmed_500K.index\"\n",
    "storage = FaissStorage()\n",
    "storage.load(STORAGE_FILE_PATH)"
   ],
   "id": "7b877c61678644bd",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:22:45.826656Z",
     "start_time": "2025-05-26T02:22:42.239880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "augmented_data = FromJsonDataset(json_file=\"./data/pubmed_500K.json\")\n",
    "augmented_data = ContentsDataset(augmented_data)"
   ],
   "id": "6447ba8bebb8c375",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:22:45.843073Z",
     "start_time": "2025-05-26T02:22:45.835107Z"
    }
   },
   "cell_type": "code",
   "source": "augmented_data[0]  # Check the first item in the dataset",
   "id": "330d36b49616977d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[Biochemical studies on camomile components/III. In vitro studies about the antipeptic activity of (--)-alpha-bisabolol (author's transl)]. (--)-alpha-Bisabolol has a primary antipeptic action depending on dosage, which is not caused by an alteration of the pH-value. The proteolytic activity of pepsin is reduced by 50 percent through addition of bisabolol in the ratio of 1/0.5. The antipeptic action of bisabolol only occurs in case of direct contact. In case of a previous contact with the substrate, the inhibiting effect is lost.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:22:45.894451Z",
     "start_time": "2025-05-26T02:22:45.888001Z"
    }
   },
   "cell_type": "code",
   "source": "storage.index.ntotal  # Check how many vectors are stored in the index",
   "id": "8fb44c6c2b9f8b93",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "490001"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:22:45.952583Z",
     "start_time": "2025-05-26T02:22:45.943360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def build_tokenizer_function(\n",
    "        tokenizer\n",
    ") -> callable:\n",
    "    return lambda query: tokenizer(\n",
    "            query,\n",
    "            max_length=800,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"np\",\n",
    "        )[\"input_ids\"].astype(\"float32\")\n",
    "\n",
    "\n",
    "querier_type = Callable[[str, int], Tuple[List, List]]\n",
    "\n",
    "def build_querier(\n",
    "        storage: FaissStorage,\n",
    "        original_dataset: Dataset,\n",
    "        tokenizer_fn: callable,\n",
    ") -> querier_type:\n",
    "    \"\"\"\n",
    "    Build a query function that can be used to query the storage system.\n",
    "\n",
    "    Args:\n",
    "        storage (Storage): The storage system to use for querying.\n",
    "        original_dataset (Dataset): The original dataset to use for querying.\n",
    "        tokenizer_fn (callable): The function to use for tokenizing the query.\n",
    "\n",
    "    Returns:\n",
    "        callable: A function that takes a query string and returns the results.\n",
    "    \"\"\"\n",
    "    def querier(query: str, k: int = 10) -> (List, List):\n",
    "        return perform_query(storage, original_dataset, tokenizer_fn, query, k)\n",
    "\n",
    "    return querier\n",
    "\n",
    "def perform_query(\n",
    "        storage: FaissStorage,\n",
    "        original_dataset: Dataset,\n",
    "        tokenizer_fn: callable,\n",
    "        query: str,\n",
    "        k: int,\n",
    ") -> (List, List):\n",
    "    query_vector = tokenizer_fn(query)\n",
    "    distances, indices = storage.query(query_vector, k)\n",
    "\n",
    "    data = []\n",
    "    for index in indices:\n",
    "        data.append(original_dataset[index])\n",
    "\n",
    "    return distances, data\n",
    "\n"
   ],
   "id": "be41990e4aff944e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:22:46.915363Z",
     "start_time": "2025-05-26T02:22:45.999239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_tokenizer():\n",
    "    \"\"\"\n",
    "    Load a Llama model from Hugging Face Hub.\n",
    "    :param model_name: The name of the model to load.\n",
    "    :return: The loaded model.\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    model = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "    model.pad_token = model.eos_token\n",
    "    model.pad_token_id = model.eos_token_id\n",
    "\n",
    "    return model\n",
    "\n",
    "rag_tokenizer = load_tokenizer()"
   ],
   "id": "4558b56294bf9155",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:22:46.987315Z",
     "start_time": "2025-05-26T02:22:46.979542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer_fn = build_tokenizer_function(rag_tokenizer)\n",
    "\n",
    "querier = build_querier(storage, augmented_data, tokenizer_fn)"
   ],
   "id": "24f1eb651492fea7",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:22:47.207295Z",
     "start_time": "2025-05-26T02:22:47.051684Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test the querier\n",
    "\n",
    "question = \"What is the role of IL-6 in the immune response?\"\n",
    "querier(question, k=5)  # Get the top 5 results for the question"
   ],
   "id": "d4a12c22075dfc6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([171539365888.0,\n",
       "  224323829760.0,\n",
       "  226616164352.0,\n",
       "  232985919488.0,\n",
       "  234069327872.0],\n",
       " [\"3,3'-Diiodothyronine production, a major pathway of peripheral iodothyronine metabolism in man. 3,3'-Diiodothyronine (3,3'-T(2)) has been detected in human serum and in thyroglobulin. However, no quantitative assessment of its clearance rate (CR), production rate (PR), or of the importance of extrathyroidal sources of 3,3'-T(2) relative to direct thyroidal secretion is yet available. This study examines these parameters in seven euthyroid subjects, and in eight athyreotic subjects (H) eumetabolic due to thyroxine therapy (HT(4)) (n = 5) or triiodothyronine replacement (HT(3)) (n = 3). A highly specific radioimmunoassay for the measurement of 3,3'-T(2) in whole serum was developed. Serum 3,3'-T(2) concentrations were (mean +/- SD) 6.0+/-1.0 ng/100 ml in 13 normal subjects, 9.0+/-4.6 ng/100 ml in 25 hyperthyroid patients, and 2.7+/-1.1 ng/100 ml in 17 hypothyroid patients. The values in each of the latter two groups were significantly different from normal. 3,3'-T(2) was detected regularly in normal concentrations in 11 hypothyroid patients eumetabolic by treatment with synthetic T(4), in 10 eumetabolic patients suffering from nonthyroidal systemic illness, and in 2 subjects with elevated serum T(4)-binding globulin. The 3,3'-T(2) CR was assessed from data acquired from the (125)I-3,3'-T(2) constant infusion technique. The 3,3'-T(2) PR was calculated from CR and serum concentration of 3,3'-T(2) determined by radio-immunoassay. In the HT(4) subjects the 3,3'-T(2) CR averaged 840+/-377 liters/day and 3,3'-T(2) PR 33.9+/-12.5 mug/day. These results were not significantly different from those in the control group: 3,3'-T(2) CR 628+/-218 liters/day and 3,3'-T(2) PR 39.8+/-19.8 mug/day (all corrected to 70 kg body wt). In addition to 3,3'-T(2) PR, T(3), and reverse triiodothyronine (rT(3)) PR were determined in three of the HT(4) subjects. In each case studied, the 3,3'-T(2) PR was close to the combined triiodothyronine (T(3) + rT(3)) PR. The mean molar ratio of T(2) PR/(T(3) + rT(3)) PR was 1.08+/-0.10. The results obtained in the HT(4) subjects indicate that the production of 3,3'-T(2) is a major route of T(4) metabolism. The combined studies of 3,3'-T(2), T(3) and rT(3) PR in the HT(4) subjects indicate that both T(3) and rT(3) are major precursors of 3,3'-T(2). In the HT(3) subjects, the conversion of T(3) to 3,3'-T(2), determined as the molar ratio of 3,3'-T(2) PR to T(3) PR, ranged from 0.36 to 0.92, providing further evidence that T(3) is a precursor of 3,3'-T(2). From the close agreement between the mean values for 3,3'-T(2) PR in the euthyroid and HT(4) group it is concluded that most, if not all of the 3,3'-T(2) produced in normal humans is derived by extrathyroidal conversion from T(3) and rT(3).\",\n",
       "  \"Physiological and pharmacological influences on thyroxine to 3,5,3'-triiodothyronine conversion and nuclear 3,5,3'-triiodothyronine binding in rat anterior pituitary. Our recent in vivo studies have suggested that intrapituitary l-thyroxine (T(4)) to 3,5,3'-triiodo-l-thyronine (T(3)) conversion with subsequent nuclear binding of T(3) is an important pathway by which circulating T(4) can inhibit thyrotropin release. The present studies were performed to evaluate various physiological and pharmacological influences on these two processes in rat anterior pituitary tissue. Intact pituitary fragments were incubated in buffer-1% bovine serum albumin containing 0.14 ng/ml [(131)I]T(3) and 3.8 ng/ml [(125)I]T(4). Nuclei were isolated after 3 h of incubation and the bound iodothyronines identified by paper chromatography. There was 0.3-1% [(125)I]T(3) contaminating the medium [(125)I]T(4), and this did not change during incubation. Nuclear [(125)I]T(4) was not decreased by 650-fold excesses of medium T(3) or T(4), suggesting that it was nonspecifically bound. The ratio of nuclear to medium [(131)I]- and [(125)I]T(3) were expressed as nuclear counts per minute per milligram wet weight of tissue:counts per minute per microliter medium. Intrapituitary T(4) to T(3) conversion was evidenced by the fact that the nuclear:medium (N:M) ratio for [(131)I]T(3) was 0.45+/-0.21, whereas that for [(125)I]T(3) was 2.23+/-1.28 (mean+/-SD, n = 51). A ratio (R), the N:M [(125)I]T(3) divided by the N:M [(131)I]T(3), was used as an index of intrapituitary T(4) to T(3) conversion. Increasing medium T(3) concentrations up to 50 ng/ml caused a progressive decrease in the N:M ratio for both T(3) isotopes, but no change in the value for R, indicating that both competed for the same limited-capacity nuclear receptors. Increasing concentrations of medium T(4) caused no change in the N:M [(131)I]T(3) but did cause a significant decrease in R in three of four experiments. These results suggest saturation of T(4)-5'-monodeiodination occurred at lower T(4) concentrations than saturation of nuclear T(3) binding sites. In hypothyroid rats, the N:M ratios for both [(131)I]T(3) and [(125)I]T(3) were increased (P &lt; 0.005), but R was three-fold higher than in controls (P &lt; 0.005). Animals given 10 mug T(4)/100 g body wt per d for 5 d had significantly decreased N:M ratios for both [(131)I]T(3) and [(125)I]T(3), as well as a decreased value for R. In fasted rats, neither N:M ratio was depressed, although hepatic T(4) to T(3) conversion in the same animals was 50% of control (P &lt; 0.005). Iopanoic acid (13 muM), but not 6-n-propylthiouracil (29 muM), decreased the N:M [(125)I]T(3) with a significant decrease in the value for R (P &lt; 0.025 or less). Neither sodium iodide (6 muM) nor thyrotropin-releasing hormone (7-700 nM) affected the T(3) N:M ratios. These results indicate that intrapituitary T(4) to T(3) conversion is stimulated in hypothyroidism and depressed in T(4)-treated animals, whereas opposite changes occur in hepatic T(4)-5'-monodeiodination. Unlike liver, anterior pituitary T(4)-5'-monodeiodination is not affected by fasting or incubation with 6-n-propyl-2-thiouracil, but T(4) to T(3) conversion is inhibited in both by iopanoic acid. These results indicate that there are important differences between anterior pituitary and other tissues in the regulation of T(4)-5'-monodeiodination.\",\n",
       "  'Metabolism of resorcinylic compounds by bacteria. Purification and properties of orcinol hydroxylase from Pseudomonas putida 01. Orcinol hydroxylase (EC 1.14.13.6), which catalyzes the first reaction of orcinol catabolism in Pseudomonas putida 01, has been purified to homogeneity, and crystallized. Orcinol hydroxylase catalyzes the hydroxylation of orcinol with equimolar consumption of O2 and NADH (or NADPH) to 2, 3, 5-trihydroxytoluene, which is nonenzymically oxidized to a quinone. The visible absorption spectrum of the enzyme shows maxima at 373 and 454 nm and a shoulder at 480 nm. FAD can be dissociated from the protein. Reconstitution of enzymic activity was achieved with FAD, and to a limited extent by FMN. The enzyme has a molecular weight of 63,000 to 68,000 and contains 1 mol of FAD per mol of protein. K-m values for the three substrates orcinol, NADH, and O2 are 0.03, 0.13, and 0.07mM, RESPECTIVELY. The molecular activity of the crystalline enzyme is 1560 min minus 1. In the absence of orcinol, NADH is only slowly oxidized with formation of H2O2. Several analogs of orcinol also serve as substrates for hydroxylation, namely resorcinol, 4-methylresorcinol, and 4-bromoresorcinol. Other analogs, m-cresol, m-ethylphenol, 4-ethylresorcinol, and phloroglucinol, mimic orcinol as effectors, in that they (a) accelerate electron flow from NADH to the flavin and (b) decrease the apparent K-m for NADH but not to the same extent as the substrates that are hydroxylated. The latter compounds are not hydroxylated. Instead H2O2 accumulates as the only product of O2 reduction. The enzyme therefore behaves either as a hydroxylase or an oxidase. The ratio of hydroxylase to oxidase activities of the enzyme is decreased by an increase in the temperature of incubation; at 60 degrees the reaction with orcinol is almost 50% uncoupled from hydroxylation. The apparent K-m values for the effectors are in good agreement with the D-D values obtained for orcinol, resorcinol, and m-cresol. K-D values were obtained by measurement of the effector-induced perturbations of the visible absorption spectrum of the flavoprotein by difference absorption spectroscopy. The circular dichroism spectrum of orcinol hydroxylase is also altered in the presence of orcinol. The participation of the flavin in the over-all reaction is demonstrated by its rapid reduction under anaerobic conditions by NADH in the presence or orcinol, resorcinol, or m-cresol. Subsequent introduction of oxygen restores the oxidized form and yields H2O2 when m-cresol is the effector, but not when orcinol is the effector. Transfer of reducing equivalents from the reduced flavoprotein to free FAD may also occur. Reduction of orcinol hydroxylase by NADH in the absence of an effector is 10-4-fold slower than in the presence of an effector. The minimal structural requirements for effectors appear to be a 1,3-dihydroxy or 1-alkyl-3-hydorxybenzene, but only the former are substrates for hydroxylation.',\n",
       "  \"[Production and absorption rate of cerebrospinal fluid in the spinal subarachnoid space of the dog (author's transl)]. Adult mongrel dogs, weighing 10-17 kg. were anesthetized with Nembutal and cervical and lumbosacral laminectomy was performed. The spinal subarachnoid space was blocked by extradural ligation at the level of the C4 to interrupt CSF communication between the cranial and spinal space. Polyethylene catheters were placed in the cervical and lumbosacral subarachnoid space, and artificial Mock CSF buffer, pH 7.35-7.40, containing inulin of 25 mg/dl or 14C-inulin of 1.5-2 muCi/dl as a tracer was perfused in the sacro-cervical direction through the catheter. After a steady state of perfusion was acommplished, the CSF was collected from the outlet catheter. Production and absorption rate of the CSF were calculated after Pappenheimer and Heisey's equation. 1) Effects of CSF pressure on the rate of production (Vf) and absorption (Va) of CSF and on the difference between outflow fluid rate (Vo) and inflow fluid rate (Vi) were studied within the pressure range of -100 to +600 mmH2O. Then, regression lines were calculated by means of the least square method. See Article. Vf was little affected by changes in CSF pressure, while Va increased linearly as CSF pressure elevated. This suggests that the spinal subarachnoid space plays an important role as a site of CSF absorption when the intracranial pressure increases. Vo-Vi, that is difference between absorption and production rate, decreased linearly as the CSF pressure increased. 2) Under a constant CSF pressure of +200 mmH2O, the effects of glucocorticoids (dexamethasone, 0.25 mg/kg and hydrocortisone, 4.15 mg/kg) and a carbonic anhydrase inhibitor (acetazolamide, 10 mg/kg) upon the production and absorption rate of CSF were determined 1/2, 1, 2 and 3 hours after intravenous administration. a) Effect of dexamethasone: The rate of CSF production was reduced to 60.5 +/- 2.4% (p less than 0.001) of the control level. The absorption rate of CSF also decreased to 59.2 +/- 6.09% (p less than 0.001) of the control. b) Effect of hydrocortisone: The production rate of CSF decreased to 67.4 +/- 6.61% (p less than 0.001), and the absorption rate to 76.5 +/- 3.94% (p less than 0.001) of the control level. c) Effect of acetazolamide: The production and absorption rate also decreased to 57.2 +/- 5.61% (p less than 0.001) and to 56.9 +/- 7.02% (p less than 0.001), respectively. 3) Pentration of tritiated dexamethasone and tritiated hydrocortisone from plasma to CSF. The penetration of tritiated dexamethasone and tritiated hydrocortisone from plasma to CSF in the spinal and cranial subarachnoid space was observed after the intravenous administration. The CSF/Plasma ratio of dexamethasone was 30.9% at 15 minutes and gradually increased to 91.5% and 93.5%, respectively, in the cranial and spinal CSF at 3 hours after the injection.\",\n",
       "  \"The role of sulfhydryl groups on the impaired hepatic 3',3,5-triiodothyronine generation from thyroxine in the hypothyroid, starved, fetal, and neonatal rodent. The role of nonprotein sulfhydryl groups (NPSH) in the decreased in vitro hepatic 3',3,5-triiodothyronine (T(3)) generation from thyroxine (T(4)) in the starved, hypothyroid, fetal and 1- to 4-d-old neonatal rat and dwarf mouse was assessed. NPSH were measured in fresh 25% liver homogenates prepared in 0.1 M PO(4)/10 mM EDTA buffer. As compared with values in adult male rats, NPSH concentration was decreased in the 2-d-starved (1.1+/-0.04 (mean+/-SE) vs. 2.2+/-0.15 mmol/250 g wet liver weight, P &lt; 0.001), fetal (1.0+/-0.04 vs. 3.2+/-0.08, P &lt; 0.001), 1-d-old neonatal (1.1+/-0.03 vs. 2.1+/-0.04, P &lt; 0.001), and hypothyroid (thyroidectomized 60 d) (1.4+/-0.06 vs. 2.2+/-0.15 P &lt; 0.001) rat. NPSH were also decreased in the hypothyroid, hypopituitary dwarf mouse as compared with values in their normal litter mates (1.3+/-0.03 vs. 2.0+/-0.2, P &lt; 0.01). Chronic administration of T(3) (0.5 mug/100 g body wt per d) markedly increased hepatic T(3) generation from T(4) in the thyroidectomized rat and in the dwarf mouse to values similar to those observed in the normal rodent without affecting NPSH concentration. In contrast, T(3) administration to the starved rat did not alter either hepatic T(3) generation from T(4) or NPSH. Reduced glutathione concentration was also markedly decreased in the starved rat (fed; 1.05+/-0.075 mmol/250 g wet tissue vs. starved 0.38+/-0.02, P &lt; 0.001). Dithiothreitol (DTT), a thiol reducing agent, increased hepatic T(3) generation from T(4) in the normal adult male rat by 45+/-5% in six experiments. When compared to DTT-stimulated control homogenates, the addition of DTT completely restored hepatic T(3) generation in starved rats, partially restored T(3) generation in 1- and 4-d-old neonates, but had little or no effect in the fetal and hypothyroid rat and dwarf mouse. Liver homogenates stored for 6 mo at -20 degrees C lost their capacity to generate T(3) from T(4). NPSH concentrations in the frozen homogenates decreased progressively with increasing storage and were absent by 6 mo. 5'-Deiodinase activity correlated with NPSH concentration in the stored homogenates (r = 0.95, P &lt; 0.005). Addition of DTT partially restored hepatic T(3) generation in the frozen homogenate. It is concluded that NPSH are important for the action of the liver 5'-deiodinase. The decreased hepatic T(3) generation in the starved rat is associated with decreased NPSH but not with a decrease in the absolute quantity of 5'-deiodinase because provision of sulfhydryl groups restored hepatic T(3) generation to normal. In contrast, the decreased hepatic T(3) generation in the adult hypothyroid rodent and in the fetal rat is probably due to a decrease in the enzyme concentration per se. In the 1- and 4-d neonatal rat, the decrease in hepatic T(3) generation is secondary to a decrease in NPSH and the deiodinating enzyme.\"])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Lets load the model and the tokenizer from pretrained",
   "id": "6919cd20e55c8968"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:22:51.910738Z",
     "start_time": "2025-05-26T02:22:47.299537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the PEFT config to find the base model\n",
    "peft_model_path = \"./outputs/fine-tuning/trainer\"\n",
    "peft_config = PeftConfig.from_pretrained(peft_model_path)\n",
    "\n",
    "# Load the base model (this must match the model used during fine-tuning)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    peft_config.base_model_name_or_path,\n",
    "    load_in_8bit=True,  # Load the model in 8-bit mode for efficiency\n",
    ")\n",
    "\n",
    "# Load the PEFT model\n",
    "model = PeftModel.from_pretrained(base_model, peft_model_path)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./outputs/fine-tuning/tokenizer\")"
   ],
   "id": "de703c004709ca27",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# How are we going to use the model for predict A, B, C or D?\n",
    "\n",
    "Nice, we are going to perform a forward pass on the model, and pick the most likely answer based on the logits returned by the model. The model will return a probability distribution over the possible answers, and we will pick the one with the highest probability."
   ],
   "id": "7a37a98d2057527"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:22:52.056475Z",
     "start_time": "2025-05-26T02:22:52.031565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ForwardType = Callable[[str, List[str]], Any]\n",
    "\n",
    "def enhanced_forward(\n",
    "        llm,\n",
    "        tokenizer,\n",
    "        augmenter: Callable[[str, int], Tuple[List[int], List[str]]],\n",
    "        k_augmentations: int,\n",
    "        prompt_builder: Callable[[str, List[str], List[str]], str],\n",
    "        question: str,\n",
    "        options: List[str],\n",
    "        device: str,\n",
    "        num_iterations: int = 1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs multiple forward passes, appending the most probable token each time,\n",
    "    and returns average probabilities across all tokens.\n",
    "\n",
    "    Args:\n",
    "        llm: The language model to use for generating the response.\n",
    "        tokenizer: The tokenizer associated with the language model.\n",
    "        augmenter (Callable): A function that takes a query string and returns the first k_augmentations in a tuple of\n",
    "            distances and items.\n",
    "        k_augmentations (int): The number of augmentations to generate.\n",
    "        prompt_builder (Callable): A function that builds the prompt for the language model.\n",
    "            It takes the augmented information, question and options, and returns a formatted string.\n",
    "        question (str): The question to ask.\n",
    "        options (List[str]): The list of options to choose from.\n",
    "        device (str): The device to run the model on ('cpu' or 'cuda').\n",
    "        num_iterations (int): Number of forward passes to perform (default: 3).\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - List of generated tokens\n",
    "            - Average probabilities tensor across all tokens\n",
    "    \"\"\"\n",
    "    llm.eval()\n",
    "    # Get augmented items\n",
    "    _, items = augmenter(question, k_augmentations)\n",
    "\n",
    "    # Generate the initial prompt\n",
    "    prompt = prompt_builder(question, options, items)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=True).to(device)\n",
    "\n",
    "    generated_token_ids =  input_ids.clone()\n",
    "    probss = []\n",
    "    generated_tokens = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_iterations):\n",
    "            # Prepare model inputs\n",
    "            # For most autoregressive models, only input_ids are strictly necessary for the forward pass\n",
    "            # if attention_mask is not explicitly handled or modified in the loop.\n",
    "            # However, it's good practice to pass it if the model uses it.\n",
    "            # As we append tokens, the attention_mask also needs to be extended.\n",
    "            #attention_mask = torch.ones_like(generated_token_ids).to(device)\n",
    "\n",
    "            # Get model outputs\n",
    "            with torch.no_grad():\n",
    "                outputs = llm(generated_token_ids)\n",
    "\n",
    "            # Get the logits for the last token in the sequence\n",
    "            # outputs.logits is typically of shape (batch_size, sequence_length, vocab_size)\n",
    "            next_token_logits = outputs.logits[:, -1, :] # Get logits for the very last token\n",
    "\n",
    "            # Apply softmax to get probabilities (optional, as argmax works on logits directly)\n",
    "            probs = torch.softmax(next_token_logits, dim=-1)\n",
    "            probss.append(probs)\n",
    "            # Get the predicted token ID (greedy decoding)\n",
    "            next_token_id = torch.argmax(probs, dim=-1).unsqueeze(-1)\n",
    "\n",
    "            # Append the predicted token ID to the generated sequence\n",
    "            generated_token_ids = torch.cat([generated_token_ids, next_token_id], dim=1)\n",
    "\n",
    "            # Check if the generated token is an end-of-sequence (EOS) token\n",
    "            if tokenizer.eos_token_id is not None and next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "\n",
    "            # Append the generated token ID to the list\n",
    "            generated_tokens.append(next_token_id.item())\n",
    "\n",
    "    # Concatenate all probabilities\n",
    "    probs = torch.cat(probss, dim=0)\n",
    "    # Average the probabilities across all iterations\n",
    "    avg_probs = torch.mean(probs, dim=0)\n",
    "\n",
    "    return generated_tokens, avg_probs\n",
    "\n",
    "def build_enhanced_forwarder(\n",
    "    llm,\n",
    "    tokenizer,\n",
    "    augmenter: Callable[[str, int], Tuple[List[int], List[str]]],\n",
    "    k_augmentations: int,\n",
    "    prompt_builder: Callable[[str, List[str], List[str]], str],\n",
    "    num_iterations: int,\n",
    "    device: str,\n",
    ") -> Callable[[str, List[str]], Tuple[List[int], torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Builds an enhanced forward function that can be used to generate responses from the language model.\n",
    "\n",
    "    Returns:\n",
    "        Callable: A function that takes a question and a list of options and returns the generated response.\n",
    "    \"\"\"\n",
    "    def forward_fn(question: str, options: List[str]) -> Tuple[List[int], torch.Tensor]:\n",
    "        return enhanced_forward(\n",
    "            llm=llm,\n",
    "            tokenizer=tokenizer,\n",
    "            augmenter=augmenter,\n",
    "            k_augmentations=k_augmentations,\n",
    "            prompt_builder=prompt_builder,\n",
    "            question=question,\n",
    "            options=options,\n",
    "            num_iterations=num_iterations,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    return forward_fn\n",
    "\n",
    "def _from_logits(\n",
    "        tokenizer,\n",
    "        logits: torch.Tensor,\n",
    "        options: list[int],\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Picks an option from a list of options based on the given logits.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: The tokenizer to use for encoding the options.\n",
    "        logits (list[float]): The logits for each option.\n",
    "        options (list[int]): encoded options to choose from.\n",
    "\n",
    "    Returns:\n",
    "        int: The index of the chosen option.\n",
    "    \"\"\"\n",
    "    # Convert logits to probabilities\n",
    "    probs = torch.softmax(logits, dim=0).tolist()\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate the score for each option\n",
    "    scores = []\n",
    "    for _, option in enumerate(options):\n",
    "        score = probs[option]\n",
    "        scores.append(score)\n",
    "\n",
    "    # Choose the option with the highest score\n",
    "    chosen_option = scores.index(max(scores))\n",
    "\n",
    "    return chosen_option\n",
    "\n",
    "def build_from_logits(\n",
    "    tokenizer,\n",
    "    options: list[str],\n",
    ") -> Callable[[torch.Tensor], int]:\n",
    "    \"\"\"\n",
    "    Builds a function that can be used to pick an option from a list of options based on the given logits.\n",
    "\n",
    "    Args:\n",
    "        tokenizer: The tokenizer to use for encoding the options.\n",
    "        options (list[str]): The list of options to choose from.\n",
    "\n",
    "    Returns:\n",
    "        Callable[[torch.Tensor], int]: A function that takes logits and returns the index of the chosen option.\n",
    "    \"\"\"\n",
    "    # Encode the options\n",
    "    options = [\n",
    "        tokenizer.encode(option, add_special_tokens=False)[0]\n",
    "        for option in options\n",
    "    ]\n",
    "\n",
    "    return lambda model_out: _from_logits(tokenizer, model_out, options)\n",
    "\n",
    "possible_answers = [\" A\", \" B\", \" C\", \" D\"]\n",
    "\n",
    "picker = build_from_logits(\n",
    "    tokenizer,\n",
    "    options=possible_answers,\n",
    ")"
   ],
   "id": "82259e5edd888f33",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:22:52.079927Z",
     "start_time": "2025-05-26T02:22:52.073546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# the same prompt used in the fine-tuning\n",
    "def prompt(\n",
    "    question: str,\n",
    "    options: List[str],\n",
    "    augmented_items: List[str],\n",
    ") -> str:\n",
    "    context = \"\\n\".join(augmented_items)\n",
    "\n",
    "    options_str = \"\\n\".join(\n",
    "        [f\"{chr(65 + i)}. {option}\" for i, option in enumerate(options)]\n",
    "    )\n",
    "\n",
    "    \"\"\"\n",
    "    Generates a prompt for the language model based on the question, options, and augmented items.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to ask.\n",
    "        options (List[str]): The list of options to choose from.\n",
    "        augmented_items (List[str]): The augmented items to include in the prompt.\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted prompt string.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are an expert AI specializing in multiple-choice questions.\n",
    "Your task is to analyze the provided context, question, and options, then identify the single best answer.\n",
    "Respond with only the capital letter (A, B, C, or D) corresponding to your choice.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Options:\n",
    "{options}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    return prompt"
   ],
   "id": "c314b02069e2c1c4",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:22:52.205026Z",
     "start_time": "2025-05-26T02:22:52.184615Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.to(device)"
   ],
   "id": "2fa3abda0c64691c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-15): 16 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear8bitLt(\n",
       "                (base_layer): Linear8bitLt(in_features=8192, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:22:52.303779Z",
     "start_time": "2025-05-26T02:22:52.294897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "forward = build_enhanced_forwarder(\n",
    "    base_model,\n",
    "    tokenizer,\n",
    "    querier,\n",
    "    k_augmentations=10,\n",
    "    prompt_builder=prompt,\n",
    "    num_iterations=1,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "def forward_and_get_last_logit(\n",
    "    question,\n",
    "    options,\n",
    "):\n",
    "    tokens, logits =  forward(\n",
    "        question,\n",
    "        options=options,\n",
    "    )\n",
    "\n",
    "    return logits"
   ],
   "id": "d6ef670a90c437a3",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lets evaluate the model\n",
    "\n",
    "We are going to load the evaluation dataset and perform the entire rag pipeline in order to get the best answer for each question."
   ],
   "id": "8d4d5532a4ed2331"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:22:52.408920Z",
     "start_time": "2025-05-26T02:22:52.401812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(\n",
    "    forward_fn: ForwardType,\n",
    "    picker_fn: Callable,\n",
    "    eval_dataset: Dataset,\n",
    "    log_each: int = 100,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation dataset.\n",
    "\n",
    "    Args:\n",
    "        forward_fn: The forward function to use for generating the response.\n",
    "        picker_fn: The function to use for picking the best option.\n",
    "        eval_dataset: The evaluation dataset.\n",
    "        log_each: The number of samples to log accuracy for.\n",
    "\n",
    "    Returns:\n",
    "        The accuracy of the model on the evaluation dataset.\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = len(eval_dataset)\n",
    "\n",
    "    for i in range(total):\n",
    "        item = eval_dataset[i]\n",
    "        question = item[\"question\"]\n",
    "        options = item[\"options\"]\n",
    "        answer_idx = item[\"answer_idx\"]\n",
    "\n",
    "        # Get the model's response\n",
    "        response = forward_fn(question, options)\n",
    "\n",
    "        # Pick the best option\n",
    "        picked_idx = picker_fn(response)\n",
    "\n",
    "        if picked_idx == answer_idx:\n",
    "            correct += 1\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Right answer: {answer_idx}, picked: {picked_idx}\")\n",
    "        if i % log_each == 0:\n",
    "            # print current accuracy\n",
    "            print(f\"Accuracy at {i}: {correct / (i + 1):.2f}\")\n",
    "\n",
    "    return correct / total"
   ],
   "id": "9ec186b3d7e186c4",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:22:52.499586Z",
     "start_time": "2025-05-26T02:22:52.493752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TrainAndEval(Dataset):\n",
    "    \"\"\"\n",
    "    This loads a map which contains:\n",
    "    - \"id\"\n",
    "    - \"excerpt\"\n",
    "    - \"question\"\n",
    "    - \"statement\": the correct option\n",
    "    - \"distractors\"\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "        self._raw_data = []\n",
    "        with open(file_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self._raw_data.append(line.strip())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._raw_data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return json.loads(self._raw_data[idx])"
   ],
   "id": "6044a6fc7087595e",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:22:52.593719Z",
     "start_time": "2025-05-26T02:22:52.587095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EvalWithAnswers(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset that takes a TrainAndEval dataset and adds the statement to the distractors\n",
    "    to create a multiple choice question. The statement is inserted at a random position\n",
    "    in the distractors.\n",
    "\n",
    "    Returns two item keys: options and answer_idx.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset: TrainAndEval):\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        item = self.dataset[idx]\n",
    "\n",
    "        options = item[\"distractors\"]\n",
    "        # insert the statement at any random position in the options\n",
    "        index = random.randint(0, len(options))\n",
    "        options.insert(index, item[\"statement\"])\n",
    "\n",
    "        item[\"options\"] = options\n",
    "        item[\"answer_idx\"] = index\n",
    "\n",
    "        return item\n"
   ],
   "id": "1a1841c7d7e40ff8",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:22:52.684867Z",
     "start_time": "2025-05-26T02:22:52.667315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "evaluationData = TrainAndEval(\"./data/pubmed_QA_eval.json\")\n",
    "evaluateWithAnswers = EvalWithAnswers(evaluationData)"
   ],
   "id": "795933d822c9c683",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:22:52.777239Z",
     "start_time": "2025-05-26T02:22:52.770637Z"
    }
   },
   "cell_type": "code",
   "source": "evaluateWithAnswers[0]  # Check the first item in the evaluation dataset",
   "id": "929445d7a586b0fe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'pubmed23n0012_5208',\n",
       " 'excerpt': 'Temporal changes in medial basal hypothalamic LH-RH correlated with plasma LH during the rat estrous cycle and following electrochemical stimulation of the medial preoptic area in pentobarbital-treated proestrous rats. In the present studies we have simultaneously measured changes in medial basal hypothalamic (MBH) leutenizing hormone-releasing hormone (LH-RH) and in plasma LH by radioimmunoassay in female rats at various hours during the 4-day estrous cycle and under experimental conditions known to alter pituitary LH secretion. In groups of rats decapitated at 12.00 h and 15.00 h on estrus and diestrus, plasma LH remained at basal levels (5-8 ng/ml) and MBH-LH-RH concentrations showed average steady state concentrations of 2231 +/- 205 pg/mg. On the day of proestrus hourly measurements of MBH-LH-RH between 12.00 h and 21.00 h suggested rhythmic rises and falls in the decapeptide concomitant with rises and falls in plasma LH. In a second group of pentobarbital-anesthetized proestrous rats a significant decline in MBH-LH-RH occurred (to 573 +/- 137 pg/mg) which then remained at low concentrations between 14.00 h and 18.00 h proestrus. Following bilateral preoptic area (MPOA) electrochemical stimulation of pentobarbital-treated proestrous rats, LH was significantly increased by 30 min, peaked between 90-120 min and returned to basal levels by 210 min poststimulation. In the same animals within 15 min poststimulation, MBH-LH-RH increased from the basal concentrations noted after pentobarbital anesthesia to elevated levels comparable to those observed throughout estrus, diestrus and on proestrous morning. Further, as plasma LH rose to peak concentrations and declined to basal plasma values, rhythmic rises and falls in MBH-LH-RH were observed with intervals between pulses of approximately 60 min. Seemingly, hypothalamic LH-RH is released as pulsatile pulses from a releasable pool; this pool is replenished and again LH-RH is discharged in response to constant stimulation by the preoptic brain.',\n",
       " 'statement': 'Temporal fluctuations in medial basal hypothalamic LH-RH levels correlate with plasma LH changes during the rat estrous cycle and following stimulation of the medial preoptic area.',\n",
       " 'question': 'What relationship exists between medial basal hypothalamic LH-RH levels and plasma LH changes during the rat estrous cycle and after stimulation of the medial preoptic area?',\n",
       " 'distractors': ['Medial basal hypothalamic LH-RH levels remain constant throughout the rat estrous cycle and do not affect plasma LH changes.',\n",
       "  'There is no relationship between medial basal hypothalamic LH-RH levels and plasma LH changes during the rat estrous cycle.',\n",
       "  'Increased medial basal hypothalamic LH-RH levels lead to a decrease in plasma LH during the rat estrous cycle.',\n",
       "  'Temporal fluctuations in medial basal hypothalamic LH-RH levels correlate with plasma LH changes during the rat estrous cycle and following stimulation of the medial preoptic area.'],\n",
       " 'options': ['Medial basal hypothalamic LH-RH levels remain constant throughout the rat estrous cycle and do not affect plasma LH changes.',\n",
       "  'There is no relationship between medial basal hypothalamic LH-RH levels and plasma LH changes during the rat estrous cycle.',\n",
       "  'Increased medial basal hypothalamic LH-RH levels lead to a decrease in plasma LH during the rat estrous cycle.',\n",
       "  'Temporal fluctuations in medial basal hypothalamic LH-RH levels correlate with plasma LH changes during the rat estrous cycle and following stimulation of the medial preoptic area.'],\n",
       " 'answer_idx': 3}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T02:23:01.219555Z",
     "start_time": "2025-05-26T02:22:52.893406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "accuracy = evaluate(\n",
    "    forward_fn=forward_and_get_last_logit,\n",
    "    picker_fn=picker,\n",
    "    eval_dataset=evaluateWithAnswers,\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ],
   "id": "454d3508da1f62ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right answer: 3, picked: 1\n",
      "Accuracy at 0: 0.00\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.20 GiB. GPU 0 has a total capacity of 5.65 GiB of which 1.43 GiB is free. Including non-PyTorch memory, this process has 4.19 GiB memory in use. Of the allocated memory 1.75 GiB is allocated by PyTorch, and 2.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforward_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforward_and_get_last_logit\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpicker_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpicker\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevaluateWithAnswers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[18], line 29\u001B[0m, in \u001B[0;36mevaluate\u001B[0;34m(forward_fn, picker_fn, eval_dataset, log_each)\u001B[0m\n\u001B[1;32m     26\u001B[0m answer_idx \u001B[38;5;241m=\u001B[39m item[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124manswer_idx\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Get the model's response\u001B[39;00m\n\u001B[0;32m---> 29\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mforward_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# Pick the best option\u001B[39;00m\n\u001B[1;32m     32\u001B[0m picked_idx \u001B[38;5;241m=\u001B[39m picker_fn(response)\n",
      "Cell \u001B[0;32mIn[17], line 15\u001B[0m, in \u001B[0;36mforward_and_get_last_logit\u001B[0;34m(question, options)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward_and_get_last_logit\u001B[39m(\n\u001B[1;32m     12\u001B[0m     question,\n\u001B[1;32m     13\u001B[0m     options,\n\u001B[1;32m     14\u001B[0m ):\n\u001B[0;32m---> 15\u001B[0m     tokens, logits \u001B[38;5;241m=\u001B[39m  \u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m logits\n",
      "Cell \u001B[0;32mIn[14], line 104\u001B[0m, in \u001B[0;36mbuild_enhanced_forwarder.<locals>.forward_fn\u001B[0;34m(question, options)\u001B[0m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward_fn\u001B[39m(question: \u001B[38;5;28mstr\u001B[39m, options: List[\u001B[38;5;28mstr\u001B[39m]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[List[\u001B[38;5;28mint\u001B[39m], torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m--> 104\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43menhanced_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    105\u001B[0m \u001B[43m        \u001B[49m\u001B[43mllm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mllm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    106\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    107\u001B[0m \u001B[43m        \u001B[49m\u001B[43maugmenter\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maugmenter\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    108\u001B[0m \u001B[43m        \u001B[49m\u001B[43mk_augmentations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mk_augmentations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    109\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprompt_builder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprompt_builder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    110\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquestion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    111\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    112\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_iterations\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_iterations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    113\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    114\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[14], line 59\u001B[0m, in \u001B[0;36menhanced_forward\u001B[0;34m(llm, tokenizer, augmenter, k_augmentations, prompt_builder, question, options, device, num_iterations)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_iterations):\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;66;03m# Prepare model inputs\u001B[39;00m\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;66;03m# For most autoregressive models, only input_ids are strictly necessary for the forward pass\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     56\u001B[0m \n\u001B[1;32m     57\u001B[0m     \u001B[38;5;66;03m# Get model outputs\u001B[39;00m\n\u001B[1;32m     58\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 59\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m \u001B[43mllm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgenerated_token_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;66;03m# Get the logits for the last token in the sequence\u001B[39;00m\n\u001B[1;32m     62\u001B[0m     \u001B[38;5;66;03m# outputs.logits is typically of shape (batch_size, sequence_length, vocab_size)\u001B[39;00m\n\u001B[1;32m     63\u001B[0m     next_token_logits \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mlogits[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :] \u001B[38;5;66;03m# Get logits for the very last token\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:965\u001B[0m, in \u001B[0;36mcan_return_tuple.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    962\u001B[0m     set_attribute_for_modules(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_is_top_level_module\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    964\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 965\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    966\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_requested_to_return_tuple \u001B[38;5;129;01mor\u001B[39;00m (is_configured_to_return_tuple \u001B[38;5;129;01mand\u001B[39;00m is_top_level_module):\n\u001B[1;32m    967\u001B[0m         output \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mto_tuple()\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/deprecation.py:172\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action\u001B[38;5;241m.\u001B[39mNOTIFY, Action\u001B[38;5;241m.\u001B[39mNOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m--> 172\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:837\u001B[0m, in \u001B[0;36mLlamaForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001B[0m\n\u001B[1;32m    835\u001B[0m \u001B[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001B[39;00m\n\u001B[1;32m    836\u001B[0m slice_indices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mslice\u001B[39m(\u001B[38;5;241m-\u001B[39mlogits_to_keep, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(logits_to_keep, \u001B[38;5;28mint\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m logits_to_keep\n\u001B[0;32m--> 837\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlm_head\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mslice_indices\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    839\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    840\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 2.20 GiB. GPU 0 has a total capacity of 5.65 GiB of which 1.43 GiB is free. Including non-PyTorch memory, this process has 4.19 GiB memory in use. Of the allocated memory 1.75 GiB is allocated by PyTorch, and 2.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Lets predict the answers for the contest",
   "id": "46c707e76ff70bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def predict(\n",
    "    forward_fn: ForwardType,\n",
    "    picker_fn: Callable,\n",
    "    eval_dataset: EvalWithAnswers,\n",
    "    log_each: int = 100,\n",
    ") -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Evaluate the model on the evaluation dataset.\n",
    "\n",
    "    Args:\n",
    "        forward_fn: The forward function to use for generating the response.\n",
    "        picker_fn: The function to use for picking the best option.\n",
    "        eval_dataset: The evaluation dataset.\n",
    "        log_each: The number of samples to log accuracy for.\n",
    "\n",
    "    Returns:\n",
    "        The id and the picked option of the model on the evaluation dataset.\n",
    "    \"\"\"\n",
    "    total = len(eval_dataset)\n",
    "    responses = []\n",
    "    for i in range(total):\n",
    "        item = eval_dataset[i]\n",
    "        question = item[\"question\"]\n",
    "        options = item[\"option\"]\n",
    "\n",
    "        # Get the model's response\n",
    "        response = forward_fn(question, options)\n",
    "\n",
    "        # Pick the best option\n",
    "        picked_idx = picker_fn(response)\n",
    "\n",
    "        responses.append((i, picked_idx))\n",
    "\n",
    "        if i != 0 and i% log_each == 0:\n",
    "            print(f\"Processed {i/total}%\")\n",
    "\n",
    "    return responses"
   ],
   "id": "7baea40f599361e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TestQuestions(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for the test questions in the competition.\n",
    "    Format:\n",
    "        id\n",
    "        question\n",
    "        option: list of options\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path: str):\n",
    "        self.file_path = file_path\n",
    "        self._raw_data = []\n",
    "        with open(file_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                self._raw_data.append(line.strip())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._raw_data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return json.loads(self._raw_data[idx])"
   ],
   "id": "269025728330046b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_data = TestQuestions(\"/data/pubmed_QA_test_questions.json\")\n",
    "responses = predict(\n",
    "    forward_fn=forward_and_get_last_logit,\n",
    "    picker_fn=picker,\n",
    "    eval_dataset=test_data,\n",
    ")\n",
    "\n",
    "responses"
   ],
   "id": "9fbf9ce65778c07e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "responses_with_ids = []\n",
    "\n",
    "for i in range(len(responses)):\n",
    "    responses_with_ids.append((test_data[i][\"id\"], responses[i][1]))\n",
    "dataset = pd.DataFrame(responses_with_ids, columns=[\"ID\", \"answer\"])\n",
    "dataset.head()\n",
    "dataset.to_csv(\"predictions.csv\", index=False)"
   ],
   "id": "e9142db97dfd247a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
