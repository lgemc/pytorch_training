{
 "cells": [
  {
   "cell_type": "code",
   "id": "1cedfb3a-4cb7-4d85-8a3e-179778102588",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T20:03:59.387455Z",
     "start_time": "2025-05-24T20:03:55.899367Z"
    }
   },
   "source": [
    "!pip install transformers==4.51.3\n",
    "!pip install sentence-transformers==4.1.0\n",
    "!pip install einops\n",
    "!pip install 'accelerate>=0.26.0'\n",
    "!pip install bitsandbytes"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.51.3 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (4.51.3)\r\n",
      "Requirement already satisfied: filelock in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from transformers==4.51.3) (3.18.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from transformers==4.51.3) (0.31.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from transformers==4.51.3) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from transformers==4.51.3) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from transformers==4.51.3) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from transformers==4.51.3) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from transformers==4.51.3) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from transformers==4.51.3) (0.21.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from transformers==4.51.3) (0.5.3)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from transformers==4.51.3) (4.67.1)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3) (2025.3.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers==4.51.3) (4.13.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from requests->transformers==4.51.3) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from requests->transformers==4.51.3) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from requests->transformers==4.51.3) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from requests->transformers==4.51.3) (2025.4.26)\r\n",
      "Requirement already satisfied: sentence-transformers==4.1.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (4.1.0)\r\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from sentence-transformers==4.1.0) (4.51.3)\r\n",
      "Requirement already satisfied: tqdm in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from sentence-transformers==4.1.0) (4.67.1)\r\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from sentence-transformers==4.1.0) (2.7.0)\r\n",
      "Requirement already satisfied: scikit-learn in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from sentence-transformers==4.1.0) (1.6.1)\r\n",
      "Requirement already satisfied: scipy in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from sentence-transformers==4.1.0) (1.13.1)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from sentence-transformers==4.1.0) (0.31.2)\r\n",
      "Requirement already satisfied: Pillow in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from sentence-transformers==4.1.0) (11.2.1)\r\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from sentence-transformers==4.1.0) (4.13.2)\r\n",
      "Requirement already satisfied: filelock in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers==4.1.0) (3.18.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers==4.1.0) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers==4.1.0) (25.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers==4.1.0) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers==4.1.0) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers==4.1.0) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers==4.1.0) (0.21.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers==4.1.0) (0.5.3)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from huggingface-hub>=0.20.0->sentence-transformers==4.1.0) (2025.3.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers==4.1.0) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers==4.1.0) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers==4.1.0) (3.1.6)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers==4.1.0) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers==4.1.0) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers==4.1.0) (12.6.80)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers==4.1.0) (9.5.1.17)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers==4.1.0) (12.6.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers==4.1.0) (11.3.0.4)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers==4.1.0) (10.3.7.77)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers==4.1.0) (11.7.1.2)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers==4.1.0) (12.5.4.2)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers==4.1.0) (0.6.3)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers==4.1.0) (2.26.2)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers==4.1.0) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers==4.1.0) (12.6.85)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers==4.1.0) (1.11.1.6)\r\n",
      "Requirement already satisfied: triton==3.3.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=1.11.0->sentence-transformers==4.1.0) (3.3.0)\r\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from triton==3.3.0->torch>=1.11.0->sentence-transformers==4.1.0) (78.1.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers==4.1.0) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from jinja2->torch>=1.11.0->sentence-transformers==4.1.0) (3.0.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers==4.1.0) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers==4.1.0) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers==4.1.0) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers==4.1.0) (2025.4.26)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from scikit-learn->sentence-transformers==4.1.0) (1.5.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from scikit-learn->sentence-transformers==4.1.0) (3.6.0)\r\n",
      "Requirement already satisfied: einops in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (0.8.1)\r\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (1.7.0)\r\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from accelerate>=0.26.0) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from accelerate>=0.26.0) (25.0)\r\n",
      "Requirement already satisfied: psutil in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from accelerate>=0.26.0) (7.0.0)\r\n",
      "Requirement already satisfied: pyyaml in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from accelerate>=0.26.0) (6.0.2)\r\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from accelerate>=0.26.0) (2.7.0)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from accelerate>=0.26.0) (0.31.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from accelerate>=0.26.0) (0.5.3)\r\n",
      "Requirement already satisfied: filelock in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.18.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2025.3.0)\r\n",
      "Requirement already satisfied: requests in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.13.2)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.6)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.6.80)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (9.5.1.17)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.6.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.3.0.4)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (10.3.7.77)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.7.1.2)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.5.4.2)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (0.6.3)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (2.26.2)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.6.85)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.11.1.6)\r\n",
      "Requirement already satisfied: triton==3.3.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.3.0)\r\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from triton==3.3.0->torch>=2.0.0->accelerate>=0.26.0) (78.1.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (3.0.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.4.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2025.4.26)\r\n",
      "Requirement already satisfied: bitsandbytes in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (0.45.5)\r\n",
      "Requirement already satisfied: torch<3,>=2.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from bitsandbytes) (2.7.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from bitsandbytes) (1.26.4)\r\n",
      "Requirement already satisfied: filelock in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (1.14.0)\r\n",
      "Requirement already satisfied: networkx in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (12.6.80)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (9.5.1.17)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (12.6.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (11.3.0.4)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (10.3.7.77)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (11.7.1.2)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (12.5.4.2)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (0.6.3)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (2.26.2)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (12.6.85)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (1.11.1.6)\r\n",
      "Requirement already satisfied: triton==3.3.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from torch<3,>=2.0->bitsandbytes) (3.3.0)\r\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from triton==3.3.0->torch<3,>=2.0->bitsandbytes) (78.1.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from sympy>=1.13.3->torch<3,>=2.0->bitsandbytes) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/lmanrique/miniconda3/envs/faiss39/lib/python3.9/site-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "76dba2b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T20:04:07.519925Z",
     "start_time": "2025-05-24T20:03:59.392933Z"
    }
   },
   "source": [
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "#from dotenv import load_dotenv\n",
    "#load_dotenv()\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "# Check HF_TOKEN environment variable\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    print(\"Hugging Face token found in environment variable.\")\n",
    "else:\n",
    "    token = input(\"Enter your Hugging Face token: \")\n",
    "    login(token=token) # Token de Hugging Face"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T20:04:08.686285Z",
     "start_time": "2025-05-24T20:04:08.683147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")"
   ],
   "id": "6f7a3d8ee0fde48c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "2030a6f1-159c-4673-867c-2ea97eb9a27e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T20:04:11.982702Z",
     "start_time": "2025-05-24T20:04:09.712650Z"
    }
   },
   "source": [
    "base_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"float16\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "# Configurar tokenizador\n",
    "base_tokenizer.pad_token = base_tokenizer.eos_token\n",
    "base_tokenizer.padding_side = \"right\""
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "c4690f9f-62cb-4444-b8ce-f010ab35841e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T20:04:13.085207Z",
     "start_time": "2025-05-24T20:04:13.080261Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = nn.Linear  # Use standard Linear layer\n",
    "    linear_module_names = set()\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            linear_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    \n",
    "    if 'lm_head' in linear_module_names:  # Optionally exclude output head\n",
    "        linear_module_names.remove('lm_head')\n",
    "    \n",
    "    return list(linear_module_names)\n",
    "\n",
    "modules = find_all_linear_names(base_model)\n",
    "modules"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gate_proj', 'k_proj', 'up_proj', 'v_proj', 'down_proj', 'o_proj', 'q_proj']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "2e79c80c-c898-4304-a42f-e0bc868ab1cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T20:04:16.492673Z",
     "start_time": "2025-05-24T20:04:13.696586Z"
    }
   },
   "source": [
    "from datasets import DatasetDict,load_dataset\n",
    "training_data = load_dataset(\"juan-carvajal/maia-pln-2025-training\")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "33b3df5d-0a76-43c5-8480-bf42e6897b7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T20:04:17.836950Z",
     "start_time": "2025-05-24T20:04:16.497730Z"
    }
   },
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank of LoRA matrices (lower = less memory)\n",
    "    lora_alpha=16,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.3,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    modules_to_save=[\"score\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.05,\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=True,\n",
    "    save_total_limit=1,\n",
    "    report_to=None\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=training_data[\"train\"].select(range(0, 500)),\n",
    "    eval_dataset=training_data[\"eval\"].select(range(0, 50)),\n",
    "    tokenizer=base_tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/tmp/ipykernel_85689/223733642.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4540\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 5.65 GiB of which 80.88 MiB is free. Process 79484 has 3.75 GiB memory in use. Including non-PyTorch memory, this process has 1.79 GiB memory in use. Of the allocated memory 1.61 GiB is allocated by PyTorch, and 77.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 44\u001B[0m\n\u001B[1;32m     20\u001B[0m training_args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[1;32m     21\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./checkpoints\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     22\u001B[0m     eval_strategy\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     33\u001B[0m     report_to\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     34\u001B[0m )\n\u001B[1;32m     36\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m     37\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m     38\u001B[0m     args\u001B[38;5;241m=\u001B[39mtraining_args,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     41\u001B[0m     tokenizer\u001B[38;5;241m=\u001B[39mbase_tokenizer,\n\u001B[1;32m     42\u001B[0m )\n\u001B[0;32m---> 44\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/trainer.py:2245\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2243\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   2244\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2245\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2246\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2247\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2248\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2249\u001B[0m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2250\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/trainer.py:2560\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2553\u001B[0m context \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2554\u001B[0m     functools\u001B[38;5;241m.\u001B[39mpartial(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mno_sync, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[1;32m   2555\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(batch_samples) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   2556\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mdistributed_type \u001B[38;5;241m!=\u001B[39m DistributedType\u001B[38;5;241m.\u001B[39mDEEPSPEED\n\u001B[1;32m   2557\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m contextlib\u001B[38;5;241m.\u001B[39mnullcontext\n\u001B[1;32m   2558\u001B[0m )\n\u001B[1;32m   2559\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[0;32m-> 2560\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2562\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2563\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   2564\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[1;32m   2565\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   2566\u001B[0m ):\n\u001B[1;32m   2567\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   2568\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/trainer.py:3736\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs, num_items_in_batch)\u001B[0m\n\u001B[1;32m   3733\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   3735\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[0;32m-> 3736\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3738\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n\u001B[1;32m   3739\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   3740\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   3741\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m   3742\u001B[0m ):\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/trainer.py:3801\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[0m\n\u001B[1;32m   3799\u001B[0m         loss_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_items_in_batch\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m num_items_in_batch\n\u001B[1;32m   3800\u001B[0m     inputs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mloss_kwargs}\n\u001B[0;32m-> 3801\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3802\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[1;32m   3803\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[1;32m   3804\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/accelerate/utils/operations.py:818\u001B[0m, in \u001B[0;36mconvert_outputs_to_fp32.<locals>.forward\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    817\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 818\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/accelerate/utils/operations.py:806\u001B[0m, in \u001B[0;36mConvertOutputsToFp32.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    805\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 806\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m convert_to_fp32(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/amp/autocast_mode.py:44\u001B[0m, in \u001B[0;36mautocast_decorator.<locals>.decorate_autocast\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mdecorate_autocast\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     43\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m autocast_instance:\n\u001B[0;32m---> 44\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/peft/peft_model.py:1757\u001B[0m, in \u001B[0;36mPeftModelForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001B[0m\n\u001B[1;32m   1755\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_enable_peft_forward_hooks(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   1756\u001B[0m         kwargs \u001B[38;5;241m=\u001B[39m {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mspecial_peft_forward_args}\n\u001B[0;32m-> 1757\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1758\u001B[0m \u001B[43m            \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1759\u001B[0m \u001B[43m            \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1760\u001B[0m \u001B[43m            \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1761\u001B[0m \u001B[43m            \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1762\u001B[0m \u001B[43m            \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1763\u001B[0m \u001B[43m            \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1764\u001B[0m \u001B[43m            \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1765\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1766\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1768\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m _get_batch_size(input_ids, inputs_embeds)\n\u001B[1;32m   1769\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m attention_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1770\u001B[0m     \u001B[38;5;66;03m# concat prompt attention mask\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/peft/tuners/tuners_utils.py:193\u001B[0m, in \u001B[0;36mBaseTuner.forward\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    192\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any):\n\u001B[0;32m--> 193\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:965\u001B[0m, in \u001B[0;36mcan_return_tuple.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    962\u001B[0m     set_attribute_for_modules(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_is_top_level_module\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    964\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 965\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    966\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_requested_to_return_tuple \u001B[38;5;129;01mor\u001B[39;00m (is_configured_to_return_tuple \u001B[38;5;129;01mand\u001B[39;00m is_top_level_module):\n\u001B[1;32m    967\u001B[0m         output \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mto_tuple()\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/deprecation.py:172\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action\u001B[38;5;241m.\u001B[39mNOTIFY, Action\u001B[38;5;241m.\u001B[39mNOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m--> 172\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:821\u001B[0m, in \u001B[0;36mLlamaForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001B[0m\n\u001B[1;32m    816\u001B[0m output_hidden_states \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    817\u001B[0m     output_hidden_states \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39moutput_hidden_states\n\u001B[1;32m    818\u001B[0m )\n\u001B[1;32m    820\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[0;32m--> 821\u001B[0m outputs: BaseModelOutputWithPast \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    822\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    823\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    824\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    825\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    826\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    827\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    828\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    829\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    830\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    831\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    832\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    834\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mlast_hidden_state\n\u001B[1;32m    835\u001B[0m \u001B[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:965\u001B[0m, in \u001B[0;36mcan_return_tuple.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    962\u001B[0m     set_attribute_for_modules(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_is_top_level_module\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m    964\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 965\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    966\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_requested_to_return_tuple \u001B[38;5;129;01mor\u001B[39;00m (is_configured_to_return_tuple \u001B[38;5;129;01mand\u001B[39;00m is_top_level_module):\n\u001B[1;32m    967\u001B[0m         output \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mto_tuple()\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:571\u001B[0m, in \u001B[0;36mLlamaModel.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001B[0m\n\u001B[1;32m    559\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m    560\u001B[0m         partial(decoder_layer\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mflash_attn_kwargs),\n\u001B[1;32m    561\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    568\u001B[0m         position_embeddings,\n\u001B[1;32m    569\u001B[0m     )\n\u001B[1;32m    570\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 571\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    572\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    573\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    574\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    575\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    576\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    577\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    578\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    579\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    580\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mflash_attn_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    581\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    583\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    585\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:334\u001B[0m, in \u001B[0;36mLlamaDecoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001B[0m\n\u001B[1;32m    332\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n\u001B[1;32m    333\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpost_attention_layernorm(hidden_states)\n\u001B[0;32m--> 334\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    335\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[1;32m    337\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (hidden_states,)\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:172\u001B[0m, in \u001B[0;36mLlamaMLP.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m--> 172\u001B[0m     down_proj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdown_proj(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mact_fn(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgate_proj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m) \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mup_proj(x))\n\u001B[1;32m    173\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m down_proj\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/peft/tuners/lora/bnb.py:500\u001B[0m, in \u001B[0;36mLinear4bit.forward\u001B[0;34m(self, x, *args, **kwargs)\u001B[0m\n\u001B[1;32m    494\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_layer(x, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    495\u001B[0m \u001B[38;5;66;03m# As per Tim Dettmers, for 4bit, we need to defensively clone here.\u001B[39;00m\n\u001B[1;32m    496\u001B[0m \u001B[38;5;66;03m# The reason is that in some cases, an error can occur that backprop\u001B[39;00m\n\u001B[1;32m    497\u001B[0m \u001B[38;5;66;03m# does not work on a manipulated view. This issue may be solved with\u001B[39;00m\n\u001B[1;32m    498\u001B[0m \u001B[38;5;66;03m# newer PyTorch versions but this would need extensive testing to be\u001B[39;00m\n\u001B[1;32m    499\u001B[0m \u001B[38;5;66;03m# sure.\u001B[39;00m\n\u001B[0;32m--> 500\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mresult\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclone\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    502\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m active_adapter \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mactive_adapters:\n\u001B[1;32m    503\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m active_adapter \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlora_A\u001B[38;5;241m.\u001B[39mkeys():\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 126.00 MiB. GPU 0 has a total capacity of 5.65 GiB of which 80.88 MiB is free. Process 79484 has 3.75 GiB memory in use. Including non-PyTorch memory, this process has 1.79 GiB memory in use. Of the allocated memory 1.61 GiB is allocated by PyTorch, and 77.53 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac720c0-5b0f-4bb0-95cd-c7a9a00d78d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5e91cb-9ab4-42dc-9a6b-c69429f21c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8781d2b8-cd68-4e62-9704-12c72973262f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./trainer-combined-logits-full-hd/tokenizer/tokenizer_config.json',\n",
       " './trainer-combined-logits-full-hd/tokenizer/special_tokens_map.json',\n",
       " './trainer-combined-logits-full-hd/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./trainer-combined-logits-full-hd/trainer\")\n",
    "base_tokenizer.save_pretrained(\"./trainer-combined-logits-full-hd/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6757f271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "\n",
    "def generate_prompt(\n",
    "    question: str,\n",
    "    context: str,\n",
    "    options: list[str]\n",
    ") -> str:\n",
    "    prompt = f\"\"\"You are an expert in multiple-choice questions. Your task is to select the best answer from the given options based on the provided context.\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Options:\n",
    "{options}\n",
    "\n",
    "Between A, B, C and D the best option is the letter\"\"\"\n",
    "    return prompt\n",
    "\n",
    "options_str = [\"A\",\"B\",\"C\",\"D\"]\n",
    "\n",
    "def format_options(options: list):\n",
    "    return '\\n'.join(f\"{options_str[i]}. {s}\" for i, s in enumerate(options))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6568b117-ac31-40a5-b8dc-b331d150c9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a13a2756ae243488895ae161a20d27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9997b9d33fd4c269de66d24a8fcb97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-3c3a879788dc56():   0%|          | 0.00/3.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c997d7458dd0430e914762725c0dfdeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds=load_dataset(\"juan-carvajal/maia-pln-2025-pubmed_QA_test_questions_contexts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdebcd4-dc4a-410a-894b-46d44959648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def format_context_as_response(context, k=5):\n",
    "    formatted_data = []\n",
    "    for c in context[:k]:\n",
    "        formatted_data.append(f\"Title: {c['title']}\\nAbstract: {c['content']}\")\n",
    "    return \"\\n---------\\n\".join(formatted_data)\n",
    "        \n",
    "\n",
    "possible_answers = [\" A\", \" B\", \" C\", \" D\",\" E\"]\n",
    "    \n",
    "option_logits = [\n",
    "        base_tokenizer.encode(option, add_special_tokens=False)[0]\n",
    "        for option in possible_answers\n",
    "]\n",
    "\n",
    "print(option_logits)\n",
    "\n",
    "base_model = model\n",
    "\n",
    "\n",
    "def predict(examples):\n",
    "    all_prompts = []\n",
    "    options_list = examples[\"option\"]\n",
    "    for i, (context, options,question) in enumerate(zip(examples[\"contexts\"], options_list,examples[\"question\"])):\n",
    "        context_text = format_context_as_response(context,1)\n",
    "        formatted_options = format_options(options)\n",
    "        prompt = generate_prompt(question, context_text, formatted_options)\n",
    "        #print(prompt)\n",
    "        all_prompts.append(prompt)\n",
    "    #outputs = pipe(all_prompts)\n",
    "    #print(all_prompts)\n",
    "    tokens=base_tokenizer(all_prompts, return_tensors=\"pt\",padding=True, truncation=True).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs=base_model(**tokens, do_sample=False,max_new_tokens=2,\n",
    "        top_p=None,\n",
    "        temperature=0)\n",
    "    \n",
    "    # Extract answers from model outputs\n",
    "    answers = []\n",
    "    print(outputs.logits.shape)\n",
    "    last_token_logits = outputs.logits\n",
    "    for i, sequence_logits in enumerate(last_token_logits):\n",
    "        attention_mask = tokens['attention_mask'][i]\n",
    "        last_token_pos = attention_mask.sum().item() - 1\n",
    "        \n",
    "        # Get logits for the last position\n",
    "        last_token_logits = sequence_logits[last_token_pos]\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        probs = torch.softmax(last_token_logits, dim=-1).tolist()\n",
    "        # Apply softmax to get probabilities\n",
    "        #probs = torch.softmax(sequence_logits, dim=-1).tolist()\n",
    "\n",
    "        # Calculate the probability for each option token\n",
    "        scores = []\n",
    "        for option_token_id in option_logits:\n",
    "            score = probs[option_token_id]\n",
    "            scores.append(score)\n",
    "\n",
    "        # Choose the option with the highest probability\n",
    "        chosen_option = scores.index(max(scores))\n",
    "        print(f\"Sample {i}: Scores = {scores}, Chosen = {chosen_option} ({possible_answers[chosen_option].strip()})\")\n",
    "\n",
    "        answers.append(chosen_option)\n",
    "    \n",
    "    # Add answers to the examples\n",
    "    examples[\"answer\"] = answers\n",
    "    \n",
    "    return examples\n",
    "\n",
    "tmp = ds['train']#.select(range(100))\n",
    "tmp = tmp.map(predict,batched=True,batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48c5effd-7c5a-427d-bb71-8a89c3fb48bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c21723ce95458a8e26c8766085051e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tmp.to_csv(\"results.csv\")\n",
    "df = pd.read_csv('results.csv')\n",
    "df['ID'] = df['id']\n",
    "\n",
    "df[['ID','answer']].to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e6fd5-1707-4f20-b0cb-940a50d38a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
