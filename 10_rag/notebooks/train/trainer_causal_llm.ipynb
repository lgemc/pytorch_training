{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12a67129-6c85-42ba-9efd-5c27cf761cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/pytorch_training/10_rag/src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed6db18fd76b2ff4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:31:29.030115Z",
     "start_time": "2025-05-24T03:31:26.719229Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# https://www.datacamp.com/tutorial/fine-tuning-llama-3-1\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "\n",
    "from data.q_and_a.train_and_eval import TrainAndEval\n",
    "from data.q_and_a.eval_with_answers import EvalWithAnswers\n",
    "from data.q_and_a.tokenized import TokenizedCausal\n",
    "from data.q_and_a.prompted import Prompted, to_transformers_dataset, index_to_answer\n",
    "from q_and_a.prompts import prompt_with_answer\n",
    "from models_.building.llama_tokenizer import load_tokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe255781f0b9c7d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:31:29.063309Z",
     "start_time": "2025-05-24T03:31:29.032995Z"
    }
   },
   "outputs": [],
   "source": [
    "train = TrainAndEval(\"../../data/pubmed_QA_train.json\")\n",
    "with_answers = EvalWithAnswers(train)\n",
    "prompted = Prompted(with_answers, prompt_with_answer)\n",
    "\n",
    "#train_data = to_transformers_dataset(prompted)\n",
    "\n",
    "test = TrainAndEval(\"../../data/pubmed_QA_eval.json\")\n",
    "test_with_answers = EvalWithAnswers(test)\n",
    "prompted_test = Prompted(test_with_answers, prompt_with_answer)\n",
    "#test_data = to_transformers_dataset(prompted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b3c8d2612d8f50b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:31:29.118013Z",
     "start_time": "2025-05-24T03:31:29.116415Z"
    }
   },
   "outputs": [],
   "source": [
    "# prompted[0][\"text\"] == prompted[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f7df54f3d19cf91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:31:29.549043Z",
     "start_time": "2025-05-24T03:31:29.159430Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af10fb6dc614d17f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:31:29.576774Z",
     "start_time": "2025-05-24T03:31:29.574399Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_tokenized = TokenizedCausal(tokenizer, prompted)\n",
    "test_data_tokenized = TokenizedCausal(tokenizer, prompted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38841be8f22ce937",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:31:29.624671Z",
     "start_time": "2025-05-24T03:31:29.617810Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([128000,   2675,    527,  ..., 128001, 128001, 128001]), 'attention_mask': tensor([1, 1, 1,  ..., 0, 0, 0]), 'labels': tensor([  2675,    527,    459,  ..., 128001, 128001,   -100])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a14f25912fac850",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:31:29.779009Z",
     "start_time": "2025-05-24T03:31:29.774561Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>You are an expert at answering multiple-choice questions. Given the context below, carefully read the question and select the single best answer from the options provided.\\n\\nContext:\\nThe rate of action of calcium on the electrical and mechanical responses of the crayfish muscle fibers. The effects of sudden changes in external Ca concentration on the time courses of the changes in size of the action potential and of the associated contraction in a single crayfish muscle fiber were investigated. Procaine-HCl was added to the bathing solution to make the muscle fiber excitable. The concentration of the divalent cations (Ca and Mg) was high enough to keep the threshold potential constant. In Ca-free solution, neither action potential nor contraction was observed. When the external Ca concentration was suddenly increased from 0 to 14 mM, the full sized action potentials were generated within several seconds, but the tensions recovered slowly in an exponential time course with the time constants of 15-40 sec depending on the muscle fiber radius. The tension recovery was further delayed by addition of Dextran to the bathing solution, and it was also slowed at temperatures as low as 4-5 degrees C. When the Ca concentration was changed from 14 mM to 0 mM, the decreased in action potential was slow rather than instantaneous. The delay in tension recovery was attributed to the diffusion time of Ca ions into the TTS, and it was suggested that the Ca entry through the TTS membranes was the first step in the excitation-contraction coupling of the crayfish muscle fibers. The diffusion coefficient of Ca ions inside the TTS was calculated from the recovery time of tension development. It was one order smaller than that in free solution.\\n\\nQuestion:\\nWhat did the study reveal about the role of external calcium concentration in the action potential and contraction recovery time of crayfish muscle fibers?\\n\\nOptions:\\nA. The study found that increasing external calcium concentration had no effect on the action potential or contraction recovery time in crayfish muscle fibers.\\nB. The research concluded that magnesium ions play a more significant role than calcium in the action potential and contraction recovery of crayfish muscle fibers.\\nC. The study investigated how changes in external calcium concentration affect the action potential and contraction recovery time in crayfish muscle fibers, revealing that calcium entry through TTS membranes is crucial for excitation-contraction coupling.\\nD. The investigation revealed that external calcium concentration only affects the resting potential of crayfish muscle fibers, not the action potential or contraction recovery.\\n\\nPlease respond only with the letter (A, B, C, or D) of the best option.\\n\\nThe correct answer is: C<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(train_data_tokenized[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "405001343c775eb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:31:31.152589Z",
     "start_time": "2025-05-24T03:31:31.149660Z"
    }
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebd87986a3dc4c95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:31:33.271797Z",
     "start_time": "2025-05-24T03:31:31.442588Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"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\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-1B\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"float16\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "tokenizer = load_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e887d6c-b795-4e16-91c7-1d42bf0ed466",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:05:57.119225Z",
     "start_time": "2025-05-24T03:05:57.100002Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "70e6dbf581ac4082",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:05:57.150311Z",
     "start_time": "2025-05-24T03:05:57.146903Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(test, model, tokenizer):\n",
    "    y_pred = []\n",
    "    text_generated = []\n",
    "    categories = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "    for i in tqdm(range(len(test))):\n",
    "        prompt = test[i]\n",
    "        pipe = pipeline(task=\"text-generation\",\n",
    "                        model=model,\n",
    "                        tokenizer=tokenizer,\n",
    "                        max_new_tokens=2,\n",
    "                        temperature=0.1)\n",
    "\n",
    "        result = pipe(prompt)\n",
    "        answer = result[0]['generated_text'].split(\"The correct answer is:\")[-1].strip()\n",
    "        text_generated.append(answer)\n",
    "        # Determine the predicted category\n",
    "        for category in categories:\n",
    "            if category.lower() in answer.lower():\n",
    "                y_pred.append(category)\n",
    "                break\n",
    "        else:\n",
    "            y_pred.append(\"none\")\n",
    "\n",
    "    return y_pred, text_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "179acbe7104598c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:06:05.709242Z",
     "start_time": "2025-05-24T03:05:57.194117Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred, answers \u001b[38;5;241m=\u001b[39m predict(\u001b[43mtrain_data\u001b[49m\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m)), model, tokenizer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "y_pred, answers = predict(train_data.select(range(100)), model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a631a7e47ec5c1f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:06:05.734739Z",
     "start_time": "2025-05-24T03:06:05.731946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C. The',\n",
       " 'D. In',\n",
       " 'B. Eight',\n",
       " 'D. In',\n",
       " 'C.',\n",
       " 'C. The',\n",
       " 'C. Long',\n",
       " 'A. Str',\n",
       " 'D. Th',\n",
       " 'A. The',\n",
       " 'B. In',\n",
       " 'A',\n",
       " 'A. Bac',\n",
       " 'D.',\n",
       " 'A. A',\n",
       " 'C. The',\n",
       " 'C. A',\n",
       " 'D. A',\n",
       " 'A',\n",
       " 'D. T',\n",
       " 'C. A',\n",
       " 'B. A',\n",
       " 'C. Dep',\n",
       " 'C. The',\n",
       " 'C. Hern',\n",
       " 'A.',\n",
       " 'A',\n",
       " 'B. Th',\n",
       " 'A. In',\n",
       " 'B. C',\n",
       " 'B. A',\n",
       " 'A',\n",
       " 'D. The',\n",
       " 'B',\n",
       " 'A. A',\n",
       " 'C. The',\n",
       " 'B. In',\n",
       " 'C. A',\n",
       " 'C. Patients',\n",
       " 'C. Retro',\n",
       " 'D. The',\n",
       " 'C. Dip',\n",
       " 'C. Three',\n",
       " 'C. After',\n",
       " 'B. From',\n",
       " 'C. A',\n",
       " 'D. Post',\n",
       " 'A. Th',\n",
       " 'D. Hist',\n",
       " 'C. The',\n",
       " 'C. The',\n",
       " 'C. B',\n",
       " 'B. Pro',\n",
       " 'C. The',\n",
       " 'A',\n",
       " 'A. Ph',\n",
       " 'B. Hal',\n",
       " 'B',\n",
       " 'D. A',\n",
       " 'B. A',\n",
       " 'D. A',\n",
       " 'A. Three',\n",
       " 'A. A',\n",
       " 'B',\n",
       " 'A. Two',\n",
       " 'B. N',\n",
       " 'C. Sup',\n",
       " 'C. Pregnancy',\n",
       " 'D.',\n",
       " 'C',\n",
       " 'B. In',\n",
       " 'A',\n",
       " 'A. HB',\n",
       " 'B. Partial',\n",
       " 'A. A',\n",
       " 'D. Ser',\n",
       " 'B',\n",
       " 'B',\n",
       " 'C. During',\n",
       " 'B. Mon',\n",
       " 'C. Surgical',\n",
       " 'A. Men',\n",
       " 'B. The',\n",
       " 'B. The',\n",
       " 'C. Two',\n",
       " 'A. Children',\n",
       " 'D. Cam',\n",
       " 'B. In',\n",
       " 'B. Bio',\n",
       " 'C. The',\n",
       " 'D. The',\n",
       " 'D. In',\n",
       " 'D. A',\n",
       " 'D. A',\n",
       " 'B. The',\n",
       " 'B. Approximately',\n",
       " 'D. The',\n",
       " 'C. Heat',\n",
       " 'C. A',\n",
       " 'D. Computer']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6b5362744b28b37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:06:05.782826Z",
     "start_time": "2025-05-24T03:06:05.779566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C',\n",
       " 'D',\n",
       " 'B',\n",
       " 'D',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'A',\n",
       " 'D',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'A',\n",
       " 'D',\n",
       " 'A',\n",
       " 'C',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'D',\n",
       " 'A',\n",
       " 'A',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'A',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'A',\n",
       " 'D',\n",
       " 'B',\n",
       " 'A',\n",
       " 'C',\n",
       " 'B',\n",
       " 'A',\n",
       " 'A',\n",
       " 'C',\n",
       " 'D',\n",
       " 'C',\n",
       " 'C',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'D',\n",
       " 'A',\n",
       " 'D',\n",
       " 'C',\n",
       " 'C',\n",
       " 'B',\n",
       " 'B',\n",
       " 'C',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'A',\n",
       " 'D',\n",
       " 'C',\n",
       " 'B',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'D',\n",
       " 'B',\n",
       " 'B',\n",
       " 'C',\n",
       " 'B',\n",
       " 'A',\n",
       " 'A',\n",
       " 'B',\n",
       " 'B',\n",
       " 'C',\n",
       " 'A',\n",
       " 'A',\n",
       " 'B',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'D',\n",
       " 'A',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'D',\n",
       " 'A',\n",
       " 'A',\n",
       " 'C']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "246550ef41ec4400",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:06:05.830806Z",
     "start_time": "2025-05-24T03:06:05.826330Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred, answers = predict(train_data.select(range(100)), model, tokenizer)\n",
    "y_true = []\n",
    "for i in range(len(y_pred)):\n",
    "    y_true.append(train_data[i][\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb2688fddffaf85c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:06:05.876968Z",
     "start_time": "2025-05-24T03:06:05.872976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'D',\n",
       " 'C',\n",
       " 'B',\n",
       " 'D',\n",
       " 'D',\n",
       " 'D',\n",
       " 'C',\n",
       " 'A',\n",
       " 'C',\n",
       " 'C',\n",
       " 'C',\n",
       " 'A',\n",
       " 'D',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'A',\n",
       " 'D',\n",
       " 'D',\n",
       " 'C',\n",
       " 'A',\n",
       " 'A',\n",
       " 'C',\n",
       " 'B',\n",
       " 'D',\n",
       " 'D',\n",
       " 'C',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'D',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'B',\n",
       " 'A',\n",
       " 'D',\n",
       " 'D',\n",
       " 'B',\n",
       " 'D',\n",
       " 'B',\n",
       " 'D',\n",
       " 'A',\n",
       " 'C',\n",
       " 'A',\n",
       " 'C',\n",
       " 'D',\n",
       " 'B',\n",
       " 'C',\n",
       " 'A',\n",
       " 'D',\n",
       " 'A',\n",
       " 'C',\n",
       " 'C',\n",
       " 'A',\n",
       " 'A',\n",
       " 'B',\n",
       " 'A',\n",
       " 'B',\n",
       " 'B',\n",
       " 'D',\n",
       " 'D',\n",
       " 'C',\n",
       " 'A',\n",
       " 'C',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'D',\n",
       " 'D',\n",
       " 'A',\n",
       " 'B',\n",
       " 'B',\n",
       " 'D',\n",
       " 'A',\n",
       " 'A',\n",
       " 'C',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'B',\n",
       " 'D',\n",
       " 'C',\n",
       " 'D',\n",
       " 'B',\n",
       " 'C',\n",
       " 'B',\n",
       " 'A',\n",
       " 'A',\n",
       " 'C',\n",
       " 'B',\n",
       " 'A',\n",
       " 'A',\n",
       " 'A',\n",
       " 'C',\n",
       " 'B']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0189fe5a-d787-4ae9-a7a9-1809c70d0116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9e25532d5d9f00f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:06:05.934556Z",
     "start_time": "2025-05-24T03:06:05.922626Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mConfusion Matrix:\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mprint\u001b[39m(conf_matrix)\n\u001b[0;32m---> 37\u001b[0m evaluate(\u001b[43my_true\u001b[49m, y_pred)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_true' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "def evaluate(y_true, y_pred):\n",
    "    labels = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    mapping = {label: idx for idx, label in enumerate(labels)}\n",
    "\n",
    "    def map_func(x):\n",
    "        return mapping.get(x, -1)  # Map to -1 if not found, but should not occur with correct data\n",
    "\n",
    "    y_true_mapped = np.vectorize(map_func)(y_true)\n",
    "    y_pred_mapped = np.vectorize(map_func)(y_pred)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true_mapped, y_pred=y_pred_mapped)\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "\n",
    "    # Generate accuracy report\n",
    "    unique_labels = set(y_true_mapped)  # Get unique labels\n",
    "\n",
    "    for label in unique_labels:\n",
    "        label_indices = [i for i in range(len(y_true_mapped)) if y_true_mapped[i] == label]\n",
    "        label_y_true = [y_true_mapped[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred_mapped[i] for i in label_indices]\n",
    "        label_accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for label {labels[label]}: {label_accuracy:.3f}')\n",
    "\n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true_mapped, y_pred=y_pred_mapped, target_names=labels, labels=list(range(len(labels))))\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "\n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true=y_true_mapped, y_pred=y_pred_mapped, labels=list(range(len(labels))))\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n",
    "evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60cfae3d58739ed9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:06:05.978260Z",
     "start_time": "2025-05-24T03:06:05.973456Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['k_proj', 'o_proj', 'up_proj', 'gate_proj', 'down_proj', 'v_proj', 'q_proj']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "modules = find_all_linear_names(model)\n",
    "modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3b095ce16362cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:31:39.965379Z",
     "start_time": "2025-05-24T03:31:39.903202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,636,096 || all params: 1,241,450,496 || trainable%: 0.4540\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank of LoRA matrices (lower = less memory)\n",
    "    lora_alpha=16,\n",
    "    target_modules=modules,  # Depends on model architecture\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f362b43d-fa7b-4e32-b5a7-043276f89dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "train_tokenized_subset = Subset(train_data_tokenized, range(0, 200))\n",
    "test_tokenized_subset = Subset(test_data_tokenized, range(0, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58ffb6caf8827a9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T03:31:45.242571Z",
     "start_time": "2025-05-24T03:31:43.661153Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/tmp/ipykernel_2279/3683742687.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/home/ubuntu/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/home/ubuntu/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='604' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 604/1500 08:48 < 13:06, 1.14 it/s, Epoch 6.03/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.478700</td>\n",
       "      <td>1.068104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.895600</td>\n",
       "      <td>0.843888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.769600</td>\n",
       "      <td>0.777242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.719000</td>\n",
       "      <td>0.747380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.730565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.668500</td>\n",
       "      <td>0.719999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/home/ubuntu/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n",
      "/home/ubuntu/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/home/ubuntu/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n",
      "/home/ubuntu/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/home/ubuntu/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n",
      "/home/ubuntu/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/home/ubuntu/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n",
      "/home/ubuntu/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/home/ubuntu/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n",
      "/home/ubuntu/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:271: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  arr = np.array(obj)\n",
      "/home/ubuntu/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:271: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(obj)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 27\u001b[0m\n\u001b[1;32m      3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      4\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./checkpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     eval_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     19\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     25\u001b[0m )\n\u001b[0;32m---> 27\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/trainer.py:2562\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m   2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m-> 2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2566\u001b[0m ):\n\u001b[1;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2569\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    #train_dataset=train_data_tokenized,\n",
    "    #eval_dataset=test_data_tokenized,\n",
    "    train_dataset = Subset(train_data_tokenized, range(0, 200)),\n",
    "    eval_dataset = Subset(test_data_tokenized, range(0, 20)),\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dfbaef56-24ab-48ba-b4f1-57a0440cbba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0                                                                                                                                                                   | 0/100 [00:00<?, ?it/s]\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "  0%|                                                                                                                                                                                      | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"tuple\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m y_pred, answers \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSubset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompted_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m y_true \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_pred)):\n",
      "Cell \u001b[0;32mIn[28], line 14\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(test, model, tokenizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m test[i]\n\u001b[1;32m      8\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m                 model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     10\u001b[0m                 tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     11\u001b[0m                 max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     12\u001b[0m                 temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m answer \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe correct answer is:\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     16\u001b[0m text_generated\u001b[38;5;241m.\u001b[39mappend(answer)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:287\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/pipelines/base.py:1379\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1372\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1373\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         )\n\u001b[1;32m   1377\u001b[0m     )\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/pipelines/base.py:1385\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m-> 1385\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1386\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1387\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:325\u001b[0m, in \u001b[0;36mTextGenerationPipeline.preprocess\u001b[0;34m(self, prompt_text, prefix, handle_long_generation, add_special_tokens, truncation, padding, max_length, continue_final_message, **generate_kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(\n\u001b[1;32m    317\u001b[0m         prompt_text\u001b[38;5;241m.\u001b[39mmessages,\n\u001b[1;32m    318\u001b[0m         add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m continue_final_message,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs,\n\u001b[1;32m    323\u001b[0m     )\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\u001b[43mprefix\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprompt_text\u001b[49m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs)\n\u001b[1;32m    327\u001b[0m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m prompt_text\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle_long_generation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhole\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"tuple\") to str"
     ]
    }
   ],
   "source": [
    "y_pred, answers = predict(Subset(prompted_test, range(100)), model, tokenizer)\n",
    "y_true = []\n",
    "for i in range(len(y_pred)):\n",
    "    y_true.append(train_data[i][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d162f20ef30d6267",
   "metadata": {},
   "source": [
    "# First, load the data\n",
    "\n",
    "We are going to load the data used for train or modify our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b421e8d542499a9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:00:47.689765Z",
     "start_time": "2025-05-23T14:00:47.686840Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tokenized(Dataset):\n",
    "    def __init__(self, tokenizer, dataset: Prompted, max_length=2000):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataset = dataset\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        text, answer = self.dataset[idx]\n",
    "\n",
    "        result = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",)\n",
    "        labels = torch.tensor(answer, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": result[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": result[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": labels,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:00:48.421730Z",
     "start_time": "2025-05-23T14:00:48.047177Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "train_dataset = TrainAndEval(\"../../data/pubmed_QA_train.json\")\n",
    "test_dataset = TrainAndEval(\"../../data/pubmed_QA_eval.json\")\n",
    "train_with_answers = EvalWithAnswers(train_dataset)\n",
    "test_with_answers = EvalWithAnswers(test_dataset)\n",
    "train_prompted= Prompted(train_with_answers, prompt)\n",
    "test_prompted = Prompted(test_with_answers, prompt)\n",
    "train_tokenized = Tokenized(tokenizer, train_prompted)\n",
    "test_tokenized = Tokenized(tokenizer, test_prompted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d0fe24ee7e060bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:00:48.431076Z",
     "start_time": "2025-05-23T14:00:48.427312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16890, 5000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tokenized), len(test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80862c4188d6abcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:00:48.560822Z",
     "start_time": "2025-05-23T14:00:48.557575Z"
    }
   },
   "outputs": [],
   "source": [
    "# per now use a subset\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "train_tokenized = Subset(train_tokenized, range(0, 2000))\n",
    "test_tokenized = Subset(test_tokenized, range(0, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42109f869b4cbf7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:00:50.601815Z",
     "start_time": "2025-05-23T14:00:48.701842Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048, padding_idx=128001)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (score): Linear(in_features=2048, out_features=4, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=4,\n",
    "    load_in_8bit=True,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aa5a9bda5cccd98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:00:50.617322Z",
     "start_time": "2025-05-23T14:00:50.614147Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad non required on:model.embed_tokens.weight\n",
      "grad non required on:model.layers.0.self_attn.q_proj.weight\n",
      "grad non required on:model.layers.0.self_attn.k_proj.weight\n",
      "grad non required on:model.layers.0.self_attn.v_proj.weight\n",
      "grad non required on:model.layers.0.self_attn.o_proj.weight\n",
      "grad non required on:model.layers.0.mlp.gate_proj.weight\n",
      "grad non required on:model.layers.0.mlp.up_proj.weight\n",
      "grad non required on:model.layers.0.mlp.down_proj.weight\n",
      "grad non required on:model.layers.0.input_layernorm.weight\n",
      "grad non required on:model.layers.0.post_attention_layernorm.weight\n",
      "grad non required on:model.layers.1.self_attn.q_proj.weight\n",
      "grad non required on:model.layers.1.self_attn.k_proj.weight\n",
      "grad non required on:model.layers.1.self_attn.v_proj.weight\n",
      "grad non required on:model.layers.1.self_attn.o_proj.weight\n",
      "grad non required on:model.layers.1.mlp.gate_proj.weight\n",
      "grad non required on:model.layers.1.mlp.up_proj.weight\n",
      "grad non required on:model.layers.1.mlp.down_proj.weight\n",
      "grad non required on:model.layers.1.input_layernorm.weight\n",
      "grad non required on:model.layers.1.post_attention_layernorm.weight\n",
      "grad non required on:model.layers.2.self_attn.q_proj.weight\n",
      "grad non required on:model.layers.2.self_attn.k_proj.weight\n",
      "grad non required on:model.layers.2.self_attn.v_proj.weight\n",
      "grad non required on:model.layers.2.self_attn.o_proj.weight\n",
      "grad non required on:model.layers.2.mlp.gate_proj.weight\n",
      "grad non required on:model.layers.2.mlp.up_proj.weight\n",
      "grad non required on:model.layers.2.mlp.down_proj.weight\n",
      "grad non required on:model.layers.2.input_layernorm.weight\n",
      "grad non required on:model.layers.2.post_attention_layernorm.weight\n",
      "grad non required on:model.layers.3.self_attn.q_proj.weight\n",
      "grad non required on:model.layers.3.self_attn.k_proj.weight\n",
      "grad non required on:model.layers.3.self_attn.v_proj.weight\n",
      "grad non required on:model.layers.3.self_attn.o_proj.weight\n",
      "grad non required on:model.layers.3.mlp.gate_proj.weight\n",
      "grad non required on:model.layers.3.mlp.up_proj.weight\n",
      "grad non required on:model.layers.3.mlp.down_proj.weight\n",
      "grad non required on:model.layers.3.input_layernorm.weight\n",
      "grad non required on:model.layers.3.post_attention_layernorm.weight\n",
      "grad non required on:model.layers.4.self_attn.q_proj.weight\n",
      "grad non required on:model.layers.4.self_attn.k_proj.weight\n",
      "grad non required on:model.layers.4.self_attn.v_proj.weight\n",
      "grad non required on:model.layers.4.self_attn.o_proj.weight\n",
      "grad non required on:model.layers.4.mlp.gate_proj.weight\n",
      "grad non required on:model.layers.4.mlp.up_proj.weight\n",
      "grad non required on:model.layers.4.mlp.down_proj.weight\n",
      "grad non required on:model.layers.4.input_layernorm.weight\n",
      "grad non required on:model.layers.4.post_attention_layernorm.weight\n",
      "grad non required on:model.layers.5.self_attn.q_proj.weight\n",
      "grad non required on:model.layers.5.self_attn.k_proj.weight\n",
      "grad non required on:model.layers.5.self_attn.v_proj.weight\n",
      "grad non required on:model.layers.5.self_attn.o_proj.weight\n",
      "grad non required on:model.layers.5.mlp.gate_proj.weight\n",
      "grad non required on:model.layers.5.mlp.up_proj.weight\n",
      "grad non required on:model.layers.5.mlp.down_proj.weight\n",
      "grad non required on:model.layers.5.input_layernorm.weight\n",
      "grad non required on:model.layers.5.post_attention_layernorm.weight\n",
      "grad non required on:model.layers.6.self_attn.q_proj.weight\n",
      "grad non required on:model.layers.6.self_attn.k_proj.weight\n",
      "grad non required on:model.layers.6.self_attn.v_proj.weight\n",
      "grad non required on:model.layers.6.self_attn.o_proj.weight\n",
      "grad non required on:model.layers.6.mlp.gate_proj.weight\n",
      "grad non required on:model.layers.6.mlp.up_proj.weight\n",
      "grad non required on:model.layers.6.mlp.down_proj.weight\n",
      "grad non required on:model.layers.6.input_layernorm.weight\n",
      "grad non required on:model.layers.6.post_attention_layernorm.weight\n",
      "grad non required on:model.layers.7.self_attn.q_proj.weight\n",
      "grad non required on:model.layers.7.self_attn.k_proj.weight\n",
      "grad non required on:model.layers.7.self_attn.v_proj.weight\n",
      "grad non required on:model.layers.7.self_attn.o_proj.weight\n",
      "grad non required on:model.layers.7.mlp.gate_proj.weight\n",
      "grad non required on:model.layers.7.mlp.up_proj.weight\n",
      "grad non required on:model.layers.7.mlp.down_proj.weight\n",
      "grad non required on:model.layers.7.input_layernorm.weight\n",
      "grad non required on:model.layers.7.post_attention_layernorm.weight\n",
      "grad non required on:model.layers.8.self_attn.q_proj.weight\n",
      "grad non required on:model.layers.8.self_attn.k_proj.weight\n",
      "grad non required on:model.layers.8.self_attn.v_proj.weight\n",
      "grad non required on:model.layers.8.self_attn.o_proj.weight\n",
      "grad non required on:model.layers.8.mlp.gate_proj.weight\n",
      "grad non required on:model.layers.8.mlp.up_proj.weight\n",
      "grad non required on:model.layers.8.mlp.down_proj.weight\n",
      "grad non required on:model.layers.8.input_layernorm.weight\n",
      "grad non required on:model.layers.8.post_attention_layernorm.weight\n",
      "grad non required on:model.layers.9.self_attn.q_proj.weight\n",
      "grad non required on:model.layers.9.self_attn.k_proj.weight\n",
      "grad non required on:model.layers.9.self_attn.v_proj.weight\n",
      "grad non required on:model.layers.9.self_attn.o_proj.weight\n",
      "grad non required on:model.layers.9.mlp.gate_proj.weight\n",
      "grad non required on:model.layers.9.mlp.up_proj.weight\n",
      "grad non required on:model.layers.9.mlp.down_proj.weight\n",
      "grad non required on:model.layers.9.input_layernorm.weight\n",
      "grad non required on:model.layers.9.post_attention_layernorm.weight\n",
      "grad non required on:model.layers.10.self_attn.q_proj.weight\n",
      "grad non required on:model.layers.10.self_attn.k_proj.weight\n",
      "grad non required on:model.layers.10.self_attn.v_proj.weight\n",
      "grad non required on:model.layers.10.self_attn.o_proj.weight\n",
      "grad non required on:model.layers.10.mlp.gate_proj.weight\n",
      "grad non required on:model.layers.10.mlp.up_proj.weight\n",
      "grad non required on:model.layers.10.mlp.down_proj.weight\n",
      "grad non required on:model.layers.10.input_layernorm.weight\n",
      "grad non required on:model.layers.10.post_attention_layernorm.weight\n",
      "grad non required on:model.layers.11.self_attn.q_proj.weight\n",
      "grad non required on:model.layers.11.self_attn.k_proj.weight\n",
      "grad non required on:model.layers.11.self_attn.v_proj.weight\n",
      "grad non required on:model.layers.11.self_attn.o_proj.weight\n",
      "grad non required on:model.layers.11.mlp.gate_proj.weight\n",
      "grad non required on:model.layers.11.mlp.up_proj.weight\n",
      "grad non required on:model.layers.11.mlp.down_proj.weight\n",
      "grad non required on:model.layers.11.input_layernorm.weight\n",
      "grad non required on:model.layers.11.post_attention_layernorm.weight\n",
      "grad non required on:model.layers.12.self_attn.q_proj.weight\n",
      "grad non required on:model.layers.12.self_attn.k_proj.weight\n",
      "grad non required on:model.layers.12.self_attn.v_proj.weight\n",
      "grad non required on:model.layers.12.self_attn.o_proj.weight\n",
      "grad non required on:model.layers.12.mlp.gate_proj.weight\n",
      "grad non required on:model.layers.12.mlp.up_proj.weight\n",
      "grad non required on:model.layers.12.mlp.down_proj.weight\n",
      "grad non required on:model.layers.12.input_layernorm.weight\n",
      "grad non required on:model.layers.12.post_attention_layernorm.weight\n",
      "grad non required on:model.layers.13.self_attn.q_proj.weight\n",
      "grad non required on:model.layers.13.self_attn.k_proj.weight\n",
      "grad non required on:model.layers.13.self_attn.v_proj.weight\n",
      "grad non required on:model.layers.13.self_attn.o_proj.weight\n",
      "grad non required on:model.layers.13.mlp.gate_proj.weight\n",
      "grad non required on:model.layers.13.mlp.up_proj.weight\n",
      "grad non required on:model.layers.13.mlp.down_proj.weight\n",
      "grad non required on:model.layers.13.input_layernorm.weight\n",
      "grad non required on:model.layers.13.post_attention_layernorm.weight\n",
      "grad non required on:model.layers.14.self_attn.q_proj.weight\n",
      "grad non required on:model.layers.14.self_attn.k_proj.weight\n",
      "grad non required on:model.layers.14.self_attn.v_proj.weight\n",
      "grad non required on:model.layers.14.self_attn.o_proj.weight\n",
      "grad non required on:model.layers.14.mlp.gate_proj.weight\n",
      "grad non required on:model.layers.14.mlp.up_proj.weight\n",
      "grad non required on:model.layers.14.mlp.down_proj.weight\n",
      "grad non required on:model.layers.14.input_layernorm.weight\n",
      "grad non required on:model.layers.14.post_attention_layernorm.weight\n",
      "grad non required on:model.layers.15.self_attn.q_proj.weight\n",
      "grad non required on:model.layers.15.self_attn.k_proj.weight\n",
      "grad non required on:model.layers.15.self_attn.v_proj.weight\n",
      "grad non required on:model.layers.15.self_attn.o_proj.weight\n",
      "grad non required on:model.layers.15.mlp.gate_proj.weight\n",
      "grad non required on:model.layers.15.mlp.up_proj.weight\n",
      "grad non required on:model.layers.15.mlp.down_proj.weight\n",
      "grad non required on:model.layers.15.input_layernorm.weight\n",
      "grad non required on:model.layers.15.post_attention_layernorm.weight\n",
      "grad non required on:model.norm.weight\n",
      "requires grad: score.weight\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"score\" not in name:\n",
    "        print(f\"grad non required on:{name}\")\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        print(f\"requires grad: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb9ff885cdf1dd8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:00:50.737648Z",
     "start_time": "2025-05-23T14:00:50.675784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 860,160 || all params: 1,236,682,752 || trainable%: 0.0696\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank of LoRA matrices (lower = less memory)\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Depends on model architecture\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    modules_to_save=[\"score\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e02fc05c8e95d278",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:00:52.319970Z",
     "start_time": "2025-05-23T14:00:50.752383Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22690/3351656164.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 5.65 GiB of which 64.00 MiB is free. Including non-PyTorch memory, this process has 5.56 GiB memory in use. Of the allocated memory 5.39 GiB is allocated by PyTorch, and 63.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m\n\u001b[1;32m      3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m      4\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./checkpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     eval_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     19\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     23\u001b[0m )\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/trainer.py:2560\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2553\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2554\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2556\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2557\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2558\u001b[0m )\n\u001b[1;32m   2559\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2560\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2563\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2564\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2565\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2566\u001b[0m ):\n\u001b[1;32m   2567\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2568\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/trainer.py:3736\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3733\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3735\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3736\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3738\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3740\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3741\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3742\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/trainer.py:3801\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3799\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3800\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3801\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3802\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/accelerate/utils/operations.py:818\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/accelerate/utils/operations.py:806\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/peft/peft_model.py:2352\u001b[0m, in \u001b[0;36mPeftModelForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mPOLY:\n\u001b[1;32m   2351\u001b[0m             kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task_ids\n\u001b[0;32m-> 2352\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2353\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2354\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2355\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2356\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2357\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2358\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2359\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2360\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2361\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2363\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   2364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2365\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/peft/tuners/tuners_utils.py:193\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:904\u001b[0m, in \u001b[0;36mLlamaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(LLAMA_INPUTS_DOCSTRING)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    895\u001b[0m     output_hidden_states: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SequenceClassifierOutputWithPast:\n\u001b[1;32m    897\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;124;03m    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;124;03m        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;124;03m        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;124;03m        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 904\u001b[0m     transformer_outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    914\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    915\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:571\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    560\u001b[0m         partial(decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs),\n\u001b[1;32m    561\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m         position_embeddings,\n\u001b[1;32m    569\u001b[0m     )\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 571\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:334\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    333\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 334\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    337\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:172\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 172\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/bitsandbytes/nn/modules.py:990\u001b[0m, in \u001b[0;36mLinear8bitLt.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype:\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 990\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mhas_fp16_weights \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCB\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:509\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(A, B, out, state, threshold, bias)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m threshold \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    508\u001b[0m     state\u001b[38;5;241m.\u001b[39mthreshold \u001b[38;5;241m=\u001b[39m threshold\n\u001b[0;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul8bitLt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:373\u001b[0m, in \u001b[0;36mMatMul8bitLt.forward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# Dequantize matmul result\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m bias\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;66;03m# we apply the fused bias here\u001b[39;00m\n\u001b[0;32m--> 373\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint8_mm_dequant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSCA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSCB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(A\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# apply bias separately\u001b[39;00m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;66;03m# TODO: Fused bias for fp32/bf16?\u001b[39;00m\n\u001b[1;32m    376\u001b[0m     output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mint8_mm_dequant(out32, SCA, state\u001b[38;5;241m.\u001b[39mSCB, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(A\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39madd_(bias)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/bitsandbytes/functional.py:2405\u001b[0m, in \u001b[0;36mint8_mm_dequant\u001b[0;34m(A, row_stats, col_stats, out, bias)\u001b[0m\n\u001b[1;32m   2402\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m bias\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16\n\u001b[1;32m   2404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2405\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2407\u001b[0m ptrA \u001b[38;5;241m=\u001b[39m get_ptr(A)\n\u001b[1;32m   2408\u001b[0m ptrOut \u001b[38;5;241m=\u001b[39m get_ptr(out)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 5.65 GiB of which 64.00 MiB is free. Including non-PyTorch memory, this process has 5.56 GiB memory in use. Of the allocated memory 5.39 GiB is allocated by PyTorch, and 63.91 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=test_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a3208d266f14f41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:00:52.322137188Z",
     "start_time": "2025-05-18T21:33:06.264229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU Name: NVIDIA H100 80GB HBM3\n",
      "Supports FP16: (9, 0)\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "print(\"Supports FP16:\", torch.cuda.get_device_capability(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13bfee5a3f760cb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:00:52.326318136Z",
     "start_time": "2025-05-18T21:33:06.311901Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./last-checkpoint/tokenizer/tokenizer_config.json',\n",
       " './last-checkpoint/tokenizer/special_tokens_map.json',\n",
       " './last-checkpoint/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./last-checkpoint/trainer\")\n",
    "tokenizer.save_pretrained(\"./last-checkpoint/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f218db450583d652",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:00:52.326850040Z",
     "start_time": "2025-05-18T21:33:06.385113Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./last-checkpoint/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e41c6d26d1a1fcf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:01:07.712296Z",
     "start_time": "2025-05-23T14:01:05.579953Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d091af245c43c294a30a2a123b31b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "\n",
    "from data.q_and_a.train_and_eval import TrainAndEval\n",
    "from data.q_and_a.eval_with_answers import EvalWithAnswers\n",
    "\n",
    "from models_.building.llama_tokenizer import  load_tokenizer\n",
    "\n",
    "from data.pubmed.from_json import FromJsonDataset\n",
    "from data.pubmed.contents import ContentsDataset\n",
    "\n",
    "from storage.faiss_ import FaissStorage\n",
    "\n",
    "from rag.tokenization.llama import build_tokenizer_function\n",
    "from rag.quering import build_querier\n",
    "import os\n",
    "from q_and_a.forward import build_enhanced_forwarder\n",
    "from q_and_a.prompts import prompt\n",
    "from q_and_a.picking.from_logits import build_from_logits\n",
    "from q_and_a.eval import evaluate\n",
    "from q_and_a.forward import build_forwarder\n",
    "\n",
    "train = TrainAndEval(\"../../data/pubmed_QA_train.json\")\n",
    "evaluationData = TrainAndEval(\"../../data/pubmed_QA_eval.json\")\n",
    "evaluateWithAnswers = EvalWithAnswers(evaluationData)\n",
    "\n",
    "augmented_data = FromJsonDataset(json_file=\"../../data/pubmed_500K.json\")\n",
    "augmented_data = ContentsDataset(augmented_data)\n",
    "\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\n",
    "storage = FaissStorage(\n",
    "    dimension=800,\n",
    ")\n",
    "\n",
    "storage.load(\"../../outputs/store/pubmed_500K.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c1dd0480019f2ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:01:26.564694Z",
     "start_time": "2025-05-23T14:01:25.962681Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_rag = load_tokenizer()\n",
    "tokenizer_fn = build_tokenizer_function(tokenizer_rag)\n",
    "\n",
    "querier = build_querier(storage, augmented_data, tokenizer_fn)\n",
    "storage = FaissStorage(\n",
    "    dimension=800,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40447f2f-446f-4b8d-9d19-bb2fb1774973",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:01:27.238001Z",
     "start_time": "2025-05-23T14:01:27.235930Z"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddff1133-aa61-48ba-8f46-185ffbffd031",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:01:28.057317Z",
     "start_time": "2025-05-23T14:01:27.740008Z"
    }
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 5.65 GiB of which 8.00 MiB is free. Including non-PyTorch memory, this process has 5.62 GiB memory in use. Of the allocated memory 5.44 GiB is allocated by PyTorch, and 67.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpick_from_classifier\u001b[39m(out):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39margmax(out\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 19\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforward_and_get_arg_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpicker_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpick_from_classifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluateWithAnswers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Try/pytorch_training/10_rag/src/q_and_a/eval.py:35\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(forward_fn, picker_fn, eval_dataset, log_each)\u001b[0m\n\u001b[1;32m     32\u001b[0m answer_idx \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Get the model's response\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Pick the best option\u001b[39;00m\n\u001b[1;32m     38\u001b[0m picked_idx \u001b[38;5;241m=\u001b[39m picker_fn(response)\n",
      "Cell \u001b[0;32mIn[15], line 11\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(question, options)\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      2\u001b[0m forward \u001b[38;5;241m=\u001b[39m build_forwarder(\n\u001b[1;32m      3\u001b[0m     model,\n\u001b[1;32m      4\u001b[0m     tokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m forward_and_get_arg_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m question, options: \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpick_from_classifier\u001b[39m(out):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39margmax(out\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/Try/pytorch_training/10_rag/src/q_and_a/forward.py:68\u001b[0m, in \u001b[0;36mbuild_forwarder.<locals>.forward_fn\u001b[0;34m(question, options)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward_fn\u001b[39m(question: \u001b[38;5;28mstr\u001b[39m, options: List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43maugmenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugmenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk_augmentations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk_augmentations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt_builder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_builder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_llm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_llm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Try/pytorch_training/10_rag/src/q_and_a/forward.py:48\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(llm, tokenizer, augmenter, k_augmentations, prompt_builder, question, options, device, causal_llm)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m  llm(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresult, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 48\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/accelerate/utils/operations.py:818\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/accelerate/utils/operations.py:806\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/amp/autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/peft/peft_model.py:2352\u001b[0m, in \u001b[0;36mPeftModelForTokenClassification.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   2350\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mPOLY:\n\u001b[1;32m   2351\u001b[0m             kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task_ids\n\u001b[0;32m-> 2352\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2353\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2354\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2355\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2356\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2357\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2358\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2359\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2360\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2361\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2363\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   2364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2365\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/peft/tuners/tuners_utils.py:193\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:904\u001b[0m, in \u001b[0;36mLlamaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(LLAMA_INPUTS_DOCSTRING)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    895\u001b[0m     output_hidden_states: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    896\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SequenceClassifierOutputWithPast:\n\u001b[1;32m    897\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;124;03m    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;124;03m        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;124;03m        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;124;03m        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 904\u001b[0m     transformer_outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    914\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    915\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/utils/generic.py:965\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    962\u001b[0m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_is_top_level_module\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 965\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[1;32m    967\u001b[0m         output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:571\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    560\u001b[0m         partial(decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mflash_attn_kwargs),\n\u001b[1;32m    561\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    568\u001b[0m         position_embeddings,\n\u001b[1;32m    569\u001b[0m     )\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 571\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    583\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:334\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    332\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    333\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 334\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    337\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/transformers/models/llama/modeling_llama.py:172\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 172\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/bitsandbytes/nn/modules.py:990\u001b[0m, in \u001b[0;36mLinear8bitLt.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype:\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 990\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mbnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mhas_fp16_weights \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCB \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mCB\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:509\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(A, B, out, state, threshold, bias)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m threshold \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    508\u001b[0m     state\u001b[38;5;241m.\u001b[39mthreshold \u001b[38;5;241m=\u001b[39m threshold\n\u001b[0;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatMul8bitLt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:373\u001b[0m, in \u001b[0;36mMatMul8bitLt.forward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# Dequantize matmul result\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m bias\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;66;03m# we apply the fused bias here\u001b[39;00m\n\u001b[0;32m--> 373\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint8_mm_dequant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSCA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSCB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(A\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# apply bias separately\u001b[39;00m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;66;03m# TODO: Fused bias for fp32/bf16?\u001b[39;00m\n\u001b[1;32m    376\u001b[0m     output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mint8_mm_dequant(out32, SCA, state\u001b[38;5;241m.\u001b[39mSCB, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(A\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39madd_(bias)\n",
      "File \u001b[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/bitsandbytes/functional.py:2405\u001b[0m, in \u001b[0;36mint8_mm_dequant\u001b[0;34m(A, row_stats, col_stats, out, bias)\u001b[0m\n\u001b[1;32m   2402\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m bias\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16\n\u001b[1;32m   2404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2405\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2407\u001b[0m ptrA \u001b[38;5;241m=\u001b[39m get_ptr(A)\n\u001b[1;32m   2408\u001b[0m ptrOut \u001b[38;5;241m=\u001b[39m get_ptr(out)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 5.65 GiB of which 8.00 MiB is free. Including non-PyTorch memory, this process has 5.62 GiB memory in use. Of the allocated memory 5.44 GiB is allocated by PyTorch, and 67.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "forward = build_forwarder(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    querier,\n",
    "    k_augmentations=1,\n",
    "    prompt_builder=prompt,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "forward_and_get_arg_max = lambda question, options: forward(\n",
    "    question,\n",
    "    options=options,\n",
    ")\n",
    "\n",
    "def pick_from_classifier(out):\n",
    "    return torch.argmax(out.logits[0])\n",
    "\n",
    "accuracy = evaluate(\n",
    "    forward_fn=forward_and_get_arg_max,\n",
    "    picker_fn=pick_from_classifier,\n",
    "    eval_dataset=evaluateWithAnswers,\n",
    ")\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4656d02-9965-4673-870d-19acefd98d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from q_and_a.predict import predict\n",
    "from data.q_and_a.test_questions import TestQuestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9190324b-0fb7-49ad-9640-99ba17542c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = TestQuestions(\"../../data/pubmed_QA_test_questions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "816fa818-c0c9-4242-a998-1606160cdc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0.1%\n",
      "Processed 0.2%\n",
      "Processed 0.3%\n",
      "Processed 0.4%\n",
      "Processed 0.5%\n",
      "Processed 0.6%\n",
      "Processed 0.7%\n",
      "Processed 0.8%\n",
      "Processed 0.9%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, tensor(1, device='cuda:0')),\n",
       " (1, tensor(3, device='cuda:0')),\n",
       " (2, tensor(2, device='cuda:0')),\n",
       " (3, tensor(1, device='cuda:0')),\n",
       " (4, tensor(2, device='cuda:0')),\n",
       " (5, tensor(1, device='cuda:0')),\n",
       " (6, tensor(2, device='cuda:0')),\n",
       " (7, tensor(2, device='cuda:0')),\n",
       " (8, tensor(1, device='cuda:0')),\n",
       " (9, tensor(0, device='cuda:0')),\n",
       " (10, tensor(3, device='cuda:0')),\n",
       " (11, tensor(3, device='cuda:0')),\n",
       " (12, tensor(2, device='cuda:0')),\n",
       " (13, tensor(3, device='cuda:0')),\n",
       " (14, tensor(3, device='cuda:0')),\n",
       " (15, tensor(1, device='cuda:0')),\n",
       " (16, tensor(3, device='cuda:0')),\n",
       " (17, tensor(2, device='cuda:0')),\n",
       " (18, tensor(0, device='cuda:0')),\n",
       " (19, tensor(2, device='cuda:0')),\n",
       " (20, tensor(1, device='cuda:0')),\n",
       " (21, tensor(3, device='cuda:0')),\n",
       " (22, tensor(2, device='cuda:0')),\n",
       " (23, tensor(1, device='cuda:0')),\n",
       " (24, tensor(1, device='cuda:0')),\n",
       " (25, tensor(1, device='cuda:0')),\n",
       " (26, tensor(3, device='cuda:0')),\n",
       " (27, tensor(0, device='cuda:0')),\n",
       " (28, tensor(2, device='cuda:0')),\n",
       " (29, tensor(2, device='cuda:0')),\n",
       " (30, tensor(0, device='cuda:0')),\n",
       " (31, tensor(3, device='cuda:0')),\n",
       " (32, tensor(3, device='cuda:0')),\n",
       " (33, tensor(1, device='cuda:0')),\n",
       " (34, tensor(2, device='cuda:0')),\n",
       " (35, tensor(0, device='cuda:0')),\n",
       " (36, tensor(1, device='cuda:0')),\n",
       " (37, tensor(0, device='cuda:0')),\n",
       " (38, tensor(3, device='cuda:0')),\n",
       " (39, tensor(3, device='cuda:0')),\n",
       " (40, tensor(0, device='cuda:0')),\n",
       " (41, tensor(0, device='cuda:0')),\n",
       " (42, tensor(0, device='cuda:0')),\n",
       " (43, tensor(1, device='cuda:0')),\n",
       " (44, tensor(3, device='cuda:0')),\n",
       " (45, tensor(0, device='cuda:0')),\n",
       " (46, tensor(2, device='cuda:0')),\n",
       " (47, tensor(1, device='cuda:0')),\n",
       " (48, tensor(0, device='cuda:0')),\n",
       " (49, tensor(1, device='cuda:0')),\n",
       " (50, tensor(3, device='cuda:0')),\n",
       " (51, tensor(3, device='cuda:0')),\n",
       " (52, tensor(0, device='cuda:0')),\n",
       " (53, tensor(1, device='cuda:0')),\n",
       " (54, tensor(3, device='cuda:0')),\n",
       " (55, tensor(2, device='cuda:0')),\n",
       " (56, tensor(3, device='cuda:0')),\n",
       " (57, tensor(3, device='cuda:0')),\n",
       " (58, tensor(1, device='cuda:0')),\n",
       " (59, tensor(2, device='cuda:0')),\n",
       " (60, tensor(0, device='cuda:0')),\n",
       " (61, tensor(0, device='cuda:0')),\n",
       " (62, tensor(3, device='cuda:0')),\n",
       " (63, tensor(0, device='cuda:0')),\n",
       " (64, tensor(2, device='cuda:0')),\n",
       " (65, tensor(2, device='cuda:0')),\n",
       " (66, tensor(1, device='cuda:0')),\n",
       " (67, tensor(0, device='cuda:0')),\n",
       " (68, tensor(0, device='cuda:0')),\n",
       " (69, tensor(0, device='cuda:0')),\n",
       " (70, tensor(3, device='cuda:0')),\n",
       " (71, tensor(1, device='cuda:0')),\n",
       " (72, tensor(3, device='cuda:0')),\n",
       " (73, tensor(2, device='cuda:0')),\n",
       " (74, tensor(3, device='cuda:0')),\n",
       " (75, tensor(3, device='cuda:0')),\n",
       " (76, tensor(0, device='cuda:0')),\n",
       " (77, tensor(1, device='cuda:0')),\n",
       " (78, tensor(2, device='cuda:0')),\n",
       " (79, tensor(2, device='cuda:0')),\n",
       " (80, tensor(3, device='cuda:0')),\n",
       " (81, tensor(0, device='cuda:0')),\n",
       " (82, tensor(0, device='cuda:0')),\n",
       " (83, tensor(3, device='cuda:0')),\n",
       " (84, tensor(1, device='cuda:0')),\n",
       " (85, tensor(3, device='cuda:0')),\n",
       " (86, tensor(0, device='cuda:0')),\n",
       " (87, tensor(0, device='cuda:0')),\n",
       " (88, tensor(3, device='cuda:0')),\n",
       " (89, tensor(0, device='cuda:0')),\n",
       " (90, tensor(2, device='cuda:0')),\n",
       " (91, tensor(1, device='cuda:0')),\n",
       " (92, tensor(3, device='cuda:0')),\n",
       " (93, tensor(3, device='cuda:0')),\n",
       " (94, tensor(0, device='cuda:0')),\n",
       " (95, tensor(2, device='cuda:0')),\n",
       " (96, tensor(3, device='cuda:0')),\n",
       " (97, tensor(0, device='cuda:0')),\n",
       " (98, tensor(3, device='cuda:0')),\n",
       " (99, tensor(0, device='cuda:0')),\n",
       " (100, tensor(3, device='cuda:0')),\n",
       " (101, tensor(1, device='cuda:0')),\n",
       " (102, tensor(1, device='cuda:0')),\n",
       " (103, tensor(3, device='cuda:0')),\n",
       " (104, tensor(3, device='cuda:0')),\n",
       " (105, tensor(1, device='cuda:0')),\n",
       " (106, tensor(3, device='cuda:0')),\n",
       " (107, tensor(0, device='cuda:0')),\n",
       " (108, tensor(1, device='cuda:0')),\n",
       " (109, tensor(0, device='cuda:0')),\n",
       " (110, tensor(3, device='cuda:0')),\n",
       " (111, tensor(0, device='cuda:0')),\n",
       " (112, tensor(1, device='cuda:0')),\n",
       " (113, tensor(3, device='cuda:0')),\n",
       " (114, tensor(0, device='cuda:0')),\n",
       " (115, tensor(0, device='cuda:0')),\n",
       " (116, tensor(1, device='cuda:0')),\n",
       " (117, tensor(1, device='cuda:0')),\n",
       " (118, tensor(3, device='cuda:0')),\n",
       " (119, tensor(3, device='cuda:0')),\n",
       " (120, tensor(0, device='cuda:0')),\n",
       " (121, tensor(0, device='cuda:0')),\n",
       " (122, tensor(2, device='cuda:0')),\n",
       " (123, tensor(0, device='cuda:0')),\n",
       " (124, tensor(1, device='cuda:0')),\n",
       " (125, tensor(3, device='cuda:0')),\n",
       " (126, tensor(2, device='cuda:0')),\n",
       " (127, tensor(3, device='cuda:0')),\n",
       " (128, tensor(2, device='cuda:0')),\n",
       " (129, tensor(3, device='cuda:0')),\n",
       " (130, tensor(3, device='cuda:0')),\n",
       " (131, tensor(2, device='cuda:0')),\n",
       " (132, tensor(0, device='cuda:0')),\n",
       " (133, tensor(1, device='cuda:0')),\n",
       " (134, tensor(3, device='cuda:0')),\n",
       " (135, tensor(3, device='cuda:0')),\n",
       " (136, tensor(1, device='cuda:0')),\n",
       " (137, tensor(3, device='cuda:0')),\n",
       " (138, tensor(3, device='cuda:0')),\n",
       " (139, tensor(3, device='cuda:0')),\n",
       " (140, tensor(0, device='cuda:0')),\n",
       " (141, tensor(3, device='cuda:0')),\n",
       " (142, tensor(1, device='cuda:0')),\n",
       " (143, tensor(2, device='cuda:0')),\n",
       " (144, tensor(1, device='cuda:0')),\n",
       " (145, tensor(3, device='cuda:0')),\n",
       " (146, tensor(3, device='cuda:0')),\n",
       " (147, tensor(1, device='cuda:0')),\n",
       " (148, tensor(3, device='cuda:0')),\n",
       " (149, tensor(3, device='cuda:0')),\n",
       " (150, tensor(1, device='cuda:0')),\n",
       " (151, tensor(1, device='cuda:0')),\n",
       " (152, tensor(0, device='cuda:0')),\n",
       " (153, tensor(0, device='cuda:0')),\n",
       " (154, tensor(1, device='cuda:0')),\n",
       " (155, tensor(3, device='cuda:0')),\n",
       " (156, tensor(2, device='cuda:0')),\n",
       " (157, tensor(1, device='cuda:0')),\n",
       " (158, tensor(3, device='cuda:0')),\n",
       " (159, tensor(2, device='cuda:0')),\n",
       " (160, tensor(3, device='cuda:0')),\n",
       " (161, tensor(2, device='cuda:0')),\n",
       " (162, tensor(0, device='cuda:0')),\n",
       " (163, tensor(3, device='cuda:0')),\n",
       " (164, tensor(1, device='cuda:0')),\n",
       " (165, tensor(2, device='cuda:0')),\n",
       " (166, tensor(1, device='cuda:0')),\n",
       " (167, tensor(2, device='cuda:0')),\n",
       " (168, tensor(3, device='cuda:0')),\n",
       " (169, tensor(0, device='cuda:0')),\n",
       " (170, tensor(2, device='cuda:0')),\n",
       " (171, tensor(2, device='cuda:0')),\n",
       " (172, tensor(3, device='cuda:0')),\n",
       " (173, tensor(1, device='cuda:0')),\n",
       " (174, tensor(0, device='cuda:0')),\n",
       " (175, tensor(1, device='cuda:0')),\n",
       " (176, tensor(1, device='cuda:0')),\n",
       " (177, tensor(1, device='cuda:0')),\n",
       " (178, tensor(0, device='cuda:0')),\n",
       " (179, tensor(2, device='cuda:0')),\n",
       " (180, tensor(0, device='cuda:0')),\n",
       " (181, tensor(3, device='cuda:0')),\n",
       " (182, tensor(0, device='cuda:0')),\n",
       " (183, tensor(2, device='cuda:0')),\n",
       " (184, tensor(1, device='cuda:0')),\n",
       " (185, tensor(3, device='cuda:0')),\n",
       " (186, tensor(1, device='cuda:0')),\n",
       " (187, tensor(3, device='cuda:0')),\n",
       " (188, tensor(3, device='cuda:0')),\n",
       " (189, tensor(3, device='cuda:0')),\n",
       " (190, tensor(3, device='cuda:0')),\n",
       " (191, tensor(1, device='cuda:0')),\n",
       " (192, tensor(0, device='cuda:0')),\n",
       " (193, tensor(3, device='cuda:0')),\n",
       " (194, tensor(1, device='cuda:0')),\n",
       " (195, tensor(0, device='cuda:0')),\n",
       " (196, tensor(0, device='cuda:0')),\n",
       " (197, tensor(2, device='cuda:0')),\n",
       " (198, tensor(3, device='cuda:0')),\n",
       " (199, tensor(0, device='cuda:0')),\n",
       " (200, tensor(2, device='cuda:0')),\n",
       " (201, tensor(3, device='cuda:0')),\n",
       " (202, tensor(1, device='cuda:0')),\n",
       " (203, tensor(0, device='cuda:0')),\n",
       " (204, tensor(0, device='cuda:0')),\n",
       " (205, tensor(1, device='cuda:0')),\n",
       " (206, tensor(3, device='cuda:0')),\n",
       " (207, tensor(3, device='cuda:0')),\n",
       " (208, tensor(2, device='cuda:0')),\n",
       " (209, tensor(2, device='cuda:0')),\n",
       " (210, tensor(3, device='cuda:0')),\n",
       " (211, tensor(0, device='cuda:0')),\n",
       " (212, tensor(3, device='cuda:0')),\n",
       " (213, tensor(3, device='cuda:0')),\n",
       " (214, tensor(3, device='cuda:0')),\n",
       " (215, tensor(3, device='cuda:0')),\n",
       " (216, tensor(2, device='cuda:0')),\n",
       " (217, tensor(1, device='cuda:0')),\n",
       " (218, tensor(1, device='cuda:0')),\n",
       " (219, tensor(3, device='cuda:0')),\n",
       " (220, tensor(0, device='cuda:0')),\n",
       " (221, tensor(0, device='cuda:0')),\n",
       " (222, tensor(2, device='cuda:0')),\n",
       " (223, tensor(3, device='cuda:0')),\n",
       " (224, tensor(3, device='cuda:0')),\n",
       " (225, tensor(2, device='cuda:0')),\n",
       " (226, tensor(3, device='cuda:0')),\n",
       " (227, tensor(3, device='cuda:0')),\n",
       " (228, tensor(2, device='cuda:0')),\n",
       " (229, tensor(2, device='cuda:0')),\n",
       " (230, tensor(1, device='cuda:0')),\n",
       " (231, tensor(1, device='cuda:0')),\n",
       " (232, tensor(2, device='cuda:0')),\n",
       " (233, tensor(1, device='cuda:0')),\n",
       " (234, tensor(2, device='cuda:0')),\n",
       " (235, tensor(0, device='cuda:0')),\n",
       " (236, tensor(2, device='cuda:0')),\n",
       " (237, tensor(0, device='cuda:0')),\n",
       " (238, tensor(2, device='cuda:0')),\n",
       " (239, tensor(0, device='cuda:0')),\n",
       " (240, tensor(0, device='cuda:0')),\n",
       " (241, tensor(3, device='cuda:0')),\n",
       " (242, tensor(1, device='cuda:0')),\n",
       " (243, tensor(1, device='cuda:0')),\n",
       " (244, tensor(2, device='cuda:0')),\n",
       " (245, tensor(2, device='cuda:0')),\n",
       " (246, tensor(0, device='cuda:0')),\n",
       " (247, tensor(2, device='cuda:0')),\n",
       " (248, tensor(1, device='cuda:0')),\n",
       " (249, tensor(3, device='cuda:0')),\n",
       " (250, tensor(3, device='cuda:0')),\n",
       " (251, tensor(3, device='cuda:0')),\n",
       " (252, tensor(0, device='cuda:0')),\n",
       " (253, tensor(3, device='cuda:0')),\n",
       " (254, tensor(0, device='cuda:0')),\n",
       " (255, tensor(0, device='cuda:0')),\n",
       " (256, tensor(0, device='cuda:0')),\n",
       " (257, tensor(1, device='cuda:0')),\n",
       " (258, tensor(0, device='cuda:0')),\n",
       " (259, tensor(2, device='cuda:0')),\n",
       " (260, tensor(2, device='cuda:0')),\n",
       " (261, tensor(3, device='cuda:0')),\n",
       " (262, tensor(1, device='cuda:0')),\n",
       " (263, tensor(2, device='cuda:0')),\n",
       " (264, tensor(0, device='cuda:0')),\n",
       " (265, tensor(2, device='cuda:0')),\n",
       " (266, tensor(3, device='cuda:0')),\n",
       " (267, tensor(2, device='cuda:0')),\n",
       " (268, tensor(3, device='cuda:0')),\n",
       " (269, tensor(2, device='cuda:0')),\n",
       " (270, tensor(0, device='cuda:0')),\n",
       " (271, tensor(1, device='cuda:0')),\n",
       " (272, tensor(1, device='cuda:0')),\n",
       " (273, tensor(1, device='cuda:0')),\n",
       " (274, tensor(0, device='cuda:0')),\n",
       " (275, tensor(3, device='cuda:0')),\n",
       " (276, tensor(1, device='cuda:0')),\n",
       " (277, tensor(3, device='cuda:0')),\n",
       " (278, tensor(3, device='cuda:0')),\n",
       " (279, tensor(1, device='cuda:0')),\n",
       " (280, tensor(3, device='cuda:0')),\n",
       " (281, tensor(0, device='cuda:0')),\n",
       " (282, tensor(1, device='cuda:0')),\n",
       " (283, tensor(3, device='cuda:0')),\n",
       " (284, tensor(3, device='cuda:0')),\n",
       " (285, tensor(3, device='cuda:0')),\n",
       " (286, tensor(3, device='cuda:0')),\n",
       " (287, tensor(3, device='cuda:0')),\n",
       " (288, tensor(3, device='cuda:0')),\n",
       " (289, tensor(1, device='cuda:0')),\n",
       " (290, tensor(1, device='cuda:0')),\n",
       " (291, tensor(0, device='cuda:0')),\n",
       " (292, tensor(2, device='cuda:0')),\n",
       " (293, tensor(0, device='cuda:0')),\n",
       " (294, tensor(3, device='cuda:0')),\n",
       " (295, tensor(3, device='cuda:0')),\n",
       " (296, tensor(0, device='cuda:0')),\n",
       " (297, tensor(1, device='cuda:0')),\n",
       " (298, tensor(0, device='cuda:0')),\n",
       " (299, tensor(1, device='cuda:0')),\n",
       " (300, tensor(1, device='cuda:0')),\n",
       " (301, tensor(3, device='cuda:0')),\n",
       " (302, tensor(0, device='cuda:0')),\n",
       " (303, tensor(0, device='cuda:0')),\n",
       " (304, tensor(3, device='cuda:0')),\n",
       " (305, tensor(1, device='cuda:0')),\n",
       " (306, tensor(3, device='cuda:0')),\n",
       " (307, tensor(1, device='cuda:0')),\n",
       " (308, tensor(3, device='cuda:0')),\n",
       " (309, tensor(0, device='cuda:0')),\n",
       " (310, tensor(3, device='cuda:0')),\n",
       " (311, tensor(2, device='cuda:0')),\n",
       " (312, tensor(3, device='cuda:0')),\n",
       " (313, tensor(2, device='cuda:0')),\n",
       " (314, tensor(0, device='cuda:0')),\n",
       " (315, tensor(1, device='cuda:0')),\n",
       " (316, tensor(2, device='cuda:0')),\n",
       " (317, tensor(3, device='cuda:0')),\n",
       " (318, tensor(3, device='cuda:0')),\n",
       " (319, tensor(3, device='cuda:0')),\n",
       " (320, tensor(2, device='cuda:0')),\n",
       " (321, tensor(3, device='cuda:0')),\n",
       " (322, tensor(3, device='cuda:0')),\n",
       " (323, tensor(3, device='cuda:0')),\n",
       " (324, tensor(1, device='cuda:0')),\n",
       " (325, tensor(0, device='cuda:0')),\n",
       " (326, tensor(3, device='cuda:0')),\n",
       " (327, tensor(3, device='cuda:0')),\n",
       " (328, tensor(3, device='cuda:0')),\n",
       " (329, tensor(3, device='cuda:0')),\n",
       " (330, tensor(3, device='cuda:0')),\n",
       " (331, tensor(0, device='cuda:0')),\n",
       " (332, tensor(3, device='cuda:0')),\n",
       " (333, tensor(2, device='cuda:0')),\n",
       " (334, tensor(2, device='cuda:0')),\n",
       " (335, tensor(0, device='cuda:0')),\n",
       " (336, tensor(3, device='cuda:0')),\n",
       " (337, tensor(1, device='cuda:0')),\n",
       " (338, tensor(3, device='cuda:0')),\n",
       " (339, tensor(0, device='cuda:0')),\n",
       " (340, tensor(0, device='cuda:0')),\n",
       " (341, tensor(1, device='cuda:0')),\n",
       " (342, tensor(2, device='cuda:0')),\n",
       " (343, tensor(1, device='cuda:0')),\n",
       " (344, tensor(0, device='cuda:0')),\n",
       " (345, tensor(1, device='cuda:0')),\n",
       " (346, tensor(3, device='cuda:0')),\n",
       " (347, tensor(2, device='cuda:0')),\n",
       " (348, tensor(2, device='cuda:0')),\n",
       " (349, tensor(3, device='cuda:0')),\n",
       " (350, tensor(2, device='cuda:0')),\n",
       " (351, tensor(1, device='cuda:0')),\n",
       " (352, tensor(3, device='cuda:0')),\n",
       " (353, tensor(1, device='cuda:0')),\n",
       " (354, tensor(1, device='cuda:0')),\n",
       " (355, tensor(3, device='cuda:0')),\n",
       " (356, tensor(2, device='cuda:0')),\n",
       " (357, tensor(2, device='cuda:0')),\n",
       " (358, tensor(2, device='cuda:0')),\n",
       " (359, tensor(3, device='cuda:0')),\n",
       " (360, tensor(3, device='cuda:0')),\n",
       " (361, tensor(0, device='cuda:0')),\n",
       " (362, tensor(0, device='cuda:0')),\n",
       " (363, tensor(3, device='cuda:0')),\n",
       " (364, tensor(3, device='cuda:0')),\n",
       " (365, tensor(2, device='cuda:0')),\n",
       " (366, tensor(3, device='cuda:0')),\n",
       " (367, tensor(3, device='cuda:0')),\n",
       " (368, tensor(3, device='cuda:0')),\n",
       " (369, tensor(0, device='cuda:0')),\n",
       " (370, tensor(1, device='cuda:0')),\n",
       " (371, tensor(0, device='cuda:0')),\n",
       " (372, tensor(2, device='cuda:0')),\n",
       " (373, tensor(1, device='cuda:0')),\n",
       " (374, tensor(0, device='cuda:0')),\n",
       " (375, tensor(0, device='cuda:0')),\n",
       " (376, tensor(3, device='cuda:0')),\n",
       " (377, tensor(1, device='cuda:0')),\n",
       " (378, tensor(3, device='cuda:0')),\n",
       " (379, tensor(0, device='cuda:0')),\n",
       " (380, tensor(1, device='cuda:0')),\n",
       " (381, tensor(3, device='cuda:0')),\n",
       " (382, tensor(3, device='cuda:0')),\n",
       " (383, tensor(2, device='cuda:0')),\n",
       " (384, tensor(3, device='cuda:0')),\n",
       " (385, tensor(0, device='cuda:0')),\n",
       " (386, tensor(0, device='cuda:0')),\n",
       " (387, tensor(1, device='cuda:0')),\n",
       " (388, tensor(0, device='cuda:0')),\n",
       " (389, tensor(0, device='cuda:0')),\n",
       " (390, tensor(3, device='cuda:0')),\n",
       " (391, tensor(3, device='cuda:0')),\n",
       " (392, tensor(2, device='cuda:0')),\n",
       " (393, tensor(0, device='cuda:0')),\n",
       " (394, tensor(3, device='cuda:0')),\n",
       " (395, tensor(0, device='cuda:0')),\n",
       " (396, tensor(0, device='cuda:0')),\n",
       " (397, tensor(1, device='cuda:0')),\n",
       " (398, tensor(3, device='cuda:0')),\n",
       " (399, tensor(1, device='cuda:0')),\n",
       " (400, tensor(2, device='cuda:0')),\n",
       " (401, tensor(0, device='cuda:0')),\n",
       " (402, tensor(2, device='cuda:0')),\n",
       " (403, tensor(0, device='cuda:0')),\n",
       " (404, tensor(3, device='cuda:0')),\n",
       " (405, tensor(1, device='cuda:0')),\n",
       " (406, tensor(1, device='cuda:0')),\n",
       " (407, tensor(0, device='cuda:0')),\n",
       " (408, tensor(3, device='cuda:0')),\n",
       " (409, tensor(1, device='cuda:0')),\n",
       " (410, tensor(1, device='cuda:0')),\n",
       " (411, tensor(2, device='cuda:0')),\n",
       " (412, tensor(3, device='cuda:0')),\n",
       " (413, tensor(1, device='cuda:0')),\n",
       " (414, tensor(3, device='cuda:0')),\n",
       " (415, tensor(2, device='cuda:0')),\n",
       " (416, tensor(3, device='cuda:0')),\n",
       " (417, tensor(3, device='cuda:0')),\n",
       " (418, tensor(3, device='cuda:0')),\n",
       " (419, tensor(3, device='cuda:0')),\n",
       " (420, tensor(1, device='cuda:0')),\n",
       " (421, tensor(3, device='cuda:0')),\n",
       " (422, tensor(3, device='cuda:0')),\n",
       " (423, tensor(0, device='cuda:0')),\n",
       " (424, tensor(0, device='cuda:0')),\n",
       " (425, tensor(1, device='cuda:0')),\n",
       " (426, tensor(1, device='cuda:0')),\n",
       " (427, tensor(3, device='cuda:0')),\n",
       " (428, tensor(3, device='cuda:0')),\n",
       " (429, tensor(2, device='cuda:0')),\n",
       " (430, tensor(1, device='cuda:0')),\n",
       " (431, tensor(3, device='cuda:0')),\n",
       " (432, tensor(2, device='cuda:0')),\n",
       " (433, tensor(0, device='cuda:0')),\n",
       " (434, tensor(3, device='cuda:0')),\n",
       " (435, tensor(3, device='cuda:0')),\n",
       " (436, tensor(3, device='cuda:0')),\n",
       " (437, tensor(0, device='cuda:0')),\n",
       " (438, tensor(2, device='cuda:0')),\n",
       " (439, tensor(2, device='cuda:0')),\n",
       " (440, tensor(1, device='cuda:0')),\n",
       " (441, tensor(1, device='cuda:0')),\n",
       " (442, tensor(3, device='cuda:0')),\n",
       " (443, tensor(3, device='cuda:0')),\n",
       " (444, tensor(1, device='cuda:0')),\n",
       " (445, tensor(0, device='cuda:0')),\n",
       " (446, tensor(2, device='cuda:0')),\n",
       " (447, tensor(2, device='cuda:0')),\n",
       " (448, tensor(3, device='cuda:0')),\n",
       " (449, tensor(0, device='cuda:0')),\n",
       " (450, tensor(3, device='cuda:0')),\n",
       " (451, tensor(0, device='cuda:0')),\n",
       " (452, tensor(2, device='cuda:0')),\n",
       " (453, tensor(1, device='cuda:0')),\n",
       " (454, tensor(0, device='cuda:0')),\n",
       " (455, tensor(1, device='cuda:0')),\n",
       " (456, tensor(2, device='cuda:0')),\n",
       " (457, tensor(3, device='cuda:0')),\n",
       " (458, tensor(3, device='cuda:0')),\n",
       " (459, tensor(2, device='cuda:0')),\n",
       " (460, tensor(2, device='cuda:0')),\n",
       " (461, tensor(3, device='cuda:0')),\n",
       " (462, tensor(1, device='cuda:0')),\n",
       " (463, tensor(1, device='cuda:0')),\n",
       " (464, tensor(3, device='cuda:0')),\n",
       " (465, tensor(3, device='cuda:0')),\n",
       " (466, tensor(3, device='cuda:0')),\n",
       " (467, tensor(3, device='cuda:0')),\n",
       " (468, tensor(1, device='cuda:0')),\n",
       " (469, tensor(1, device='cuda:0')),\n",
       " (470, tensor(3, device='cuda:0')),\n",
       " (471, tensor(3, device='cuda:0')),\n",
       " (472, tensor(2, device='cuda:0')),\n",
       " (473, tensor(0, device='cuda:0')),\n",
       " (474, tensor(3, device='cuda:0')),\n",
       " (475, tensor(1, device='cuda:0')),\n",
       " (476, tensor(2, device='cuda:0')),\n",
       " (477, tensor(3, device='cuda:0')),\n",
       " (478, tensor(3, device='cuda:0')),\n",
       " (479, tensor(3, device='cuda:0')),\n",
       " (480, tensor(3, device='cuda:0')),\n",
       " (481, tensor(3, device='cuda:0')),\n",
       " (482, tensor(3, device='cuda:0')),\n",
       " (483, tensor(3, device='cuda:0')),\n",
       " (484, tensor(3, device='cuda:0')),\n",
       " (485, tensor(0, device='cuda:0')),\n",
       " (486, tensor(2, device='cuda:0')),\n",
       " (487, tensor(1, device='cuda:0')),\n",
       " (488, tensor(1, device='cuda:0')),\n",
       " (489, tensor(0, device='cuda:0')),\n",
       " (490, tensor(2, device='cuda:0')),\n",
       " (491, tensor(3, device='cuda:0')),\n",
       " (492, tensor(3, device='cuda:0')),\n",
       " (493, tensor(1, device='cuda:0')),\n",
       " (494, tensor(0, device='cuda:0')),\n",
       " (495, tensor(1, device='cuda:0')),\n",
       " (496, tensor(3, device='cuda:0')),\n",
       " (497, tensor(0, device='cuda:0')),\n",
       " (498, tensor(1, device='cuda:0')),\n",
       " (499, tensor(3, device='cuda:0')),\n",
       " (500, tensor(1, device='cuda:0')),\n",
       " (501, tensor(2, device='cuda:0')),\n",
       " (502, tensor(1, device='cuda:0')),\n",
       " (503, tensor(3, device='cuda:0')),\n",
       " (504, tensor(0, device='cuda:0')),\n",
       " (505, tensor(2, device='cuda:0')),\n",
       " (506, tensor(0, device='cuda:0')),\n",
       " (507, tensor(2, device='cuda:0')),\n",
       " (508, tensor(2, device='cuda:0')),\n",
       " (509, tensor(2, device='cuda:0')),\n",
       " (510, tensor(2, device='cuda:0')),\n",
       " (511, tensor(3, device='cuda:0')),\n",
       " (512, tensor(3, device='cuda:0')),\n",
       " (513, tensor(1, device='cuda:0')),\n",
       " (514, tensor(0, device='cuda:0')),\n",
       " (515, tensor(3, device='cuda:0')),\n",
       " (516, tensor(3, device='cuda:0')),\n",
       " (517, tensor(1, device='cuda:0')),\n",
       " (518, tensor(3, device='cuda:0')),\n",
       " (519, tensor(1, device='cuda:0')),\n",
       " (520, tensor(0, device='cuda:0')),\n",
       " (521, tensor(1, device='cuda:0')),\n",
       " (522, tensor(3, device='cuda:0')),\n",
       " (523, tensor(3, device='cuda:0')),\n",
       " (524, tensor(3, device='cuda:0')),\n",
       " (525, tensor(1, device='cuda:0')),\n",
       " (526, tensor(1, device='cuda:0')),\n",
       " (527, tensor(0, device='cuda:0')),\n",
       " (528, tensor(0, device='cuda:0')),\n",
       " (529, tensor(2, device='cuda:0')),\n",
       " (530, tensor(3, device='cuda:0')),\n",
       " (531, tensor(1, device='cuda:0')),\n",
       " (532, tensor(2, device='cuda:0')),\n",
       " (533, tensor(0, device='cuda:0')),\n",
       " (534, tensor(3, device='cuda:0')),\n",
       " (535, tensor(1, device='cuda:0')),\n",
       " (536, tensor(3, device='cuda:0')),\n",
       " (537, tensor(3, device='cuda:0')),\n",
       " (538, tensor(0, device='cuda:0')),\n",
       " (539, tensor(2, device='cuda:0')),\n",
       " (540, tensor(3, device='cuda:0')),\n",
       " (541, tensor(1, device='cuda:0')),\n",
       " (542, tensor(3, device='cuda:0')),\n",
       " (543, tensor(0, device='cuda:0')),\n",
       " (544, tensor(3, device='cuda:0')),\n",
       " (545, tensor(1, device='cuda:0')),\n",
       " (546, tensor(2, device='cuda:0')),\n",
       " (547, tensor(2, device='cuda:0')),\n",
       " (548, tensor(0, device='cuda:0')),\n",
       " (549, tensor(2, device='cuda:0')),\n",
       " (550, tensor(0, device='cuda:0')),\n",
       " (551, tensor(2, device='cuda:0')),\n",
       " (552, tensor(3, device='cuda:0')),\n",
       " (553, tensor(3, device='cuda:0')),\n",
       " (554, tensor(3, device='cuda:0')),\n",
       " (555, tensor(0, device='cuda:0')),\n",
       " (556, tensor(1, device='cuda:0')),\n",
       " (557, tensor(3, device='cuda:0')),\n",
       " (558, tensor(3, device='cuda:0')),\n",
       " (559, tensor(1, device='cuda:0')),\n",
       " (560, tensor(2, device='cuda:0')),\n",
       " (561, tensor(0, device='cuda:0')),\n",
       " (562, tensor(3, device='cuda:0')),\n",
       " (563, tensor(0, device='cuda:0')),\n",
       " (564, tensor(0, device='cuda:0')),\n",
       " (565, tensor(1, device='cuda:0')),\n",
       " (566, tensor(3, device='cuda:0')),\n",
       " (567, tensor(1, device='cuda:0')),\n",
       " (568, tensor(3, device='cuda:0')),\n",
       " (569, tensor(1, device='cuda:0')),\n",
       " (570, tensor(1, device='cuda:0')),\n",
       " (571, tensor(3, device='cuda:0')),\n",
       " (572, tensor(3, device='cuda:0')),\n",
       " (573, tensor(2, device='cuda:0')),\n",
       " (574, tensor(3, device='cuda:0')),\n",
       " (575, tensor(2, device='cuda:0')),\n",
       " (576, tensor(2, device='cuda:0')),\n",
       " (577, tensor(3, device='cuda:0')),\n",
       " (578, tensor(0, device='cuda:0')),\n",
       " (579, tensor(2, device='cuda:0')),\n",
       " (580, tensor(3, device='cuda:0')),\n",
       " (581, tensor(0, device='cuda:0')),\n",
       " (582, tensor(0, device='cuda:0')),\n",
       " (583, tensor(3, device='cuda:0')),\n",
       " (584, tensor(3, device='cuda:0')),\n",
       " (585, tensor(0, device='cuda:0')),\n",
       " (586, tensor(1, device='cuda:0')),\n",
       " (587, tensor(3, device='cuda:0')),\n",
       " (588, tensor(1, device='cuda:0')),\n",
       " (589, tensor(3, device='cuda:0')),\n",
       " (590, tensor(3, device='cuda:0')),\n",
       " (591, tensor(0, device='cuda:0')),\n",
       " (592, tensor(3, device='cuda:0')),\n",
       " (593, tensor(0, device='cuda:0')),\n",
       " (594, tensor(1, device='cuda:0')),\n",
       " (595, tensor(3, device='cuda:0')),\n",
       " (596, tensor(1, device='cuda:0')),\n",
       " (597, tensor(3, device='cuda:0')),\n",
       " (598, tensor(1, device='cuda:0')),\n",
       " (599, tensor(1, device='cuda:0')),\n",
       " (600, tensor(0, device='cuda:0')),\n",
       " (601, tensor(3, device='cuda:0')),\n",
       " (602, tensor(3, device='cuda:0')),\n",
       " (603, tensor(0, device='cuda:0')),\n",
       " (604, tensor(3, device='cuda:0')),\n",
       " (605, tensor(3, device='cuda:0')),\n",
       " (606, tensor(2, device='cuda:0')),\n",
       " (607, tensor(1, device='cuda:0')),\n",
       " (608, tensor(3, device='cuda:0')),\n",
       " (609, tensor(3, device='cuda:0')),\n",
       " (610, tensor(2, device='cuda:0')),\n",
       " (611, tensor(0, device='cuda:0')),\n",
       " (612, tensor(2, device='cuda:0')),\n",
       " (613, tensor(1, device='cuda:0')),\n",
       " (614, tensor(2, device='cuda:0')),\n",
       " (615, tensor(0, device='cuda:0')),\n",
       " (616, tensor(3, device='cuda:0')),\n",
       " (617, tensor(3, device='cuda:0')),\n",
       " (618, tensor(1, device='cuda:0')),\n",
       " (619, tensor(0, device='cuda:0')),\n",
       " (620, tensor(3, device='cuda:0')),\n",
       " (621, tensor(3, device='cuda:0')),\n",
       " (622, tensor(2, device='cuda:0')),\n",
       " (623, tensor(3, device='cuda:0')),\n",
       " (624, tensor(1, device='cuda:0')),\n",
       " (625, tensor(2, device='cuda:0')),\n",
       " (626, tensor(0, device='cuda:0')),\n",
       " (627, tensor(2, device='cuda:0')),\n",
       " (628, tensor(2, device='cuda:0')),\n",
       " (629, tensor(2, device='cuda:0')),\n",
       " (630, tensor(0, device='cuda:0')),\n",
       " (631, tensor(3, device='cuda:0')),\n",
       " (632, tensor(2, device='cuda:0')),\n",
       " (633, tensor(2, device='cuda:0')),\n",
       " (634, tensor(0, device='cuda:0')),\n",
       " (635, tensor(3, device='cuda:0')),\n",
       " (636, tensor(1, device='cuda:0')),\n",
       " (637, tensor(0, device='cuda:0')),\n",
       " (638, tensor(0, device='cuda:0')),\n",
       " (639, tensor(3, device='cuda:0')),\n",
       " (640, tensor(3, device='cuda:0')),\n",
       " (641, tensor(1, device='cuda:0')),\n",
       " (642, tensor(1, device='cuda:0')),\n",
       " (643, tensor(1, device='cuda:0')),\n",
       " (644, tensor(3, device='cuda:0')),\n",
       " (645, tensor(0, device='cuda:0')),\n",
       " (646, tensor(0, device='cuda:0')),\n",
       " (647, tensor(3, device='cuda:0')),\n",
       " (648, tensor(0, device='cuda:0')),\n",
       " (649, tensor(3, device='cuda:0')),\n",
       " (650, tensor(2, device='cuda:0')),\n",
       " (651, tensor(2, device='cuda:0')),\n",
       " (652, tensor(3, device='cuda:0')),\n",
       " (653, tensor(1, device='cuda:0')),\n",
       " (654, tensor(3, device='cuda:0')),\n",
       " (655, tensor(3, device='cuda:0')),\n",
       " (656, tensor(3, device='cuda:0')),\n",
       " (657, tensor(3, device='cuda:0')),\n",
       " (658, tensor(3, device='cuda:0')),\n",
       " (659, tensor(0, device='cuda:0')),\n",
       " (660, tensor(1, device='cuda:0')),\n",
       " (661, tensor(2, device='cuda:0')),\n",
       " (662, tensor(2, device='cuda:0')),\n",
       " (663, tensor(0, device='cuda:0')),\n",
       " (664, tensor(1, device='cuda:0')),\n",
       " (665, tensor(3, device='cuda:0')),\n",
       " (666, tensor(2, device='cuda:0')),\n",
       " (667, tensor(0, device='cuda:0')),\n",
       " (668, tensor(0, device='cuda:0')),\n",
       " (669, tensor(2, device='cuda:0')),\n",
       " (670, tensor(0, device='cuda:0')),\n",
       " (671, tensor(2, device='cuda:0')),\n",
       " (672, tensor(3, device='cuda:0')),\n",
       " (673, tensor(3, device='cuda:0')),\n",
       " (674, tensor(1, device='cuda:0')),\n",
       " (675, tensor(0, device='cuda:0')),\n",
       " (676, tensor(3, device='cuda:0')),\n",
       " (677, tensor(2, device='cuda:0')),\n",
       " (678, tensor(0, device='cuda:0')),\n",
       " (679, tensor(2, device='cuda:0')),\n",
       " (680, tensor(0, device='cuda:0')),\n",
       " (681, tensor(2, device='cuda:0')),\n",
       " (682, tensor(2, device='cuda:0')),\n",
       " (683, tensor(2, device='cuda:0')),\n",
       " (684, tensor(1, device='cuda:0')),\n",
       " (685, tensor(2, device='cuda:0')),\n",
       " (686, tensor(3, device='cuda:0')),\n",
       " (687, tensor(0, device='cuda:0')),\n",
       " (688, tensor(1, device='cuda:0')),\n",
       " (689, tensor(1, device='cuda:0')),\n",
       " (690, tensor(2, device='cuda:0')),\n",
       " (691, tensor(1, device='cuda:0')),\n",
       " (692, tensor(3, device='cuda:0')),\n",
       " (693, tensor(1, device='cuda:0')),\n",
       " (694, tensor(2, device='cuda:0')),\n",
       " (695, tensor(2, device='cuda:0')),\n",
       " (696, tensor(1, device='cuda:0')),\n",
       " (697, tensor(0, device='cuda:0')),\n",
       " (698, tensor(0, device='cuda:0')),\n",
       " (699, tensor(3, device='cuda:0')),\n",
       " (700, tensor(0, device='cuda:0')),\n",
       " (701, tensor(3, device='cuda:0')),\n",
       " (702, tensor(2, device='cuda:0')),\n",
       " (703, tensor(0, device='cuda:0')),\n",
       " (704, tensor(2, device='cuda:0')),\n",
       " (705, tensor(0, device='cuda:0')),\n",
       " (706, tensor(3, device='cuda:0')),\n",
       " (707, tensor(2, device='cuda:0')),\n",
       " (708, tensor(3, device='cuda:0')),\n",
       " (709, tensor(0, device='cuda:0')),\n",
       " (710, tensor(2, device='cuda:0')),\n",
       " (711, tensor(1, device='cuda:0')),\n",
       " (712, tensor(3, device='cuda:0')),\n",
       " (713, tensor(2, device='cuda:0')),\n",
       " (714, tensor(2, device='cuda:0')),\n",
       " (715, tensor(3, device='cuda:0')),\n",
       " (716, tensor(3, device='cuda:0')),\n",
       " (717, tensor(3, device='cuda:0')),\n",
       " (718, tensor(3, device='cuda:0')),\n",
       " (719, tensor(1, device='cuda:0')),\n",
       " (720, tensor(0, device='cuda:0')),\n",
       " (721, tensor(0, device='cuda:0')),\n",
       " (722, tensor(0, device='cuda:0')),\n",
       " (723, tensor(0, device='cuda:0')),\n",
       " (724, tensor(0, device='cuda:0')),\n",
       " (725, tensor(3, device='cuda:0')),\n",
       " (726, tensor(2, device='cuda:0')),\n",
       " (727, tensor(1, device='cuda:0')),\n",
       " (728, tensor(1, device='cuda:0')),\n",
       " (729, tensor(3, device='cuda:0')),\n",
       " (730, tensor(1, device='cuda:0')),\n",
       " (731, tensor(1, device='cuda:0')),\n",
       " (732, tensor(0, device='cuda:0')),\n",
       " (733, tensor(1, device='cuda:0')),\n",
       " (734, tensor(0, device='cuda:0')),\n",
       " (735, tensor(2, device='cuda:0')),\n",
       " (736, tensor(3, device='cuda:0')),\n",
       " (737, tensor(0, device='cuda:0')),\n",
       " (738, tensor(0, device='cuda:0')),\n",
       " (739, tensor(3, device='cuda:0')),\n",
       " (740, tensor(0, device='cuda:0')),\n",
       " (741, tensor(1, device='cuda:0')),\n",
       " (742, tensor(0, device='cuda:0')),\n",
       " (743, tensor(3, device='cuda:0')),\n",
       " (744, tensor(1, device='cuda:0')),\n",
       " (745, tensor(3, device='cuda:0')),\n",
       " (746, tensor(2, device='cuda:0')),\n",
       " (747, tensor(1, device='cuda:0')),\n",
       " (748, tensor(3, device='cuda:0')),\n",
       " (749, tensor(3, device='cuda:0')),\n",
       " (750, tensor(3, device='cuda:0')),\n",
       " (751, tensor(3, device='cuda:0')),\n",
       " (752, tensor(0, device='cuda:0')),\n",
       " (753, tensor(1, device='cuda:0')),\n",
       " (754, tensor(2, device='cuda:0')),\n",
       " (755, tensor(1, device='cuda:0')),\n",
       " (756, tensor(2, device='cuda:0')),\n",
       " (757, tensor(1, device='cuda:0')),\n",
       " (758, tensor(0, device='cuda:0')),\n",
       " (759, tensor(2, device='cuda:0')),\n",
       " (760, tensor(0, device='cuda:0')),\n",
       " (761, tensor(1, device='cuda:0')),\n",
       " (762, tensor(3, device='cuda:0')),\n",
       " (763, tensor(0, device='cuda:0')),\n",
       " (764, tensor(2, device='cuda:0')),\n",
       " (765, tensor(0, device='cuda:0')),\n",
       " (766, tensor(2, device='cuda:0')),\n",
       " (767, tensor(3, device='cuda:0')),\n",
       " (768, tensor(2, device='cuda:0')),\n",
       " (769, tensor(3, device='cuda:0')),\n",
       " (770, tensor(0, device='cuda:0')),\n",
       " (771, tensor(0, device='cuda:0')),\n",
       " (772, tensor(2, device='cuda:0')),\n",
       " (773, tensor(1, device='cuda:0')),\n",
       " (774, tensor(1, device='cuda:0')),\n",
       " (775, tensor(3, device='cuda:0')),\n",
       " (776, tensor(0, device='cuda:0')),\n",
       " (777, tensor(2, device='cuda:0')),\n",
       " (778, tensor(1, device='cuda:0')),\n",
       " (779, tensor(1, device='cuda:0')),\n",
       " (780, tensor(1, device='cuda:0')),\n",
       " (781, tensor(1, device='cuda:0')),\n",
       " (782, tensor(2, device='cuda:0')),\n",
       " (783, tensor(2, device='cuda:0')),\n",
       " (784, tensor(0, device='cuda:0')),\n",
       " (785, tensor(1, device='cuda:0')),\n",
       " (786, tensor(1, device='cuda:0')),\n",
       " (787, tensor(2, device='cuda:0')),\n",
       " (788, tensor(2, device='cuda:0')),\n",
       " (789, tensor(3, device='cuda:0')),\n",
       " (790, tensor(2, device='cuda:0')),\n",
       " (791, tensor(0, device='cuda:0')),\n",
       " (792, tensor(1, device='cuda:0')),\n",
       " (793, tensor(2, device='cuda:0')),\n",
       " (794, tensor(0, device='cuda:0')),\n",
       " (795, tensor(3, device='cuda:0')),\n",
       " (796, tensor(2, device='cuda:0')),\n",
       " (797, tensor(0, device='cuda:0')),\n",
       " (798, tensor(3, device='cuda:0')),\n",
       " (799, tensor(3, device='cuda:0')),\n",
       " (800, tensor(2, device='cuda:0')),\n",
       " (801, tensor(3, device='cuda:0')),\n",
       " (802, tensor(0, device='cuda:0')),\n",
       " (803, tensor(3, device='cuda:0')),\n",
       " (804, tensor(2, device='cuda:0')),\n",
       " (805, tensor(0, device='cuda:0')),\n",
       " (806, tensor(3, device='cuda:0')),\n",
       " (807, tensor(1, device='cuda:0')),\n",
       " (808, tensor(0, device='cuda:0')),\n",
       " (809, tensor(3, device='cuda:0')),\n",
       " (810, tensor(0, device='cuda:0')),\n",
       " (811, tensor(1, device='cuda:0')),\n",
       " (812, tensor(1, device='cuda:0')),\n",
       " (813, tensor(1, device='cuda:0')),\n",
       " (814, tensor(3, device='cuda:0')),\n",
       " (815, tensor(1, device='cuda:0')),\n",
       " (816, tensor(0, device='cuda:0')),\n",
       " (817, tensor(3, device='cuda:0')),\n",
       " (818, tensor(1, device='cuda:0')),\n",
       " (819, tensor(2, device='cuda:0')),\n",
       " (820, tensor(3, device='cuda:0')),\n",
       " (821, tensor(1, device='cuda:0')),\n",
       " (822, tensor(0, device='cuda:0')),\n",
       " (823, tensor(2, device='cuda:0')),\n",
       " (824, tensor(0, device='cuda:0')),\n",
       " (825, tensor(1, device='cuda:0')),\n",
       " (826, tensor(1, device='cuda:0')),\n",
       " (827, tensor(3, device='cuda:0')),\n",
       " (828, tensor(3, device='cuda:0')),\n",
       " (829, tensor(1, device='cuda:0')),\n",
       " (830, tensor(1, device='cuda:0')),\n",
       " (831, tensor(1, device='cuda:0')),\n",
       " (832, tensor(2, device='cuda:0')),\n",
       " (833, tensor(0, device='cuda:0')),\n",
       " (834, tensor(1, device='cuda:0')),\n",
       " (835, tensor(3, device='cuda:0')),\n",
       " (836, tensor(0, device='cuda:0')),\n",
       " (837, tensor(3, device='cuda:0')),\n",
       " (838, tensor(0, device='cuda:0')),\n",
       " (839, tensor(3, device='cuda:0')),\n",
       " (840, tensor(1, device='cuda:0')),\n",
       " (841, tensor(1, device='cuda:0')),\n",
       " (842, tensor(3, device='cuda:0')),\n",
       " (843, tensor(1, device='cuda:0')),\n",
       " (844, tensor(3, device='cuda:0')),\n",
       " (845, tensor(2, device='cuda:0')),\n",
       " (846, tensor(1, device='cuda:0')),\n",
       " (847, tensor(2, device='cuda:0')),\n",
       " (848, tensor(1, device='cuda:0')),\n",
       " (849, tensor(3, device='cuda:0')),\n",
       " (850, tensor(1, device='cuda:0')),\n",
       " (851, tensor(3, device='cuda:0')),\n",
       " (852, tensor(0, device='cuda:0')),\n",
       " (853, tensor(3, device='cuda:0')),\n",
       " (854, tensor(2, device='cuda:0')),\n",
       " (855, tensor(2, device='cuda:0')),\n",
       " (856, tensor(2, device='cuda:0')),\n",
       " (857, tensor(3, device='cuda:0')),\n",
       " (858, tensor(3, device='cuda:0')),\n",
       " (859, tensor(0, device='cuda:0')),\n",
       " (860, tensor(3, device='cuda:0')),\n",
       " (861, tensor(0, device='cuda:0')),\n",
       " (862, tensor(3, device='cuda:0')),\n",
       " (863, tensor(1, device='cuda:0')),\n",
       " (864, tensor(0, device='cuda:0')),\n",
       " (865, tensor(3, device='cuda:0')),\n",
       " (866, tensor(3, device='cuda:0')),\n",
       " (867, tensor(1, device='cuda:0')),\n",
       " (868, tensor(1, device='cuda:0')),\n",
       " (869, tensor(1, device='cuda:0')),\n",
       " (870, tensor(0, device='cuda:0')),\n",
       " (871, tensor(3, device='cuda:0')),\n",
       " (872, tensor(2, device='cuda:0')),\n",
       " (873, tensor(3, device='cuda:0')),\n",
       " (874, tensor(3, device='cuda:0')),\n",
       " (875, tensor(1, device='cuda:0')),\n",
       " (876, tensor(1, device='cuda:0')),\n",
       " (877, tensor(1, device='cuda:0')),\n",
       " (878, tensor(0, device='cuda:0')),\n",
       " (879, tensor(1, device='cuda:0')),\n",
       " (880, tensor(0, device='cuda:0')),\n",
       " (881, tensor(2, device='cuda:0')),\n",
       " (882, tensor(0, device='cuda:0')),\n",
       " (883, tensor(3, device='cuda:0')),\n",
       " (884, tensor(2, device='cuda:0')),\n",
       " (885, tensor(0, device='cuda:0')),\n",
       " (886, tensor(0, device='cuda:0')),\n",
       " (887, tensor(2, device='cuda:0')),\n",
       " (888, tensor(3, device='cuda:0')),\n",
       " (889, tensor(2, device='cuda:0')),\n",
       " (890, tensor(2, device='cuda:0')),\n",
       " (891, tensor(2, device='cuda:0')),\n",
       " (892, tensor(3, device='cuda:0')),\n",
       " (893, tensor(0, device='cuda:0')),\n",
       " (894, tensor(1, device='cuda:0')),\n",
       " (895, tensor(3, device='cuda:0')),\n",
       " (896, tensor(2, device='cuda:0')),\n",
       " (897, tensor(3, device='cuda:0')),\n",
       " (898, tensor(3, device='cuda:0')),\n",
       " (899, tensor(2, device='cuda:0')),\n",
       " (900, tensor(3, device='cuda:0')),\n",
       " (901, tensor(2, device='cuda:0')),\n",
       " (902, tensor(2, device='cuda:0')),\n",
       " (903, tensor(0, device='cuda:0')),\n",
       " (904, tensor(1, device='cuda:0')),\n",
       " (905, tensor(1, device='cuda:0')),\n",
       " (906, tensor(0, device='cuda:0')),\n",
       " (907, tensor(0, device='cuda:0')),\n",
       " (908, tensor(2, device='cuda:0')),\n",
       " (909, tensor(0, device='cuda:0')),\n",
       " (910, tensor(2, device='cuda:0')),\n",
       " (911, tensor(3, device='cuda:0')),\n",
       " (912, tensor(0, device='cuda:0')),\n",
       " (913, tensor(3, device='cuda:0')),\n",
       " (914, tensor(3, device='cuda:0')),\n",
       " (915, tensor(1, device='cuda:0')),\n",
       " (916, tensor(3, device='cuda:0')),\n",
       " (917, tensor(1, device='cuda:0')),\n",
       " (918, tensor(1, device='cuda:0')),\n",
       " (919, tensor(0, device='cuda:0')),\n",
       " (920, tensor(1, device='cuda:0')),\n",
       " (921, tensor(3, device='cuda:0')),\n",
       " (922, tensor(1, device='cuda:0')),\n",
       " (923, tensor(3, device='cuda:0')),\n",
       " (924, tensor(1, device='cuda:0')),\n",
       " (925, tensor(0, device='cuda:0')),\n",
       " (926, tensor(2, device='cuda:0')),\n",
       " (927, tensor(3, device='cuda:0')),\n",
       " (928, tensor(3, device='cuda:0')),\n",
       " (929, tensor(3, device='cuda:0')),\n",
       " (930, tensor(0, device='cuda:0')),\n",
       " (931, tensor(3, device='cuda:0')),\n",
       " (932, tensor(2, device='cuda:0')),\n",
       " (933, tensor(1, device='cuda:0')),\n",
       " (934, tensor(0, device='cuda:0')),\n",
       " (935, tensor(0, device='cuda:0')),\n",
       " (936, tensor(3, device='cuda:0')),\n",
       " (937, tensor(1, device='cuda:0')),\n",
       " (938, tensor(1, device='cuda:0')),\n",
       " (939, tensor(3, device='cuda:0')),\n",
       " (940, tensor(2, device='cuda:0')),\n",
       " (941, tensor(1, device='cuda:0')),\n",
       " (942, tensor(3, device='cuda:0')),\n",
       " (943, tensor(0, device='cuda:0')),\n",
       " (944, tensor(3, device='cuda:0')),\n",
       " (945, tensor(2, device='cuda:0')),\n",
       " (946, tensor(2, device='cuda:0')),\n",
       " (947, tensor(2, device='cuda:0')),\n",
       " (948, tensor(2, device='cuda:0')),\n",
       " (949, tensor(0, device='cuda:0')),\n",
       " (950, tensor(2, device='cuda:0')),\n",
       " (951, tensor(3, device='cuda:0')),\n",
       " (952, tensor(1, device='cuda:0')),\n",
       " (953, tensor(1, device='cuda:0')),\n",
       " (954, tensor(3, device='cuda:0')),\n",
       " (955, tensor(1, device='cuda:0')),\n",
       " (956, tensor(1, device='cuda:0')),\n",
       " (957, tensor(1, device='cuda:0')),\n",
       " (958, tensor(2, device='cuda:0')),\n",
       " (959, tensor(3, device='cuda:0')),\n",
       " (960, tensor(2, device='cuda:0')),\n",
       " (961, tensor(0, device='cuda:0')),\n",
       " (962, tensor(1, device='cuda:0')),\n",
       " (963, tensor(3, device='cuda:0')),\n",
       " (964, tensor(2, device='cuda:0')),\n",
       " (965, tensor(3, device='cuda:0')),\n",
       " (966, tensor(3, device='cuda:0')),\n",
       " (967, tensor(0, device='cuda:0')),\n",
       " (968, tensor(2, device='cuda:0')),\n",
       " (969, tensor(3, device='cuda:0')),\n",
       " (970, tensor(2, device='cuda:0')),\n",
       " (971, tensor(2, device='cuda:0')),\n",
       " (972, tensor(3, device='cuda:0')),\n",
       " (973, tensor(1, device='cuda:0')),\n",
       " (974, tensor(0, device='cuda:0')),\n",
       " (975, tensor(1, device='cuda:0')),\n",
       " (976, tensor(2, device='cuda:0')),\n",
       " (977, tensor(3, device='cuda:0')),\n",
       " (978, tensor(3, device='cuda:0')),\n",
       " (979, tensor(1, device='cuda:0')),\n",
       " (980, tensor(1, device='cuda:0')),\n",
       " (981, tensor(1, device='cuda:0')),\n",
       " (982, tensor(2, device='cuda:0')),\n",
       " (983, tensor(2, device='cuda:0')),\n",
       " (984, tensor(3, device='cuda:0')),\n",
       " (985, tensor(1, device='cuda:0')),\n",
       " (986, tensor(1, device='cuda:0')),\n",
       " (987, tensor(2, device='cuda:0')),\n",
       " (988, tensor(0, device='cuda:0')),\n",
       " (989, tensor(1, device='cuda:0')),\n",
       " (990, tensor(2, device='cuda:0')),\n",
       " (991, tensor(3, device='cuda:0')),\n",
       " (992, tensor(2, device='cuda:0')),\n",
       " (993, tensor(3, device='cuda:0')),\n",
       " (994, tensor(3, device='cuda:0')),\n",
       " (995, tensor(2, device='cuda:0')),\n",
       " (996, tensor(0, device='cuda:0')),\n",
       " (997, tensor(0, device='cuda:0')),\n",
       " (998, tensor(3, device='cuda:0')),\n",
       " (999, tensor(1, device='cuda:0'))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses = predict(\n",
    "    forward_fn=forward_and_get_arg_max,\n",
    "    picker_fn=pick_from_classifier,\n",
    "    eval_dataset=test_data,\n",
    ")\n",
    "\n",
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43aeaf51-0577-4430-aff8-6249e0093416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94d048aa-3455-4ccd-aafe-9fb879b26752",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_with_ids = []\n",
    "\n",
    "for i in range(len(responses)):\n",
    "    responses_with_ids.append((test_data[i][\"id\"], responses[i][1].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86e3b418-073f-48ce-a579-b845ae631f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(responses_with_ids, columns=[\"ID\", \"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95c17786-a3e1-47b9-b097-37ad7aa1f017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>109</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  answer\n",
       "0   26       1\n",
       "1   29       3\n",
       "2   37       2\n",
       "3   70       1\n",
       "4  109       2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a6fa9f6-9278-4111-8950-b9d93b90aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a0ff0-258c-445a-a8ac-660028db86ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss39",
   "language": "python",
   "name": "faiss39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
