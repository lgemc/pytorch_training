{
 "cells": [
  {
   "cell_type": "code",
   "id": "b1bfc35f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T04:32:11.218594Z",
     "start_time": "2025-05-15T04:32:11.212854Z"
    }
   },
   "source": [
    "def is_colab():\n",
    "    \"\"\"Check if the code is running in Google Colab.\"\"\"\n",
    "    try:\n",
    "        import google.colab\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "# Use the function to conditionally run magic commands\n",
    "if is_colab():\n",
    "    # Run Colab-specific magic commands\n",
    "    print(\"Running in Colab, executing magic commands.\")\n",
    "    !rm -rf microproyecto3NLP/\n",
    "    !git clone https://github.com/cjohana031/microproyecto3NLP\n",
    "    !cp -R microproyecto3NLP/* .\n",
    "    # Add any other Colab-specific setup\n",
    "else:\n",
    "    # Alternative setup for non-Colab environments\n",
    "    print(\"Not running in Colab, nothing else is needed.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in Colab, nothing else is needed.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "71532f81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-15T04:33:48.966124Z",
     "start_time": "2025-05-15T04:33:48.953594Z"
    }
   },
   "source": [
    "from datasets import DatasetDict\n",
    "questions = DatasetDict.load_from_disk(\"data/pubmed_QA_train.json\")\n",
    "eval_questions = DatasetDict.load_from_disk(\"../data/pubmed_QA_eval.json\")\n",
    "corpus = DatasetDict.load_from_disk(\"../data/pubmed_500K.json\")"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file: '/home/lmanrique/Learn/maestria/pln/NLP_avanzado/pytorch_by_levels/010_rag/notebooks/data/pubmed_QA_train.json/dataset_dict.json'. Expected to load a `DatasetDict` object, but provided path is not a `DatasetDict`.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mdatasets\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m DatasetDict\n\u001B[0;32m----> 2\u001B[0m questions \u001B[38;5;241m=\u001B[39m \u001B[43mDatasetDict\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_from_disk\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata/pubmed_QA_train.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m eval_questions \u001B[38;5;241m=\u001B[39m DatasetDict\u001B[38;5;241m.\u001B[39mload_from_disk(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../data/pubmed_QA_eval.json\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m corpus \u001B[38;5;241m=\u001B[39m DatasetDict\u001B[38;5;241m.\u001B[39mload_from_disk(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../data/pubmed_500K.json\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/faiss39/lib/python3.9/site-packages/datasets/dataset_dict.py:1399\u001B[0m, in \u001B[0;36mDatasetDict.load_from_disk\u001B[0;34m(dataset_dict_path, keep_in_memory, storage_options)\u001B[0m\n\u001B[1;32m   1395\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m fs\u001B[38;5;241m.\u001B[39misfile(dataset_info_path) \u001B[38;5;129;01mand\u001B[39;00m fs\u001B[38;5;241m.\u001B[39misfile(dataset_state_json_path):\n\u001B[1;32m   1396\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\n\u001B[1;32m   1397\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo such file: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset_dict_json_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. Expected to load a `DatasetDict` object, but got a `Dataset`. Please use either `datasets.load_from_disk` or `Dataset.load_from_disk` instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1398\u001B[0m         )\n\u001B[0;32m-> 1399\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\n\u001B[1;32m   1400\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo such file: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset_dict_json_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. Expected to load a `DatasetDict` object, but provided path is not a `DatasetDict`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1401\u001B[0m     )\n\u001B[1;32m   1403\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m fs\u001B[38;5;241m.\u001B[39mopen(dataset_dict_json_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m   1404\u001B[0m     splits \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mload(f)[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplits\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: No such file: '/home/lmanrique/Learn/maestria/pln/NLP_avanzado/pytorch_by_levels/010_rag/notebooks/data/pubmed_QA_train.json/dataset_dict.json'. Expected to load a `DatasetDict` object, but provided path is not a `DatasetDict`."
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a26be72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'excerpt', 'statement', 'question', 'distractors'],\n",
       "        num_rows: 16890\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['id', 'excerpt', 'statement', 'question', 'distractors'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'question', 'option'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'content', 'contents', 'PMID'],\n",
       "        num_rows: 500000\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(questions)\n",
    "display(eval_questions)\n",
    "display(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f651dedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1f24a551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500000/500000 [00:21<00:00, 23692.03 examples/s]\n",
      "c:\\Users\\cjoha\\OneDrive\\Documentos\\Maestria en Inteligencia Artificial\\Semestre 3\\Modelos avanzados para el procesamiento de lenguaje natural\\CompetenciaNLPAvanzado\\.venv\\lib\\site-packages\\dill\\_dill.py:414: PicklingWarning: Cannot locate reference to <class 'transformers_modules.nomic-ai.nomic-bert-2048.7710840340a098cfb869c4f65e87cf2b1b70caca.modeling_hf_nomic_bert.NomicBertPreTrainedModel'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "c:\\Users\\cjoha\\OneDrive\\Documentos\\Maestria en Inteligencia Artificial\\Semestre 3\\Modelos avanzados para el procesamiento de lenguaje natural\\CompetenciaNLPAvanzado\\.venv\\lib\\site-packages\\dill\\_dill.py:414: PicklingWarning: Cannot pickle <class 'transformers_modules.nomic-ai.nomic-bert-2048.7710840340a098cfb869c4f65e87cf2b1b70caca.modeling_hf_nomic_bert.NomicBertPreTrainedModel'>: transformers_modules.nomic-ai.nomic-bert-2048.7710840340a098cfb869c4f65e87cf2b1b70caca.modeling_hf_nomic_bert.NomicBertPreTrainedModel has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "Batches: 100%|██████████| 1/1 [00:21<00:00, 21.94s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:13<00:00, 13.28s/it]es/s]\n",
      "Map: 100%|██████████| 50/50 [00:35<00:00,  1.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def add_document_search_prefix(batch):\n",
    "    batch[\"contents\"] = [\"search_document: \" + content for content in batch[\"contents\"]]\n",
    "    return batch\n",
    "\n",
    "def encode_fn(batch, model):\n",
    "    batch['embeddings']= model.encode(batch[\"contents\"], show_progress_bar=True, convert_to_tensor=True)\n",
    "    return batch\n",
    "\n",
    "corpus = corpus.map(add_document_search_prefix, batched=True, batch_size=32)\n",
    "corpus2=corpus[\"train\"].select(range(50)).map(encode_fn, fn_kwargs={\"model\": model}, batched=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63b55614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus2['PMID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1f12a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import sqlite_vec\n",
    "from sqlite_vec import serialize_float32\n",
    "\n",
    "db = sqlite3.connect(\"temp.db\")\n",
    "db.enable_load_extension(True)\n",
    "sqlite_vec.load(db)\n",
    "db.enable_load_extension(False)\n",
    "\n",
    "db.execute(\n",
    "    \"\"\"\n",
    "        CREATE TABLE sentences(\n",
    "          id INTEGER PRIMARY KEY,\n",
    "          sentence TEXT\n",
    "        );\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "with db:\n",
    "    for example in corpus2:\n",
    "        db.execute(\"INSERT INTO sentences(id, sentence) VALUES(?, ?)\", [example['PMID'],example['contents']])\n",
    "\n",
    "\n",
    "serialize_float32(example['embeddings'])\n",
    "db.execute(\n",
    "    \"\"\"\n",
    "        CREATE VIRTUAL TABLE vec_sentences USING vec0(\n",
    "          id INTEGER PRIMARY KEY,\n",
    "          sentence_embedding FLOAT[768]\n",
    "        );\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "with db:\n",
    "    for example in corpus2:\n",
    "         db.execute(\n",
    "            \"INSERT INTO vec_sentences(id, sentence_embedding) VALUES(?, ?)\",\n",
    "            [example['PMID'], serialize_float32(example['embeddings'])],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "214d9e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 19.193632125854492, 'Effect of human erythrocyte stromata on complement activation. Stroma from either normal or PNH-like red cells is capable of inhibiting, to some extent, lysis in the sucrose test and enhancing lysis in the acidified-serum test. The same opposing effects are displayed by the exclusion peaks from Sephadex G-200 obtained from each stroma preparation, suggesting that the same factor could be responsible for both activities. Stromata and peaks also induce lysis of PNH-like cells in unacidified serum, indicating activation of complement through the alternate pathway. This is confirmed by immunoelectrophoretic observation. When serum previously activated through the alternate pathway is used in the sucrose test the amount of lysis is markedly reduced. This would indicate that the classical pathway activation can be controlled by the alternate pathway. The possible clinical significance of these factors in determining the haemolytic crisis in PNH patients is discussed.')\n",
      "(72, 19.3155517578125, 'Preparation and characterization of an enzymatically active immobilized derivative of myosin. Purified skeletal muscle myosin (EC 3.6.1.3) has been covalently bound to Sepharose 4B by the cyanogen bromide procedure. The resulting complex, Sepharose-Myosin, possesses adenosine triphosphatase activity and is relatively stable for long periods of time. Under optimal binding conditions, approximately 33% of the specific ATPase activity of the bound myosin is retained. Polyacrylamide gel electrophoresis of polypeptides released from denatured Sepharose-Myosin indicates that 85% of the myosin is attached to the agarose beads through the heavy chains and the remainder through the light chains, in agreement with predictions of binding and release based upon either the lysine contents or molecular weights of themyosin subunits. The adenosine triphosphatase of the immobilized myosin has been investigated under conditions of varying pH, ionic strength, and cation concentration. The ATPase profiles of immobilized myosin are quite similar to those for free myosin, however subtle differences are found. The Sepharose-Myosin ATPase is not as sensitive as myosin to alterations in salt concentration and the apparent KM is approximately two-fold higher than that of myosin. These differences are probably due to chemical modification in the region of the attachment site(s) to the agarose beads and hydration and diffusion limitations imposed by the polymeric agarose matrix.')\n",
      "(70, 19.391193389892578, 'Specificity studies on alpha-mannosidases using oligosaccharides from mannosidosis urine as substrates. Oligosaccharides containing terminal non-reducing alpha(1 leads to 2)-, alpha(1 leads to 3)-, and alpha(1 leads to 6)-linked mannose residues, isolated from human and bovine mannosidosis urines were used as substrates to test the specificities of acidic alpha-mannosidases isolated from human and bovine liver. The enzymes released all the alpha-linked mannose residues from each oligosaccharide and were most effective on the smallest substrate. Enzyme A in each case was less active on the oligosaccharides than alpha-mannosidase B2, even though the apparent Km value for the substrates was the same with each enzyme. The human acidic alpha-mannosidases were also found to be more active on substrates isolated from human rather than bovine mannosidosis urine. Human alpha-mannosidase C, which has a neutral pH optimum when assayed with a synthetic substrate, did not hydrolyse any of the oligosaccharides at neutral pH, but was found to be active at an acidic pH.')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "question = \"search_query: sucrose\"\n",
    "\n",
    "query_embedding = model.encode(question)\n",
    "query_embedding\n",
    "\n",
    "results = db.execute(\n",
    "    \"\"\"\n",
    "      SELECT\n",
    "        vec_sentences.id,\n",
    "        distance,\n",
    "        sentence\n",
    "      FROM vec_sentences\n",
    "      LEFT JOIN sentences ON sentences.id = vec_sentences.id\n",
    "      WHERE sentence_embedding MATCH ?\n",
    "        AND k = 3\n",
    "      ORDER BY distance\n",
    "    \"\"\",\n",
    "    [query_embedding.astype(np.float32)],\n",
    ").fetchall()\n",
    "\n",
    "for row in results:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d642aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 768)\n"
     ]
    }
   ],
   "source": [
    "sentences = ['search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten']\n",
    "emb = model.encode(sentences)\n",
    "print(emb.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
