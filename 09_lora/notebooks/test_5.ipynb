{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "714b0c60-fc7b-4afa-9680-62c0ea10c69d",
   "metadata": {},
   "source": [
    "# LORA: Low Rank Adaptation: an efficient way to fine tune large language models\n",
    "\n",
    "When we have a specific task to perform with large language models we have various options:\n",
    "\n",
    "1. Use the model as it is using prompt engineering\n",
    "2. Fine tune the whole model, updating all its weights\n",
    "3. Fine tune only some layers instead the whole model.\n",
    "\n",
    "Pros and cons of each one:\n",
    "\n",
    "| Approach             | Pros                                                                 | Cons                                                                 |\n",
    "|----------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|\n",
    "| Prompt Engineering   | - Fast and cheap                                                     | - Limited control over behavior                                      |\n",
    "|                      | - No training or infrastructure needed                               | - Performance highly sensitive to prompt wording                     |\n",
    "|                      | - Easily updated or changed                                          | - May hit model limits on specific tasks                             |\n",
    "| Fine-Tune Full Model | - Full control over model behavior                                   | - Very resource-intensive (GPU, time, data)                          |\n",
    "|                      | - Better performance on domain-specific or complex tasks             | - Risk of overfitting or catastrophic forgetting                    |\n",
    "|                      | - Can learn new capabilities                                         | - Requires re-deployment of large models                             |\n",
    "| LoRA Fine-Tuning     | - Much less compute and memory than full fine-tuning                 | - Slightly less flexible than full fine-tuning                      |\n",
    "|                      | - Retains base model unchanged (can swap adapters)                   | - Still needs training pipeline setup                                |\n",
    "|                      | - Modular and efficient for multiple tasks/domains                   | - May not reach full model’s potential on highly specialized tasks   |\n",
    "\n",
    "And important remarks:\n",
    "\n",
    "- If you want to use a high llm from a provider, like GPT from OpenAI or Gemini from google, you simply can't fine tune this model, so prompt engineering is your only available option\n",
    "- If you have low models, like 1B or 8B, them does not perform very well in very specific tasks, but you can perform fine tune over them with limited resources, greatly improving performance\n",
    "\n",
    "So, right now we are going to implement and compare two ways to resolve a specific task: **prompt engineering** leaving the original model as it is, and the second way to test: **lora fine-tuning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8c3ec1-d250-4593-9ed9-5a978279369f",
   "metadata": {},
   "source": [
    "# The task: Question and answers with RACE\n",
    "\n",
    "We want to train a system with the ability of solve questions and answers where the answer should be picked from a list of options:\n",
    "\n",
    "```text\n",
    "Context: A subject which seems to have been insufficiently studied by doctors and psychologists is the influence of geography and climate on the psychological and physical health of mankind. There seems no doubt that the general character of the landscape, the relative length of day and night, and the climate must all play a big part in determining what kind of people we are.\n",
    "It is true that a few studies have been made. Where all the inhabitants of a particular area enjoy exceptionally good or bad health, scientists have identified contributory factors such as the presence or absence of substances like iodine, fluoride, calcium, or iron in the water supply, or perhaps types of land that provide breeding places for pests like mosquitoes or rats.\n",
    "Moreover, we can all generalize about types of people we have met. Those living in countries with long dark winters are apt to be less talkative and less vivacious than inhabitants of countries where the climate is more equable. And where the olive and the orange grow, there the inhabitants are cheerful, talkative, and spontaneous.\n",
    "But these commonplace generalizations are inadequate: the influence of climate and geography should be studied in depth. Do all mountain dwellers live to a ripe old age? Does the drinking of wine, rather than beer, result in a sunny and open temperament? Is the strength and height of one of the Kenyan tribes due to their habitual drinking of the blood of cows?\n",
    "We are not yet sure of the answers to such questions, but let us hope that something of benefit to mankind may eventually result from such studies.\n",
    "\n",
    "Question: According to the author, research into the influence of geography and climate should  _  .\n",
    "\n",
    "Options:\n",
    "A) focus on some unknown aspects\n",
    "B) be pursued on a larger scale\n",
    "C) be carried out within a larger scope\n",
    "D) go much deeper\n",
    "\n",
    "Answer: D\n",
    "```\n",
    "\n",
    "We are using the `transformers` dataset called `ehvoy/race`, composed by 97k of questions and answers, but\n",
    "to reduce training times we are going to use only subsets with `context.length < 800`, reducing the original\n",
    "dataset to a length of `800` in the train set and `56` items in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91016d2d-e16a-43f9-a952-c22ddbb6f385",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "01682fc5-9148-44c6-a2bf-164f8249dbc2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:21:14.483409Z",
     "start_time": "2025-05-13T03:21:14.432126Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import math\n",
    "\n",
    "login(token=\"\")"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "7d55fda2-ed26-4ff9-98a0-5fcfb6c7bbf6",
   "metadata": {},
   "source": [
    "### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "id": "a97c73ae-2d7c-4443-93d4-61abd6cc91a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:21:14.492164Z",
     "start_time": "2025-05-13T03:21:14.487915Z"
    }
   },
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "MAX_ARTICLE_CHAR_LENGTH = 800\n",
    "MAX_TOKEN_LENGTH = 512\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "11019a1f-4dda-4319-a178-589e87b5bbfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:21:14.647843Z",
     "start_time": "2025-05-13T03:21:14.644767Z"
    }
   },
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "d13a9fe9-b0ec-4570-9e8b-991435f76380",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:21:15.153553Z",
     "start_time": "2025-05-13T03:21:14.691050Z"
    }
   },
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "bea7e26f-9d96-4a93-8646-b1556b122d6a",
   "metadata": {},
   "source": [
    "### Download dataset race"
   ]
  },
  {
   "cell_type": "code",
   "id": "281e06d0-0142-4fbd-a7bf-f9fdd9415484",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:21:15.843777Z",
     "start_time": "2025-05-13T03:21:15.295944Z"
    }
   },
   "source": [
    "ds_full = load_dataset(\"ehovy/race\", \"high\", trust_remote_code=True)"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "95817e8e-9bff-4376-b907-b1d4bf032ff6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:21:15.922634Z",
     "start_time": "2025-05-13T03:21:15.917137Z"
    }
   },
   "source": [
    "ds_test = ds_full.get('test')\n",
    "filtered_test_data = ds_test.filter(\n",
    "    lambda example: len(example['article']) < MAX_ARTICLE_CHAR_LENGTH\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "9d0b2fc0-625c-4987-8b0c-596166978630",
   "metadata": {},
   "source": [
    "### Funtion\n",
    "#### Model Evaluation\n",
    "\n",
    "After the language model processes the context and question up to \"Answer:\", it produces logits. These are raw scores for each possible next word (token) in its vocabulary.\n",
    "\n",
    "- Softmax (or `log_softmax`): This function converts these raw scores (logits) into probabilities (numbers between 0 and 1 that sum up to 1). We often use `log_softmax` for numerical stability, as working with logarithms of probabilities helps avoid issues with extremely small numbers.\n",
    "- Evaluation: We then look at the probabilities (or log-probabilities) the model assigned specifically to your answer option letters ('A', 'B', 'C', 'D').\n",
    "- Prediction: The predicted answer is the option (A, B, C, or D) that the model assigned the highest probability to. This fulfills the `argmax s ∈S P (s|c)` requirement, because the logarithm doesn't change the order, so the maximum of the logarithm is the maximum of the probability itself."
   ]
  },
  {
   "cell_type": "code",
   "id": "55001cab-9261-4db2-8fae-edcca048bb97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:21:16.035644Z",
     "start_time": "2025-05-13T03:21:16.028966Z"
    }
   },
   "source": [
    "def model_evaluation(model_llama, prompt, test_data):\n",
    "    total_correct_original = 0\n",
    "    total_examples_original = 0\n",
    "    \n",
    "    model_llama.eval()\n",
    "    with torch.no_grad():\n",
    "        for example in tqdm(test_data):\n",
    "            \n",
    "            options_str = \"\\n\".join([f\"{chr(65+i)}) {opt}\" for i, opt in enumerate(example['options'])])\n",
    "            prompt_for_inference = prompt.format(example['article'], example['question'], options_str)\n",
    "    \n",
    "            inputs = tokenizer(prompt_for_inference, return_tensors=\"pt\", truncation=True, max_length=MAX_TOKEN_LENGTH).to(device)\n",
    "    \n",
    "            outputs = model_llama(**inputs)\n",
    "            logits_next_token = outputs.logits[:, -1, :] \n",
    "            log_probabilities = torch.nn.functional.log_softmax(logits_next_token, dim=-1)\n",
    "    \n",
    "            predicted_answer_char = None\n",
    "            max_log_prob_for_option = -float('inf')\n",
    "    \n",
    "            for i in range(len(example['options'])):\n",
    "                option_char = chr(ord('A') + i)\n",
    "                option_char_token_ids = tokenizer.encode(option_char, add_special_tokens=False)\n",
    "                current_option_char_token_id = option_char_token_ids[0]\n",
    "                \n",
    "                if current_option_char_token_id in range(log_probabilities.shape[-1]):\n",
    "                    current_log_prob = log_probabilities[:, current_option_char_token_id].item()\n",
    "\n",
    "                    if current_log_prob > max_log_prob_for_option:\n",
    "                        max_log_prob_for_option = current_log_prob\n",
    "                        predicted_answer_char = option_char\n",
    "\n",
    "            if predicted_answer_char == example['answer']:\n",
    "                total_correct_original += 1\n",
    "            total_examples_original += 1\n",
    "            \n",
    "        return total_correct_original, total_examples_original "
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "767f1462-c290-4ca1-b6c9-72518091a861",
   "metadata": {},
   "source": [
    "# The model: Llama 3.2 1B\n",
    "\n",
    "Nowadays, we have a lot of small models with open weights offered by big tech that can be used for free and downloaded\n",
    "from various repositories like hugging face.\n",
    "\n",
    "On this list we can find:\n",
    "- Gemma: A model trained by Google offered in various sizes, included 3B\n",
    "- Phi: A model trained by Microsoft\n",
    "- Llama: A model trained by Meta\n",
    "\n",
    "Special mentions: SmolLM2, a model built by hugging face community, OpenELM, a model built by apple\n",
    "\n",
    "*All this models are based on decoder only architectures, which makes them easier to train*\n",
    "\n",
    "**Our chosen model is Llama 3.2 1B**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c80fbba-8783-4fee-8ec4-1d6a46c3493a",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "id": "d126c8f1-f543-4c24-9d84-f4615c6e640c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:21:17.314141Z",
     "start_time": "2025-05-13T03:21:16.108661Z"
    }
   },
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model moves to {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moves to cuda\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "71e5d0a2-7ddf-4c5d-9fbf-6aa620d4e6e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:21:18.604982Z",
     "start_time": "2025-05-13T03:21:17.387632Z"
    }
   },
   "source": [
    "prompt_for_inference = (\n",
    "    \"\"\"You are a smart question answering model.  \n",
    "    Answer the question based on the next information, \n",
    "    and at the end you will find the answer options.\n",
    "    Choose the best one, only give the letter of the answer which could be A, B, C or D.\\n\\n\n",
    "    Context: {}\\n\\n\n",
    "    Question: {}\\n\\n\n",
    "    Options:\\n{}\\n\\n\n",
    "    Answer:\"\"\"\n",
    ")\n",
    "\n",
    "total_correct_original, total_examples_original = model_evaluation(model, prompt_for_inference, filtered_test_data)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:01<00:00, 46.24it/s]\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "647d7db5-d23c-4947-a3dd-cbb7bd1b50e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:21:18.681832Z",
     "start_time": "2025-05-13T03:21:18.678908Z"
    }
   },
   "source": [
    "accuracy_original = total_correct_original / total_examples_original\n",
    "\n",
    "print(f\"\\n--- Resultados de la Evaluación en Test (Modelo Original) ---\")\n",
    "print(f\"Ejemplos Totales: {total_examples_original}\")\n",
    "print(f\"Predicciones Correctas: {total_correct_original}\")\n",
    "print(f\"Exactitud (Accuracy): {accuracy_original * 100:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Resultados de la Evaluación en Test (Modelo Original) ---\n",
      "Ejemplos Totales: 56\n",
      "Predicciones Correctas: 20\n",
      "Exactitud (Accuracy): 35.71%\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "643512dc-cee5-43de-a90d-b86383d655c1",
   "metadata": {},
   "source": [
    "## Fine tuning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d464859-1b1a-4003-b99b-259bd000557f",
   "metadata": {},
   "source": [
    "### Data tokenization and transformation\n",
    "\n",
    "This `transform_and_tokenize_example` function prepares each example in your dataset for model training, specifically for fine-tuning.\n",
    "\n",
    "Constructing the Prompt:\n",
    "\n",
    "- First, format the `article`, `question`, and `options` into a structured text (`prompt_template`) that the model can understand, ending with \"Answer:\".\n",
    "- Then, add the actual `answer` (e.g., 'A') to the end of this prompt to create the `full_text_for_training`.\n",
    "\n",
    "Tokenization:\n",
    "\n",
    "- Convert this `full_text_for_training` into numbers (tokens) that the model understands, using the `tokenizer`.\n",
    "- Ensure the length is `MAX_TOKEN_LENGTH` (truncating if too long, padding if too short).\n",
    "- `return_offsets_mapping=True` is crucial: it generates a map that tells you which tokens correspond to which characters in the original text.\n",
    "Label Masking:\n",
    "\n",
    "- Create a copy of the `input_ids` (the input tokens) to use as `labels`.\n",
    "- The key point: Find where the actual answer begins (the character 'A', 'B', 'C', or 'D') within the tokenized text (`full_text_for_training`).\n",
    "- Using `offset_mapping`, identify the index of the token where the answer begins.\n",
    "- Finally, set the `labels` of all tokens before the answer (the context, the question, the options, and the \"Answer:\" part) to `-100`.\n",
    "\n",
    "Why `-100`?\n",
    "\n",
    "During fine-tuning, the model only has to learn how to generate the `answer`. By setting `-100` to the context `labels`, the model's cost function ignores these tokens, focusing solely on optimizing the answer prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "267698c9-3611-42d4-95fa-734de718b8d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:21:18.796262Z",
     "start_time": "2025-05-13T03:21:18.790592Z"
    }
   },
   "source": [
    "def transform_and_tokenize_example(example):\n",
    "    options_str = \"\\n\".join([f\"{chr(65+i)}) {opt}\" for i, opt in enumerate(example['options'])])\n",
    "    \n",
    "    prompt_template =  (\n",
    "    \"\"\"Context: {}\\n\n",
    "    Question: {}\\n\n",
    "    Options:\\n{}\\n\n",
    "    Answer:\"\"\"\n",
    "    ).format(example['article'], example['question'], options_str)\n",
    "        \n",
    "    full_text_for_training = prompt_template + \" \" + example['answer'] # Add a space before the answer for clarity in tokenization\n",
    "\n",
    "    tokenized_full = tokenizer(\n",
    "        full_text_for_training,\n",
    "        truncation=True,\n",
    "        max_length=MAX_TOKEN_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        return_offsets_mapping=True \n",
    "    )\n",
    "\n",
    "    labels = tokenized_full[\"input_ids\"].copy()\n",
    "\n",
    "    answer_start_char_idx = full_text_for_training.find(\"Answer:\")\n",
    "    if answer_start_char_idx != -1:\n",
    "        answer_token_char_start_idx = answer_start_char_idx + len(\"Answer: \")\n",
    "\n",
    "        answer_token_start_index = -1\n",
    "        for i, (start_offset, end_offset) in enumerate(tokenized_full['offset_mapping']):\n",
    "\n",
    "            if start_offset <= answer_token_char_start_idx < end_offset:\n",
    "                answer_token_start_index = i\n",
    "                break\n",
    "        \n",
    "        if answer_token_start_index != -1:\n",
    "            for i in range(answer_token_start_index):\n",
    "                labels[i] = -100\n",
    "            \n",
    "    tokenized_full[\"labels\"] = labels\n",
    "    del tokenized_full[\"offset_mapping\"] \n",
    "    return tokenized_full"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "id": "abeeb0c6-3207-4414-bb5d-18decf1863fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:21:19.222373Z",
     "start_time": "2025-05-13T03:21:18.907251Z"
    }
   },
   "source": [
    "processed_ds = DatasetDict()\n",
    "\n",
    "for split, data in ds_full.items():\n",
    "    if split != 'test':\n",
    "        filtered_data = data.filter(\n",
    "            lambda example: len(example['article']) < MAX_ARTICLE_CHAR_LENGTH,\n",
    "            desc=f\"Filtrando artículos en {split}\"\n",
    "        )\n",
    "\n",
    "        mapped_data = filtered_data.map(\n",
    "            transform_and_tokenize_example,\n",
    "            batched=False,\n",
    "            remove_columns=filtered_data.column_names,\n",
    "            desc=f\"Tokenizando {split}\"\n",
    "        )\n",
    "        processed_ds[split] = mapped_data\n",
    "        processed_ds[split].set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    processed_ds.get(\"train\"),\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    processed_ds.get(\"validation\"),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=data_collator\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "3683491c-2de3-4a6a-9279-e477436f8005",
   "metadata": {},
   "source": [
    "### Class Low Rank Adaptation "
   ]
  },
  {
   "cell_type": "code",
   "id": "f09eccd7-0880-47ac-b6b5-8ebd1d380591",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:21:19.306807Z",
     "start_time": "2025-05-13T03:21:19.300217Z"
    }
   },
   "source": [
    "class LoraLinear(torch.nn.Module):\n",
    "    def __init__(self, linear_layer, alpha = 1, r = 1):\n",
    "        super().__init__()\n",
    "        self.linear_layer = linear_layer.to(torch.float32) \n",
    "        self.r = r\n",
    "        fan_in = self.linear_layer.in_features\n",
    "        fan_out = self.linear_layer.out_features\n",
    "        self.lora_A = torch.nn.Parameter(torch.zeros((fan_in, r), device=linear_layer.weight.device)) \n",
    "        self.lora_B = torch.nn.Parameter(torch.zeros((r, fan_out), device=linear_layer.weight.device)) \n",
    "        torch.nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        self.linear_layer.weight.requires_grad = False\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        self.training = mode\n",
    "        if not mode:\n",
    "            self.merged_weight = (self.linear_layer.weight.transpose(0,1) + self.lora_A @ self.lora_B).to(torch.float16)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            x = x.to(torch.float32) \n",
    "            output = self.linear_layer(x)\n",
    "            output += x @ self.lora_A @ self.lora_B\n",
    "            output = output.to(torch.float16) \n",
    "        else:\n",
    "            if not hasattr(self, 'merged_weight'):\n",
    "                self.merged_weight = (self.linear_layer.weight.transpose(0,1) + self.lora_A @ self.lora_B).to(torch.float16)\n",
    "            output = x @ self.merged_weight\n",
    "        return output"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "1cf42c65-1edf-44fe-a38f-900afa64fbd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:21:19.438346Z",
     "start_time": "2025-05-13T03:21:19.421590Z"
    }
   },
   "source": [
    "# Congelamos parametros\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Reemplazamos las capas lineales del mecanismo de atención por capas LoRA\n",
    "for layer in model.model.layers:\n",
    "    if hasattr(layer, 'self_attn'):\n",
    "        layer.self_attn.q_proj = LoraLinear(layer.self_attn.q_proj, r=16)\n",
    "        layer.self_attn.k_proj = LoraLinear(layer.self_attn.k_proj, r=16)\n",
    "        layer.self_attn.v_proj = LoraLinear(layer.self_attn.v_proj, r=16)\n",
    "        layer.self_attn.o_proj = LoraLinear(layer.self_attn.o_proj, r=16)\n",
    "\n",
    "params_without_lora = 0\n",
    "params_with_lora = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if 'self_attn' in name and 'linear_layer' in name: # This counts the original linear layer's parameters\n",
    "        params_without_lora += param.numel()\n",
    "    if param.requires_grad:\n",
    "        params_with_lora += param.numel()\n",
    "        \n",
    "print(f'Parámetros sin LoRA (originales no entrenables): {params_without_lora:,} || Parámetros con LoRA (entrenables): {params_with_lora:,} || Porcentaje de parámetros con LoRA: {100 * params_with_lora / (params_without_lora + params_with_lora):.2f}%')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros sin LoRA (originales no entrenables): 167,772,160 || Parámetros con LoRA (entrenables): 3,407,872 || Porcentaje de parámetros con LoRA: 1.99%\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "a4bcb443-1fdc-49d2-9a77-7c17a72c0b47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:21:19.545980Z",
     "start_time": "2025-05-13T03:21:19.537297Z"
    }
   },
   "source": [
    "model.to(device)\n",
    "\n",
    "print(f\"Lora model moves to {device}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lora model moves to cuda\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "id": "aacdd241-953a-4503-8fb2-ebabe1f8ae90",
   "metadata": {},
   "source": [
    "### Traing/fine tuning loop"
   ]
  },
  {
   "cell_type": "code",
   "id": "5e190790-2399-4208-a562-58da22f129dc",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-05-13T03:25:31.221356Z",
     "start_time": "2025-05-13T03:21:19.656395Z"
    }
   },
   "source": [
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Iniciando entrenamiento...\")\n",
    "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    print(f\"\\n--- Época {epoch + 1}/{NUM_EPOCHS} ---\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_train_loss += loss.item()\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_description(f\"Época {epoch+1}, Batch {batch_idx+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"Fin de Época {epoch + 1}: Pérdida de Entrenamiento Promedio = {avg_train_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    print(f\"\\nEvaluando al final de la época {epoch + 1}...\")\n",
    "    with torch.no_grad():\n",
    "        for eval_batch in tqdm(eval_dataloader):\n",
    "            eval_batch = {k: v.to(device) for k, v in eval_batch.items()}\n",
    "            outputs = model(**eval_batch)\n",
    "            total_eval_loss += outputs.loss.item()\n",
    "    avg_eval_loss = total_eval_loss / len(eval_dataloader)\n",
    "    print(f\"Fin de Época {epoch + 1}: Pérdida de Validación Promedio = {avg_eval_loss:.4f}\")\n",
    "\n",
    "progress_bar.close()\n",
    "print(\"Entrenamiento completado.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2409 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Época 1/3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 1, Batch 803, Loss: 1.6844:  33%|███▎      | 803/2409 [01:22<02:45,  9.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 1: Pérdida de Entrenamiento Promedio = 2.1072\n",
      "\n",
      "Evaluando al final de la época 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "  9%|▉         | 3/34 [00:00<00:01, 26.51it/s]\u001B[A\n",
      " 18%|█▊        | 6/34 [00:00<00:01, 26.22it/s]\u001B[A\n",
      " 26%|██▋       | 9/34 [00:00<00:00, 26.19it/s]\u001B[A\n",
      " 35%|███▌      | 12/34 [00:00<00:00, 26.11it/s]\u001B[A\n",
      " 44%|████▍     | 15/34 [00:00<00:00, 26.11it/s]\u001B[A\n",
      " 53%|█████▎    | 18/34 [00:00<00:00, 26.11it/s]\u001B[A\n",
      " 62%|██████▏   | 21/34 [00:00<00:00, 26.16it/s]\u001B[A\n",
      " 71%|███████   | 24/34 [00:00<00:00, 26.21it/s]\u001B[A\n",
      " 79%|███████▉  | 27/34 [00:01<00:00, 26.11it/s]\u001B[A\n",
      " 88%|████████▊ | 30/34 [00:01<00:00, 26.16it/s]\u001B[A\n",
      "100%|██████████| 34/34 [00:01<00:00, 26.14it/s]\u001B[A\n",
      "Época 2, Batch 1, Loss: 2.0634:  33%|███▎      | 804/2409 [01:23<13:16,  2.02it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 1: Pérdida de Validación Promedio = 2.3132\n",
      "\n",
      "--- Época 2/3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 2, Batch 803, Loss: 1.1751:  67%|██████▋   | 1606/2409 [02:46<01:22,  9.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 2: Pérdida de Entrenamiento Promedio = 1.7175\n",
      "\n",
      "Evaluando al final de la época 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "  9%|▉         | 3/34 [00:00<00:01, 26.53it/s]\u001B[A\n",
      " 18%|█▊        | 6/34 [00:00<00:01, 26.35it/s]\u001B[A\n",
      " 26%|██▋       | 9/34 [00:00<00:00, 26.33it/s]\u001B[A\n",
      " 35%|███▌      | 12/34 [00:00<00:00, 26.17it/s]\u001B[A\n",
      " 44%|████▍     | 15/34 [00:00<00:00, 26.20it/s]\u001B[A\n",
      " 53%|█████▎    | 18/34 [00:00<00:00, 26.16it/s]\u001B[A\n",
      " 62%|██████▏   | 21/34 [00:00<00:00, 26.21it/s]\u001B[A\n",
      " 71%|███████   | 24/34 [00:00<00:00, 26.22it/s]\u001B[A\n",
      " 79%|███████▉  | 27/34 [00:01<00:00, 26.23it/s]\u001B[A\n",
      " 88%|████████▊ | 30/34 [00:01<00:00, 26.22it/s]\u001B[A\n",
      "100%|██████████| 34/34 [00:01<00:00, 26.19it/s]\u001B[A\n",
      "Época 3, Batch 1, Loss: 1.0944:  67%|██████▋   | 1607/2409 [02:47<06:37,  2.02it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 2: Pérdida de Validación Promedio = 2.5852\n",
      "\n",
      "--- Época 3/3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 3, Batch 803, Loss: 0.8945: 100%|██████████| 2409/2409 [04:10<00:00,  9.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 3: Pérdida de Entrenamiento Promedio = 1.2885\n",
      "\n",
      "Evaluando al final de la época 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "  9%|▉         | 3/34 [00:00<00:01, 26.49it/s]\u001B[A\n",
      " 18%|█▊        | 6/34 [00:00<00:01, 26.28it/s]\u001B[A\n",
      " 26%|██▋       | 9/34 [00:00<00:00, 26.22it/s]\u001B[A\n",
      " 35%|███▌      | 12/34 [00:00<00:00, 26.20it/s]\u001B[A\n",
      " 44%|████▍     | 15/34 [00:00<00:00, 26.14it/s]\u001B[A\n",
      " 53%|█████▎    | 18/34 [00:00<00:00, 26.17it/s]\u001B[A\n",
      " 62%|██████▏   | 21/34 [00:00<00:00, 26.19it/s]\u001B[A\n",
      " 71%|███████   | 24/34 [00:00<00:00, 26.19it/s]\u001B[A\n",
      " 79%|███████▉  | 27/34 [00:01<00:00, 26.22it/s]\u001B[A\n",
      " 88%|████████▊ | 30/34 [00:01<00:00, 26.21it/s]\u001B[A\n",
      "100%|██████████| 34/34 [00:01<00:00, 26.16it/s]\u001B[A\n",
      "Época 3, Batch 803, Loss: 0.8945: 100%|██████████| 2409/2409 [04:11<00:00,  9.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 3: Pérdida de Validación Promedio = 2.8917\n",
      "Entrenamiento completado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "6292e301-36fe-4f01-92ec-f36705e2bd71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:25:56.717398Z",
     "start_time": "2025-05-13T03:25:31.311408Z"
    }
   },
   "source": "torch.save(model.state_dict(), \"llama3.2-1B_fine-tuned.pt\")",
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "2337d19f-6064-41c1-a847-b4e1beea61a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:25:58.730533Z",
     "start_time": "2025-05-13T03:25:56.794435Z"
    }
   },
   "source": "model.load_state_dict(torch.load(\"llama3.2-1B_fine-tuned.pt\"))",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "0a0df7c3-ec5e-46a6-9022-b909432863ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:25:58.858555Z",
     "start_time": "2025-05-13T03:25:58.856134Z"
    }
   },
   "source": [
    "prompt_for_inference_ft = (\n",
    "        \"\"\"Context: {}\\n\n",
    "        Question: {}\\n\n",
    "        Options:\\n{}\\n\n",
    "        Answer:\"\"\"\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "id": "f5b703eb-3395-4df8-a2a5-d2551dc86c5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:26:00.111217Z",
     "start_time": "2025-05-13T03:25:58.970965Z"
    }
   },
   "source": [
    "total_correct_fine_tuning, total_examples_fine_tuning = model_evaluation(model, prompt_for_inference_ft, filtered_test_data)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:01<00:00, 49.75it/s]\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "id": "4ebc7d59-a41e-4ad0-9947-4c4b8f1ded92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T03:26:00.229436Z",
     "start_time": "2025-05-13T03:26:00.226077Z"
    }
   },
   "source": [
    "accuracy = total_correct_fine_tuning / total_examples_fine_tuning\n",
    "\n",
    "print(f\"\\n--- Resultados de la Evaluación en Test (Probabilidad Basada) ---\")\n",
    "print(f\"Ejemplos Totales: {total_examples_fine_tuning}\")\n",
    "print(f\"Predicciones Correctas: {total_correct_fine_tuning}\")\n",
    "print(f\"Exactitud (Accuracy): {accuracy * 100:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Resultados de la Evaluación en Test (Probabilidad Basada) ---\n",
      "Ejemplos Totales: 56\n",
      "Predicciones Correctas: 22\n",
      "Exactitud (Accuracy): 39.29%\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "id": "2912768d-41d8-4594-86e4-39a167b634b8",
   "metadata": {},
   "source": [
    "# Result analysis\n",
    "\n",
    "As can be shown in the previous experiments, the fine-tuned model with lora over performs the baseline model. The results are as follows:\n",
    "| Model | Accuracy |\n",
    "|--------|----------|\n",
    "| Baseline | 0.35 |\n",
    "| Fine-tuned | 0.41 |\n",
    "\n",
    "Which is a significant improvement over the base-line model.\n",
    "\n",
    "The race instead of being an easy task (as can be simple classification) is a task that needs advanced reasoning skills, common sense and deep analysis in order\n",
    "to solve those problems, so a result of `0.41` with only a portion of the entire dataset, and small amount of epochs (3) is a good result.\n",
    "\n",
    "For example, on 2017 the state-of-the-art models were able to achieve 43% accuracy on this dataset [(as said on the original dataset paper)](https://arxiv.org/abs/1704.04683)\n",
    "\n",
    "Our results still far from human performance, humans can reach 95% of accuracy, but our 42% is a good result whit limited resources.\n",
    "\n",
    "\n",
    "![race results](https://media.githubusercontent.com/media/lgemc/pytorch_training/refs/heads/master/static/race_q_and_a_results.png)\n",
    "<br> Source: [papers with code](https://paperswithcode.com/sota/question-answering-on-race)\n",
    "\n",
    "## Interesting facts\n",
    "\n",
    "We have noticed that the model is very sensitive to small changes in the prompt used, for example, at one experiment we added an `space` to the\n",
    "prompt, and it started to predict always the option `C`, obviously ending in a downgraded performance of 25% accuracy (which is the same as a random model can reach).\n",
    "\n",
    "## About computation efficiency of LORA\n",
    "\n",
    "On our experiments we have noticed the next computation and time ussage:\n",
    "\n",
    "| Model      | Time until finish | Ram ussed |\n",
    "|------------|-------------------|-----------|\n",
    "| Baseline   | 8.5 Minutes       | 13GB      |\n",
    "| Fine-tuned | 3 Minutes         | 5GB       |\n",
    "\n",
    "So, LORA consumes a half of the resources required in order to train the full model, which is a huge improvement.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.9",
   "language": "python",
   "name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
