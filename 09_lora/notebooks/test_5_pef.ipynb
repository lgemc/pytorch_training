{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "714b0c60-fc7b-4afa-9680-62c0ea10c69d",
   "metadata": {},
   "source": [
    "# LORA: Low Rank Adaptation: an efficient way to fine tune large language models\n",
    "\n",
    "When we have a specific task to perform with large language models we have various options:\n",
    "\n",
    "1. Use the model as it is using prompt engineering\n",
    "2. Fine tune the whole model, updating all its weights\n",
    "3. Fine tune only some layers instead the whole model.\n",
    "\n",
    "Pros and cons of each one:\n",
    "\n",
    "| Approach             | Pros                                                                 | Cons                                                                 |\n",
    "|----------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|\n",
    "| Prompt Engineering   | - Fast and cheap                                                     | - Limited control over behavior                                      |\n",
    "|                      | - No training or infrastructure needed                               | - Performance highly sensitive to prompt wording                     |\n",
    "|                      | - Easily updated or changed                                          | - May hit model limits on specific tasks                             |\n",
    "| Fine-Tune Full Model | - Full control over model behavior                                   | - Very resource-intensive (GPU, time, data)                          |\n",
    "|                      | - Better performance on domain-specific or complex tasks             | - Risk of overfitting or catastrophic forgetting                    |\n",
    "|                      | - Can learn new capabilities                                         | - Requires re-deployment of large models                             |\n",
    "| LoRA Fine-Tuning     | - Much less compute and memory than full fine-tuning                 | - Slightly less flexible than full fine-tuning                      |\n",
    "|                      | - Retains base model unchanged (can swap adapters)                   | - Still needs training pipeline setup                                |\n",
    "|                      | - Modular and efficient for multiple tasks/domains                   | - May not reach full model’s potential on highly specialized tasks   |\n",
    "\n",
    "And important remarks:\n",
    "\n",
    "- If you want to use a high llm from a provider, like GPT from OpenAI or Gemini from google, you simply can't fine tune this model, so prompt engineering is your only available option\n",
    "- If you have low models, like 1B or 8B, them does not perform very well in very specific tasks, but you can perform fine tune over them with limited resources, greatly improving performance\n",
    "\n",
    "So, right now we are going to implement and compare two ways to resolve a specific task: **prompt engineering** leaving the original model as it is, and the second way to test: **lora fine-tuning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8c3ec1-d250-4593-9ed9-5a978279369f",
   "metadata": {},
   "source": [
    "# The task: Question and answers with RACE\n",
    "\n",
    "We want to train a system with the ability of solve questions and answers where the answer should be picked from a list of options:\n",
    "\n",
    "```text\n",
    "Context: A subject which seems to have been insufficiently studied by doctors and psychologists is the influence of geography and climate on the psychological and physical health of mankind. There seems no doubt that the general character of the landscape, the relative length of day and night, and the climate must all play a big part in determining what kind of people we are.\n",
    "It is true that a few studies have been made. Where all the inhabitants of a particular area enjoy exceptionally good or bad health, scientists have identified contributory factors such as the presence or absence of substances like iodine, fluoride, calcium, or iron in the water supply, or perhaps types of land that provide breeding places for pests like mosquitoes or rats.\n",
    "Moreover, we can all generalize about types of people we have met. Those living in countries with long dark winters are apt to be less talkative and less vivacious than inhabitants of countries where the climate is more equable. And where the olive and the orange grow, there the inhabitants are cheerful, talkative, and spontaneous.\n",
    "But these commonplace generalizations are inadequate: the influence of climate and geography should be studied in depth. Do all mountain dwellers live to a ripe old age? Does the drinking of wine, rather than beer, result in a sunny and open temperament? Is the strength and height of one of the Kenyan tribes due to their habitual drinking of the blood of cows?\n",
    "We are not yet sure of the answers to such questions, but let us hope that something of benefit to mankind may eventually result from such studies.\n",
    "\n",
    "Question: According to the author, research into the influence of geography and climate should  _  .\n",
    "\n",
    "Options:\n",
    "A) focus on some unknown aspects\n",
    "B) be pursued on a larger scale\n",
    "C) be carried out within a larger scope\n",
    "D) go much deeper\n",
    "\n",
    "Answer: D\n",
    "```\n",
    "\n",
    "We are using the `transformers` dataset called `ehvoy/race`, composed by 97k of questions and answers, but\n",
    "to reduce training times we are going to use only subsets with `context.length < 800`, reducing the original\n",
    "dataset to a length of `800` in the train set and `56` items in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91016d2d-e16a-43f9-a952-c22ddbb6f385",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01682fc5-9148-44c6-a2bf-164f8249dbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/alloc/data/.cache/virtualenvs/app-1Cl30gML-py3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    get_scheduler,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import os\n",
    "import math\n",
    "\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d55fda2-ed26-4ff9-98a0-5fcfb6c7bbf6",
   "metadata": {},
   "source": [
    "### Set variables"
   ]
  },
  {
   "cell_type": "code",
   "id": "a97c73ae-2d7c-4443-93d4-61abd6cc91a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T00:00:30.050006Z",
     "start_time": "2025-05-13T00:00:30.044844Z"
    }
   },
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "MAX_ARTICLE_CHAR_LENGTH = 800\n",
    "MAX_TOKEN_LENGTH = 512\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 10"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11019a1f-4dda-4319-a178-589e87b5bbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d13a9fe9-b0ec-4570-9e8b-991435f76380",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea7e26f-9d96-4a93-8646-b1556b122d6a",
   "metadata": {},
   "source": [
    "### Download dataset race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "281e06d0-0142-4fbd-a7bf-f9fdd9415484",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_full = load_dataset(\"ehovy/race\", \"high\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95817e8e-9bff-4376-b907-b1d4bf032ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_test = ds_full.get('test')\n",
    "filtered_test_data = ds_test.filter(\n",
    "    lambda example: len(example['article']) < MAX_ARTICLE_CHAR_LENGTH\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0b2fc0-625c-4987-8b0c-596166978630",
   "metadata": {},
   "source": [
    "### Funtion\n",
    "#### Model Evaluation\n",
    "\n",
    "After the language model processes the context and question up to \"Answer:\", it produces logits. These are raw scores for each possible next word (token) in its vocabulary.\n",
    "\n",
    "- Softmax (or `log_softmax`): This function converts these raw scores (logits) into probabilities (numbers between 0 and 1 that sum up to 1). We often use `log_softmax` for numerical stability, as working with logarithms of probabilities helps avoid issues with extremely small numbers.\n",
    "- Evaluation: We then look at the probabilities (or log-probabilities) the model assigned specifically to your answer option letters ('A', 'B', 'C', 'D').\n",
    "- Prediction: The predicted answer is the option (A, B, C, or D) that the model assigned the highest probability to. This fulfills the `argmax s ∈S P (s|c)` requirement, because the logarithm doesn't change the order, so the maximum of the logarithm is the maximum of the probability itself."
   ]
  },
  {
   "cell_type": "code",
   "id": "55001cab-9261-4db2-8fae-edcca048bb97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T23:59:51.564256Z",
     "start_time": "2025-05-12T23:59:51.556194Z"
    }
   },
   "source": [
    "def model_evaluation(model_llama, prompt, test_data):\n",
    "    total_correct_original = 0\n",
    "    total_examples_original = 0\n",
    "    \n",
    "    model_llama.eval()\n",
    "    with torch.no_grad():\n",
    "        for example in tqdm(test_data):\n",
    "            \n",
    "            options_str = \"\\n\".join([f\"{chr(65+i)}) {opt}\" for i, opt in enumerate(example['options'])])\n",
    "            prompt_for_inference = prompt.format(example['article'], example['question'], options_str)\n",
    "    \n",
    "            inputs = tokenizer(prompt_for_inference, return_tensors=\"pt\", truncation=True, max_length=MAX_TOKEN_LENGTH).to(device)\n",
    "    \n",
    "            outputs = model_llama(**inputs)\n",
    "            logits_next_token = outputs.logits[:, -1, :] \n",
    "            log_probabilities = torch.nn.functional.log_softmax(logits_next_token, dim=-1)\n",
    "    \n",
    "            predicted_answer_char = None\n",
    "            max_log_prob_for_option = -float('inf')\n",
    "    \n",
    "            for i in range(len(example['options'])):\n",
    "                option_char = chr(ord('A') + i)\n",
    "                option_char_token_ids = tokenizer.encode(option_char, add_special_tokens=False)\n",
    "                current_option_char_token_id = option_char_token_ids[0]\n",
    "                \n",
    "                if current_option_char_token_id in range(log_probabilities.shape[-1]):\n",
    "                    current_log_prob = log_probabilities[:, current_option_char_token_id].item()\n",
    "\n",
    "                    if current_log_prob > max_log_prob_for_option:\n",
    "                        max_log_prob_for_option = current_log_prob\n",
    "                        predicted_answer_char = option_char\n",
    "\n",
    "            if predicted_answer_char == example['answer']:\n",
    "                total_correct_original += 1\n",
    "            total_examples_original += 1\n",
    "            \n",
    "        return total_correct_original, total_examples_original "
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "767f1462-c290-4ca1-b6c9-72518091a861",
   "metadata": {},
   "source": [
    "# The model: Llama 3.2 1B\n",
    "\n",
    "Nowadays, we have a lot of small models with open weights offered by big tech that can be used for free and downloaded\n",
    "from various repositories like hugging face.\n",
    "\n",
    "On this list we can find:\n",
    "- Gemma: A model trained by Google offered in various sizes, included 3B\n",
    "- Phi: A model trained by Microsoft\n",
    "- Llama: A model trained by Meta\n",
    "\n",
    "Special mentions: SmolLM2, a model built by hugging face community, OpenELM, a model built by apple\n",
    "\n",
    "*All this models are based on decoder only architectures, which makes them easier to train*\n",
    "\n",
    "**Our chosen model is Llama 3.2 1B**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c80fbba-8783-4fee-8ec4-1d6a46c3493a",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d126c8f1-f543-4c24-9d84-f4615c6e640c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moves to cuda\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model moves to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "71e5d0a2-7ddf-4c5d-9fbf-6aa620d4e6e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T00:14:54.455127Z",
     "start_time": "2025-05-13T00:14:54.452348Z"
    }
   },
   "source": [
    "prompt_for_inference = (\n",
    "    \"\"\"You are a smart question answering model.  \n",
    "    Answer the question based on the next information, \n",
    "    and at the end you will find the answer options.\n",
    "    Choose the best one, only give the letter of the answer which could be A, B, C or D.\\n\\n\n",
    "    Context: {}\\n\\n\n",
    "    Question: {}\\n\\n\n",
    "    Options:\\n{}\\n\\n\n",
    "    Answer:\"\"\"\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "647d7db5-d23c-4947-a3dd-cbb7bd1b50e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Resultados de la Evaluación en Test (Modelo Original) ---\n",
      "Ejemplos Totales: 56\n",
      "Predicciones Correctas: 20\n",
      "Exactitud (Accuracy): 35.71%\n"
     ]
    }
   ],
   "source": [
    "accuracy_original = total_correct_original / total_examples_original\n",
    "\n",
    "print(f\"\\n--- Resultados de la Evaluación en Test (Modelo Original) ---\")\n",
    "print(f\"Ejemplos Totales: {total_examples_original}\")\n",
    "print(f\"Predicciones Correctas: {total_correct_original}\")\n",
    "print(f\"Exactitud (Accuracy): {accuracy_original * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643512dc-cee5-43de-a90d-b86383d655c1",
   "metadata": {},
   "source": [
    "## Fine tuning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d464859-1b1a-4003-b99b-259bd000557f",
   "metadata": {},
   "source": [
    "### Data tokenization and transformation\n",
    "\n",
    "This `transform_and_tokenize_example` function prepares each example in your dataset for model training, specifically for fine-tuning.\n",
    "\n",
    "Constructing the Prompt:\n",
    "\n",
    "- First, format the `article`, `question`, and `options` into a structured text (`prompt_template`) that the model can understand, ending with \"Answer:\".\n",
    "- Then, add the actual `answer` (e.g., 'A') to the end of this prompt to create the `full_text_for_training`.\n",
    "\n",
    "Tokenization:\n",
    "\n",
    "- Convert this `full_text_for_training` into numbers (tokens) that the model understands, using the `tokenizer`.\n",
    "- Ensure the length is `MAX_TOKEN_LENGTH` (truncating if too long, padding if too short).\n",
    "- `return_offsets_mapping=True` is crucial: it generates a map that tells you which tokens correspond to which characters in the original text.\n",
    "Label Masking:\n",
    "\n",
    "- Create a copy of the `input_ids` (the input tokens) to use as `labels`.\n",
    "- The key point: Find where the actual answer begins (the character 'A', 'B', 'C', or 'D') within the tokenized text (`full_text_for_training`).\n",
    "- Using `offset_mapping`, identify the index of the token where the answer begins.\n",
    "- Finally, set the `labels` of all tokens before the answer (the context, the question, the options, and the \"Answer:\" part) to `-100`.\n",
    "\n",
    "Why `-100`?\n",
    "\n",
    "During fine-tuning, the model only has to learn how to generate the `answer`. By setting `-100` to the context `labels`, the model's cost function ignores these tokens, focusing solely on optimizing the answer prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "267698c9-3611-42d4-95fa-734de718b8d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T00:00:04.561692Z",
     "start_time": "2025-05-13T00:00:04.556308Z"
    }
   },
   "source": [
    "def transform_and_tokenize_example(example):\n",
    "    options_str = \"\\n\".join([f\"{chr(65+i)}) {opt}\" for i, opt in enumerate(example['options'])])\n",
    "    \n",
    "    prompt_template =  (\n",
    "    \"\"\"Context: {}\\n\n",
    "    Question: {}\\n\n",
    "    Options:\\n{}\\n\n",
    "    Answer:\"\"\"\n",
    "    ).format(example['article'], example['question'], options_str)\n",
    "        \n",
    "    full_text_for_training = prompt_template + \" \" + example['answer'] # Add a space before the answer for clarity in tokenization\n",
    "\n",
    "    tokenized_full = tokenizer(\n",
    "        full_text_for_training,\n",
    "        truncation=True,\n",
    "        max_length=MAX_TOKEN_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        return_offsets_mapping=True \n",
    "    )\n",
    "\n",
    "    labels = tokenized_full[\"input_ids\"].copy()\n",
    "\n",
    "    answer_start_char_idx = full_text_for_training.find(\"Answer:\")\n",
    "    if answer_start_char_idx != -1:\n",
    "        answer_token_char_start_idx = answer_start_char_idx + len(\"Answer: \")\n",
    "\n",
    "        answer_token_start_index = -1\n",
    "        for i, (start_offset, end_offset) in enumerate(tokenized_full['offset_mapping']):\n",
    "\n",
    "            if start_offset <= answer_token_char_start_idx < end_offset:\n",
    "                answer_token_start_index = i\n",
    "                break\n",
    "        \n",
    "        if answer_token_start_index != -1:\n",
    "            for i in range(answer_token_start_index):\n",
    "                labels[i] = -100\n",
    "            \n",
    "    tokenized_full[\"labels\"] = labels\n",
    "    del tokenized_full[\"offset_mapping\"] \n",
    "    return tokenized_full"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "abeeb0c6-3207-4414-bb5d-18decf1863fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizando train: 100%|██████████| 803/803 [00:01<00:00, 633.79 examples/s]\n",
      "Tokenizando validation: 100%|██████████| 34/34 [00:00<00:00, 548.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "processed_ds = DatasetDict()\n",
    "\n",
    "for split, data in ds_full.items():\n",
    "    if split != 'test':\n",
    "        filtered_data = data.filter(\n",
    "            lambda example: len(example['article']) < MAX_ARTICLE_CHAR_LENGTH,\n",
    "            desc=f\"Filtrando artículos en {split}\"\n",
    "        )\n",
    "\n",
    "        mapped_data = filtered_data.map(\n",
    "            transform_and_tokenize_example,\n",
    "            batched=False,\n",
    "            remove_columns=filtered_data.column_names,\n",
    "            desc=f\"Tokenizando {split}\"\n",
    "        )\n",
    "        processed_ds[split] = mapped_data\n",
    "        processed_ds[split].set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    processed_ds.get(\"train\"),\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    processed_ds.get(\"validation\"),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3683491c-2de3-4a6a-9279-e477436f8005",
   "metadata": {},
   "source": [
    "### Class Low Rank Adaptation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f09eccd7-0880-47ac-b6b5-8ebd1d380591",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoraLinear(torch.nn.Module):\n",
    "    def __init__(self, linear_layer, alpha = 1, r = 1):\n",
    "        super().__init__()\n",
    "        self.linear_layer = linear_layer.to(torch.float32) \n",
    "        self.r = r\n",
    "        fan_in = self.linear_layer.in_features\n",
    "        fan_out = self.linear_layer.out_features\n",
    "        self.lora_A = torch.nn.Parameter(torch.zeros((fan_in, r), device=linear_layer.weight.device)) \n",
    "        self.lora_B = torch.nn.Parameter(torch.zeros((r, fan_out), device=linear_layer.weight.device)) \n",
    "        torch.nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        self.linear_layer.weight.requires_grad = False\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        self.training = mode\n",
    "        if not mode:\n",
    "            self.merged_weight = (self.linear_layer.weight.transpose(0,1) + self.lora_A @ self.lora_B).to(torch.float16)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            x = x.to(torch.float32) \n",
    "            output = self.linear_layer(x)\n",
    "            output += x @ self.lora_A @ self.lora_B\n",
    "            output = output.to(torch.float16) \n",
    "        else:\n",
    "            if not hasattr(self, 'merged_weight'):\n",
    "                self.merged_weight = (self.linear_layer.weight.transpose(0,1) + self.lora_A @ self.lora_B).to(torch.float16)\n",
    "            output = x @ self.merged_weight\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1cf42c65-1edf-44fe-a38f-900afa64fbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros sin LoRA (originales no entrenables): 167,772,160 || Parámetros con LoRA (entrenables): 3,407,872 || Porcentaje de parámetros con LoRA: 1.99%\n"
     ]
    }
   ],
   "source": [
    "# Congelamos parametros\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Reemplazamos las capas lineales del mecanismo de atención por capas LoRA\n",
    "for layer in model.model.layers:\n",
    "    if hasattr(layer, 'self_attn'):\n",
    "        layer.self_attn.q_proj = LoraLinear(layer.self_attn.q_proj, r=16)\n",
    "        layer.self_attn.k_proj = LoraLinear(layer.self_attn.k_proj, r=16)\n",
    "        layer.self_attn.v_proj = LoraLinear(layer.self_attn.v_proj, r=16)\n",
    "        layer.self_attn.o_proj = LoraLinear(layer.self_attn.o_proj, r=16)\n",
    "\n",
    "params_without_lora = 0\n",
    "params_with_lora = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if 'self_attn' in name and 'linear_layer' in name: # This counts the original linear layer's parameters\n",
    "        params_without_lora += param.numel()\n",
    "    if param.requires_grad:\n",
    "        params_with_lora += param.numel()\n",
    "        \n",
    "print(f'Parámetros sin LoRA (originales no entrenables): {params_without_lora:,} || Parámetros con LoRA (entrenables): {params_with_lora:,} || Porcentaje de parámetros con LoRA: {100 * params_with_lora / (params_without_lora + params_with_lora):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4bcb443-1fdc-49d2-9a77-7c17a72c0b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lora model moves to cuda\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "print(f\"Lora model moves to {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacdd241-953a-4503-8fb2-ebabe1f8ae90",
   "metadata": {},
   "source": [
    "### Traing/fine tuning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e190790-2399-4208-a562-58da22f129dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 1, Batch 1, Loss: 2.6039:   0%|          | 1/2409 [00:00<05:34,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Época 1/3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 1, Batch 803, Loss: 1.9934:  33%|███▎      | 803/2409 [01:44<03:26,  7.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 1: Pérdida de Entrenamiento Promedio = 2.1068\n",
      "\n",
      "Evaluando al final de la época 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "  9%|▉         | 3/34 [00:00<00:01, 26.09it/s]\u001B[A\n",
      " 18%|█▊        | 6/34 [00:00<00:01, 24.87it/s]\u001B[A\n",
      " 26%|██▋       | 9/34 [00:00<00:01, 24.95it/s]\u001B[A\n",
      " 35%|███▌      | 12/34 [00:00<00:00, 25.58it/s]\u001B[A\n",
      " 44%|████▍     | 15/34 [00:00<00:00, 25.24it/s]\u001B[A\n",
      " 53%|█████▎    | 18/34 [00:00<00:00, 25.31it/s]\u001B[A\n",
      " 62%|██████▏   | 21/34 [00:00<00:00, 25.33it/s]\u001B[A\n",
      " 71%|███████   | 24/34 [00:00<00:00, 25.40it/s]\u001B[A\n",
      " 79%|███████▉  | 27/34 [00:01<00:00, 25.41it/s]\u001B[A\n",
      " 88%|████████▊ | 30/34 [00:01<00:00, 25.47it/s]\u001B[A\n",
      "100%|██████████| 34/34 [00:01<00:00, 25.33it/s]\u001B[A\n",
      "Época 2, Batch 1, Loss: 1.5833:  33%|███▎      | 804/2409 [01:46<14:21,  1.86it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 1: Pérdida de Validación Promedio = 2.3208\n",
      "\n",
      "--- Época 2/3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 2, Batch 803, Loss: 1.6596:  67%|██████▋   | 1606/2409 [03:31<01:42,  7.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 2: Pérdida de Entrenamiento Promedio = 1.7206\n",
      "\n",
      "Evaluando al final de la época 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "  9%|▉         | 3/34 [00:00<00:01, 25.95it/s]\u001B[A\n",
      " 18%|█▊        | 6/34 [00:00<00:01, 25.32it/s]\u001B[A\n",
      " 26%|██▋       | 9/34 [00:00<00:00, 25.31it/s]\u001B[A\n",
      " 35%|███▌      | 12/34 [00:00<00:00, 25.23it/s]\u001B[A\n",
      " 44%|████▍     | 15/34 [00:00<00:00, 24.80it/s]\u001B[A\n",
      " 53%|█████▎    | 18/34 [00:00<00:00, 24.21it/s]\u001B[A\n",
      " 62%|██████▏   | 21/34 [00:00<00:00, 23.65it/s]\u001B[A\n",
      " 71%|███████   | 24/34 [00:01<00:00, 23.00it/s]\u001B[A\n",
      " 79%|███████▉  | 27/34 [00:01<00:00, 22.91it/s]\u001B[A\n",
      " 88%|████████▊ | 30/34 [00:01<00:00, 22.94it/s]\u001B[A\n",
      "100%|██████████| 34/34 [00:01<00:00, 23.93it/s]\u001B[A\n",
      "Época 3, Batch 1, Loss: 1.1813:  67%|██████▋   | 1607/2409 [03:32<07:29,  1.79it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 2: Pérdida de Validación Promedio = 2.5918\n",
      "\n",
      "--- Época 3/3 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 3, Batch 803, Loss: 1.2567: 100%|██████████| 2409/2409 [05:17<00:00,  7.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 3: Pérdida de Entrenamiento Promedio = 1.2988\n",
      "\n",
      "Evaluando al final de la época 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "  9%|▉         | 3/34 [00:00<00:01, 22.88it/s]\u001B[A\n",
      " 18%|█▊        | 6/34 [00:00<00:01, 23.91it/s]\u001B[A\n",
      " 26%|██▋       | 9/34 [00:00<00:01, 23.92it/s]\u001B[A\n",
      " 35%|███▌      | 12/34 [00:00<00:00, 23.04it/s]\u001B[A\n",
      " 44%|████▍     | 15/34 [00:00<00:00, 23.51it/s]\u001B[A\n",
      " 53%|█████▎    | 18/34 [00:00<00:00, 24.20it/s]\u001B[A\n",
      " 62%|██████▏   | 21/34 [00:00<00:00, 24.45it/s]\u001B[A\n",
      " 71%|███████   | 24/34 [00:00<00:00, 24.74it/s]\u001B[A\n",
      " 79%|███████▉  | 27/34 [00:01<00:00, 24.79it/s]\u001B[A\n",
      " 88%|████████▊ | 30/34 [00:01<00:00, 24.82it/s]\u001B[A\n",
      "100%|██████████| 34/34 [00:01<00:00, 24.33it/s]\u001B[A\n",
      "Época 3, Batch 803, Loss: 1.2567: 100%|██████████| 2409/2409 [05:19<00:00,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 3: Pérdida de Validación Promedio = 3.0419\n",
      "Entrenamiento completado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Iniciando entrenamiento...\")\n",
    "num_training_steps = NUM_EPOCHS * len(train_dataloader)\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    print(f\"\\n--- Época {epoch + 1}/{NUM_EPOCHS} ---\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_train_loss += loss.item()\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_description(f\"Época {epoch+1}, Batch {batch_idx+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    print(f\"Fin de Época {epoch + 1}: Pérdida de Entrenamiento Promedio = {avg_train_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    total_eval_loss = 0\n",
    "    print(f\"\\nEvaluando al final de la época {epoch + 1}...\")\n",
    "    with torch.no_grad():\n",
    "        for eval_batch in tqdm(eval_dataloader):\n",
    "            eval_batch = {k: v.to(device) for k, v in eval_batch.items()}\n",
    "            outputs = model(**eval_batch)\n",
    "            total_eval_loss += outputs.loss.item()\n",
    "    avg_eval_loss = total_eval_loss / len(eval_dataloader)\n",
    "    print(f\"Fin de Época {epoch + 1}: Pérdida de Validación Promedio = {avg_eval_loss:.4f}\")\n",
    "\n",
    "progress_bar.close()\n",
    "print(\"Entrenamiento completado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6292e301-36fe-4f01-92ec-f36705e2bd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"llama3.2-1B_fine-tuned.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2337d19f-6064-41c1-a847-b4e1beea61a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"llama3.2-1B_fine-tuned.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a0df7c3-ec5e-46a6-9022-b909432863ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_for_inference_ft = (\n",
    "        \"\"\"Context: {}\\n\n",
    "        Question: {}\\n\n",
    "        Options:\\n{}\\n\n",
    "        Answer:\"\"\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5b703eb-3395-4df8-a2a5-d2551dc86c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:01<00:00, 29.95it/s]\n"
     ]
    }
   ],
   "source": [
    "total_correct_fine_tuning, total_examples_fine_tuning = model_evaluation(model, prompt_for_inference_ft, filtered_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ebc7d59-a41e-4ad0-9947-4c4b8f1ded92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Resultados de la Evaluación en Test (Probabilidad Basada) ---\n",
      "Ejemplos Totales: 56\n",
      "Predicciones Correctas: 24\n",
      "Exactitud (Accuracy): 42.86%\n"
     ]
    }
   ],
   "source": [
    "accuracy = total_correct_fine_tuning / total_examples_fine_tuning\n",
    "\n",
    "print(f\"\\n--- Resultados de la Evaluación en Test (Probabilidad Basada) ---\")\n",
    "print(f\"Ejemplos Totales: {total_examples_fine_tuning}\")\n",
    "print(f\"Predicciones Correctas: {total_correct_fine_tuning}\")\n",
    "print(f\"Exactitud (Accuracy): {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2912768d-41d8-4594-86e4-39a167b634b8",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "id": "e171c2c0-8f61-4a25-9379-80f39fd3773f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T00:00:39.714052Z",
     "start_time": "2025-05-13T00:00:36.619501Z"
    }
   },
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # LORA & PEFT: Efficient Fine-Tuning Comparison\n",
    "# ... (rest of your initial comments) ...\n",
    "\n",
    "# ### Import libraries\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    get_scheduler,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset, DatasetDict\n",
    "import os\n",
    "import math\n",
    "\n",
    "# NEW: Import PEFT components\n",
    "from peft import LoraConfig, get_peft_model, TaskType # Added\n",
    "\n",
    "login(token=\"\") #\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Llama-3.2-1B\" #\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") #\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True) #\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token #\n",
    "\n",
    "# ### Download dataset race\n",
    "ds_full = load_dataset(\"ehovy/race\", \"high\", trust_remote_code=True) #\n",
    "ds_test = ds_full.get('test') #\n",
    "filtered_test_data = ds_test.filter( #\n",
    "    lambda example: len(example['article']) < MAX_ARTICLE_CHAR_LENGTH #\n",
    ")\n",
    "\n",
    "\n",
    "processed_ds = DatasetDict()\n",
    "\n",
    "for split, data in ds_full.items():\n",
    "    if split != 'test':\n",
    "        filtered_data = data.filter(\n",
    "            lambda example: len(example['article']) < MAX_ARTICLE_CHAR_LENGTH,\n",
    "            desc=f\"Filtrando artículos en {split}\"\n",
    "        )\n",
    "\n",
    "        mapped_data = filtered_data.map(\n",
    "            transform_and_tokenize_example,\n",
    "            batched=False,\n",
    "            remove_columns=filtered_data.column_names,\n",
    "            desc=f\"Tokenizando {split}\"\n",
    "        )\n",
    "        processed_ds[split] = mapped_data\n",
    "        processed_ds[split].set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) #\n",
    "train_dataloader = DataLoader( #\n",
    "    processed_ds.get(\"train\"), #\n",
    "    shuffle=True, #\n",
    "    batch_size=BATCH_SIZE, #\n",
    "    collate_fn=data_collator #\n",
    ")\n",
    "eval_dataloader = DataLoader( #\n",
    "    processed_ds.get(\"validation\"), #\n",
    "    batch_size=BATCH_SIZE, #\n",
    "    collate_fn=data_collator #\n",
    ")\n",
    "\n",
    "# ### Load Base Model for PEFT Fine-tuning\n",
    "print(\"--- Loading Model for PEFT Fine-tuning ---\")\n",
    "model_for_peft = AutoModelForCausalLM.from_pretrained( #\n",
    "    MODEL_NAME, #\n",
    "    torch_dtype=torch.float16, #\n",
    "    trust_remote_code=True #\n",
    ")\n",
    "model_for_peft.config.pad_token_id = tokenizer.pad_token_id #\n",
    "\n",
    "# ### Configure PEFT (using LoRA config as an example)\n",
    "# Note: You were using r=16 in your manual implementation\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, # Specify the task type\n",
    "    inference_mode=False,         # Set to False for training\n",
    "    r=16,                         # Rank of the update matrices (matches your script)\n",
    "    lora_alpha=32,                # Alpha scaling factor (common practice to set 2*r)\n",
    "    lora_dropout=0.1,             # Dropout probability for LoRA layers\n",
    "    # Target modules can often be inferred, but explicitly listing is safer:\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"] # Matches your script\n",
    ")\n",
    "\n",
    "# ### Apply PEFT to the Model\n",
    "model_peft = get_peft_model(model_for_peft, peft_config)\n",
    "model_peft.print_trainable_parameters() # See how few parameters are trainable!\n",
    "\n",
    "model_peft.to(device) # Now move the PEFT model to the device\n",
    "print(f\"PEFT Model moved to {device}\")\n",
    "\n",
    "# ### Training/Fine-tuning loop (using the PEFT model)\n",
    "optimizer = AdamW(model_peft.parameters(), lr=LEARNING_RATE) # Optimizer uses PEFT model params\n",
    "\n",
    "print(\"Iniciando entrenamiento con PEFT...\") #"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizando validation: 100%|██████████| 34/34 [00:00<00:00, 878.50 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Model for PEFT Fine-tuning ---\n",
      "trainable params: 3,407,872 || all params: 1,239,222,272 || trainable%: 0.2750\n",
      "PEFT Model moved to cuda\n",
      "Iniciando entrenamiento con PEFT...\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T00:14:09.639788Z",
     "start_time": "2025-05-13T00:00:41.375201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_training_steps = NUM_EPOCHS * len(train_dataloader) #\n",
    "progress_bar = tqdm(range(num_training_steps)) #\n",
    "\n",
    "for epoch in range(NUM_EPOCHS): #\n",
    "    model_peft.train() # Set PEFT model to train mode\n",
    "    total_train_loss = 0 #\n",
    "    print(f\"\\n--- Época {epoch + 1}/{NUM_EPOCHS} ---\") #\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_dataloader): #\n",
    "        batch = {k: v.to(device) for k, v in batch.items()} #\n",
    "        outputs = model_peft(**batch) # Use PEFT model\n",
    "        loss = outputs.loss #\n",
    "        loss.backward() #\n",
    "        optimizer.step() #\n",
    "        optimizer.zero_grad() #\n",
    "        total_train_loss += loss.item() #\n",
    "        progress_bar.update(1) #\n",
    "        progress_bar.set_description(f\"Época {epoch+1}, Batch {batch_idx+1}, Loss: {loss.item():.4f}\") #\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader) #\n",
    "    print(f\"Fin de Época {epoch + 1}: Pérdida de Entrenamiento Promedio = {avg_train_loss:.4f}\") #\n",
    "\n",
    "    # --- Evaluation Loop ---\n",
    "    model_peft.eval() # Set PEFT model to eval mode\n",
    "    total_eval_loss = 0 #\n",
    "    print(f\"\\nEvaluando al final de la época {epoch + 1}...\") #\n",
    "    with torch.no_grad(): #\n",
    "        for eval_batch in tqdm(eval_dataloader): #\n",
    "            eval_batch = {k: v.to(device) for k, v in eval_batch.items()} #\n",
    "            outputs = model_peft(**eval_batch) # Use PEFT model\n",
    "            total_eval_loss += outputs.loss.item() #\n",
    "    avg_eval_loss = total_eval_loss / len(eval_dataloader) #\n",
    "    print(f\"Fin de Época {epoch + 1}: Pérdida de Validación Promedio = {avg_eval_loss:.4f}\") #\n",
    "\n",
    "progress_bar.close() #\n",
    "print(\"Entrenamiento con PEFT completado.\") #\n",
    "\n",
    "# ### Save the PEFT adapter (not the whole model)\n",
    "peft_adapter_path = \"llama3.2-1B-peft-adapter\"\n",
    "model_peft.save_pretrained(peft_adapter_path)\n",
    "print(f\"PEFT adapter saved to {peft_adapter_path}\")\n",
    "\n",
    "# Optional: Save the tokenizer too\n",
    "# tokenizer.save_pretrained(peft_adapter_path)\n",
    "\n",
    "# ### Evaluate the PEFT Fine-tuned Model\n",
    "print(\"--- Evaluating PEFT Fine-tuned Model ---\")\n",
    "# To evaluate, load the base model and apply the adapter\n",
    "from peft import PeftModel # Added\n",
    "\n",
    "eval_base_model = AutoModelForCausalLM.from_pretrained( #\n",
    "    MODEL_NAME, #\n",
    "    torch_dtype=torch.float16, #\n",
    "    trust_remote_code=True #\n",
    ")"
   ],
   "id": "fca0f51e10f5083a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8030 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Época 1/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 1, Batch 803, Loss: 2.5177:  10%|█         | 803/8030 [01:28<11:41, 10.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 1: Pérdida de Entrenamiento Promedio = 2.2025\n",
      "\n",
      "Evaluando al final de la época 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "  9%|▉         | 3/34 [00:00<00:01, 23.51it/s]\u001B[A\n",
      " 18%|█▊        | 6/34 [00:00<00:01, 22.67it/s]\u001B[A\n",
      " 26%|██▋       | 9/34 [00:00<00:01, 22.75it/s]\u001B[A\n",
      " 35%|███▌      | 12/34 [00:00<00:00, 22.95it/s]\u001B[A\n",
      " 44%|████▍     | 15/34 [00:00<00:00, 23.02it/s]\u001B[A\n",
      " 53%|█████▎    | 18/34 [00:00<00:00, 23.11it/s]\u001B[A\n",
      " 62%|██████▏   | 21/34 [00:00<00:00, 23.16it/s]\u001B[A\n",
      " 71%|███████   | 24/34 [00:01<00:00, 23.19it/s]\u001B[A\n",
      " 79%|███████▉  | 27/34 [00:01<00:00, 23.25it/s]\u001B[A\n",
      " 88%|████████▊ | 30/34 [00:01<00:00, 23.28it/s]\u001B[A\n",
      "100%|██████████| 34/34 [00:01<00:00, 23.11it/s]\u001B[A\n",
      "Época 2, Batch 2, Loss: 2.2665:  10%|█         | 805/8030 [01:30<38:25,  3.13it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 1: Pérdida de Validación Promedio = 2.3045\n",
      "\n",
      "--- Época 2/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 2, Batch 803, Loss: 1.2657:  20%|██        | 1606/8030 [02:48<10:22, 10.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 2: Pérdida de Entrenamiento Promedio = 2.0036\n",
      "\n",
      "Evaluando al final de la época 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "  9%|▉         | 3/34 [00:00<00:01, 23.49it/s]\u001B[A\n",
      " 18%|█▊        | 6/34 [00:00<00:01, 23.27it/s]\u001B[A\n",
      " 26%|██▋       | 9/34 [00:00<00:01, 23.30it/s]\u001B[A\n",
      " 35%|███▌      | 12/34 [00:00<00:00, 23.30it/s]\u001B[A\n",
      " 44%|████▍     | 15/34 [00:00<00:00, 23.29it/s]\u001B[A\n",
      " 53%|█████▎    | 18/34 [00:00<00:00, 23.29it/s]\u001B[A\n",
      " 62%|██████▏   | 21/34 [00:00<00:00, 23.32it/s]\u001B[A\n",
      " 71%|███████   | 24/34 [00:01<00:00, 23.31it/s]\u001B[A\n",
      " 79%|███████▉  | 27/34 [00:01<00:00, 23.30it/s]\u001B[A\n",
      " 88%|████████▊ | 30/34 [00:01<00:00, 23.29it/s]\u001B[A\n",
      "100%|██████████| 34/34 [00:01<00:00, 23.27it/s]\u001B[A\n",
      "Época 3, Batch 2, Loss: 1.7409:  20%|██        | 1608/8030 [02:50<33:58,  3.15it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 2: Pérdida de Validación Promedio = 2.3676\n",
      "\n",
      "--- Época 3/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 3, Batch 803, Loss: 1.6659:  30%|███       | 2409/8030 [04:08<09:05, 10.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 3: Pérdida de Entrenamiento Promedio = 1.7987\n",
      "\n",
      "Evaluando al final de la época 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "  9%|▉         | 3/34 [00:00<00:01, 23.54it/s]\u001B[A\n",
      " 18%|█▊        | 6/34 [00:00<00:01, 23.32it/s]\u001B[A\n",
      " 26%|██▋       | 9/34 [00:00<00:01, 23.27it/s]\u001B[A\n",
      " 35%|███▌      | 12/34 [00:00<00:00, 23.27it/s]\u001B[A\n",
      " 44%|████▍     | 15/34 [00:00<00:00, 23.24it/s]\u001B[A\n",
      " 53%|█████▎    | 18/34 [00:00<00:00, 23.24it/s]\u001B[A\n",
      " 62%|██████▏   | 21/34 [00:00<00:00, 23.25it/s]\u001B[A\n",
      " 71%|███████   | 24/34 [00:01<00:00, 23.27it/s]\u001B[A\n",
      " 79%|███████▉  | 27/34 [00:01<00:00, 23.26it/s]\u001B[A\n",
      " 88%|████████▊ | 30/34 [00:01<00:00, 23.28it/s]\u001B[A\n",
      "100%|██████████| 34/34 [00:01<00:00, 23.23it/s]\u001B[A\n",
      "Época 4, Batch 2, Loss: 1.8525:  30%|███       | 2411/8030 [04:09<29:47,  3.14it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 3: Pérdida de Validación Promedio = 2.4959\n",
      "\n",
      "--- Época 4/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 4, Batch 803, Loss: 1.5371:  40%|████      | 3212/8030 [05:27<07:47, 10.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 4: Pérdida de Entrenamiento Promedio = 1.5576\n",
      "\n",
      "Evaluando al final de la época 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "  9%|▉         | 3/34 [00:00<00:01, 23.41it/s]\u001B[A\n",
      " 18%|█▊        | 6/34 [00:00<00:01, 23.31it/s]\u001B[A\n",
      " 26%|██▋       | 9/34 [00:00<00:01, 23.26it/s]\u001B[A\n",
      " 35%|███▌      | 12/34 [00:00<00:00, 23.26it/s]\u001B[A\n",
      " 44%|████▍     | 15/34 [00:00<00:00, 23.25it/s]\u001B[A\n",
      " 53%|█████▎    | 18/34 [00:00<00:00, 23.15it/s]\u001B[A\n",
      " 62%|██████▏   | 21/34 [00:00<00:00, 23.18it/s]\u001B[A\n",
      " 71%|███████   | 24/34 [00:01<00:00, 23.16it/s]\u001B[A\n",
      " 79%|███████▉  | 27/34 [00:01<00:00, 23.13it/s]\u001B[A\n",
      " 88%|████████▊ | 30/34 [00:01<00:00, 23.17it/s]\u001B[A\n",
      "100%|██████████| 34/34 [00:01<00:00, 23.16it/s]\u001B[A\n",
      "Época 5, Batch 2, Loss: 1.8054:  40%|████      | 3214/8030 [05:29<25:34,  3.14it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 4: Pérdida de Validación Promedio = 2.7503\n",
      "\n",
      "--- Época 5/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 5, Batch 803, Loss: 1.7545:  50%|█████     | 4015/8030 [06:47<06:30, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 5: Pérdida de Entrenamiento Promedio = 1.3196\n",
      "\n",
      "Evaluando al final de la época 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "  9%|▉         | 3/34 [00:00<00:01, 23.47it/s]\u001B[A\n",
      " 18%|█▊        | 6/34 [00:00<00:01, 23.27it/s]\u001B[A\n",
      " 26%|██▋       | 9/34 [00:00<00:01, 23.18it/s]\u001B[A\n",
      " 35%|███▌      | 12/34 [00:00<00:00, 23.23it/s]\u001B[A\n",
      " 44%|████▍     | 15/34 [00:00<00:00, 23.14it/s]\u001B[A\n",
      " 53%|█████▎    | 18/34 [00:00<00:00, 23.17it/s]\u001B[A\n",
      " 62%|██████▏   | 21/34 [00:00<00:00, 23.20it/s]\u001B[A\n",
      " 71%|███████   | 24/34 [00:01<00:00, 23.25it/s]\u001B[A\n",
      " 79%|███████▉  | 27/34 [00:01<00:00, 23.05it/s]\u001B[A\n",
      " 88%|████████▊ | 30/34 [00:01<00:00, 23.13it/s]\u001B[A\n",
      "100%|██████████| 34/34 [00:01<00:00, 23.14it/s]\u001B[A\n",
      "Época 6, Batch 2, Loss: 0.5313:  50%|█████     | 4017/8030 [06:49<21:20,  3.13it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 5: Pérdida de Validación Promedio = 2.9189\n",
      "\n",
      "--- Época 6/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 6, Batch 803, Loss: 0.5279:  60%|██████    | 4818/8030 [08:07<05:13, 10.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 6: Pérdida de Entrenamiento Promedio = 1.1073\n",
      "\n",
      "Evaluando al final de la época 6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "  9%|▉         | 3/34 [00:00<00:01, 23.54it/s]\u001B[A\n",
      " 18%|█▊        | 6/34 [00:00<00:01, 23.20it/s]\u001B[A\n",
      " 26%|██▋       | 9/34 [00:00<00:01, 23.26it/s]\u001B[A\n",
      " 35%|███▌      | 12/34 [00:00<00:00, 23.29it/s]\u001B[A\n",
      " 44%|████▍     | 15/34 [00:00<00:00, 23.28it/s]\u001B[A\n",
      " 53%|█████▎    | 18/34 [00:00<00:00, 23.22it/s]\u001B[A\n",
      " 62%|██████▏   | 21/34 [00:00<00:00, 23.24it/s]\u001B[A\n",
      " 71%|███████   | 24/34 [00:01<00:00, 23.26it/s]\u001B[A\n",
      " 79%|███████▉  | 27/34 [00:01<00:00, 23.28it/s]\u001B[A\n",
      " 88%|████████▊ | 30/34 [00:01<00:00, 23.24it/s]\u001B[A\n",
      "100%|██████████| 34/34 [00:01<00:00, 23.22it/s]\u001B[A\n",
      "Época 7, Batch 2, Loss: 1.0031:  60%|██████    | 4820/8030 [08:08<17:02,  3.14it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 6: Pérdida de Validación Promedio = 3.1770\n",
      "\n",
      "--- Época 7/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 7, Batch 803, Loss: 0.9531:  70%|███████   | 5621/8030 [09:26<03:54, 10.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 7: Pérdida de Entrenamiento Promedio = 0.9346\n",
      "\n",
      "Evaluando al final de la época 7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "  9%|▉         | 3/34 [00:00<00:01, 23.40it/s]\u001B[A\n",
      " 18%|█▊        | 6/34 [00:00<00:01, 23.21it/s]\u001B[A\n",
      " 26%|██▋       | 9/34 [00:00<00:01, 23.17it/s]\u001B[A\n",
      " 35%|███▌      | 12/34 [00:00<00:00, 23.18it/s]\u001B[A\n",
      " 44%|████▍     | 15/34 [00:00<00:00, 23.20it/s]\u001B[A\n",
      " 53%|█████▎    | 18/34 [00:00<00:00, 23.20it/s]\u001B[A\n",
      " 62%|██████▏   | 21/34 [00:00<00:00, 23.24it/s]\u001B[A\n",
      " 71%|███████   | 24/34 [00:01<00:00, 23.26it/s]\u001B[A\n",
      " 79%|███████▉  | 27/34 [00:01<00:00, 23.26it/s]\u001B[A\n",
      " 88%|████████▊ | 30/34 [00:01<00:00, 23.25it/s]\u001B[A\n",
      "100%|██████████| 34/34 [00:01<00:00, 23.20it/s]\u001B[A\n",
      "Época 8, Batch 2, Loss: 0.5559:  70%|███████   | 5623/8030 [09:28<12:46,  3.14it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 7: Pérdida de Validación Promedio = 3.5715\n",
      "\n",
      "--- Época 8/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 8, Batch 803, Loss: 0.8919:  80%|████████  | 6424/8030 [10:46<02:36, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 8: Pérdida de Entrenamiento Promedio = 0.8033\n",
      "\n",
      "Evaluando al final de la época 8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "  9%|▉         | 3/34 [00:00<00:01, 23.44it/s]\u001B[A\n",
      " 18%|█▊        | 6/34 [00:00<00:01, 23.22it/s]\u001B[A\n",
      " 26%|██▋       | 9/34 [00:00<00:01, 23.20it/s]\u001B[A\n",
      " 35%|███▌      | 12/34 [00:00<00:00, 23.24it/s]\u001B[A\n",
      " 44%|████▍     | 15/34 [00:00<00:00, 23.14it/s]\u001B[A\n",
      " 53%|█████▎    | 18/34 [00:00<00:00, 23.17it/s]\u001B[A\n",
      " 62%|██████▏   | 21/34 [00:00<00:00, 23.17it/s]\u001B[A\n",
      " 71%|███████   | 24/34 [00:01<00:00, 23.11it/s]\u001B[A\n",
      " 79%|███████▉  | 27/34 [00:01<00:00, 23.16it/s]\u001B[A\n",
      " 88%|████████▊ | 30/34 [00:01<00:00, 23.14it/s]\u001B[A\n",
      "100%|██████████| 34/34 [00:01<00:00, 23.10it/s]\u001B[A\n",
      "Época 9, Batch 2, Loss: 0.8007:  80%|████████  | 6426/8030 [10:48<08:32,  3.13it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 8: Pérdida de Validación Promedio = 3.6343\n",
      "\n",
      "--- Época 9/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 9, Batch 803, Loss: 0.4757:  90%|█████████ | 7227/8030 [12:06<01:18, 10.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 9: Pérdida de Entrenamiento Promedio = 0.7039\n",
      "\n",
      "Evaluando al final de la época 9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "  9%|▉         | 3/34 [00:00<00:01, 23.33it/s]\u001B[A\n",
      " 18%|█▊        | 6/34 [00:00<00:01, 23.17it/s]\u001B[A\n",
      " 26%|██▋       | 9/34 [00:00<00:01, 23.18it/s]\u001B[A\n",
      " 35%|███▌      | 12/34 [00:00<00:00, 23.20it/s]\u001B[A\n",
      " 44%|████▍     | 15/34 [00:00<00:00, 23.16it/s]\u001B[A\n",
      " 53%|█████▎    | 18/34 [00:00<00:00, 23.18it/s]\u001B[A\n",
      " 62%|██████▏   | 21/34 [00:00<00:00, 23.10it/s]\u001B[A\n",
      " 71%|███████   | 24/34 [00:01<00:00, 23.15it/s]\u001B[A\n",
      " 79%|███████▉  | 27/34 [00:01<00:00, 23.16it/s]\u001B[A\n",
      " 88%|████████▊ | 30/34 [00:01<00:00, 23.16it/s]\u001B[A\n",
      "100%|██████████| 34/34 [00:01<00:00, 23.14it/s]\u001B[A\n",
      "Época 10, Batch 2, Loss: 0.3015:  90%|█████████ | 7229/8030 [12:07<04:15,  3.13it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 9: Pérdida de Validación Promedio = 3.8243\n",
      "\n",
      "--- Época 10/10 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 10, Batch 803, Loss: 0.6064: 100%|██████████| 8030/8030 [13:25<00:00, 10.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 10: Pérdida de Entrenamiento Promedio = 0.6285\n",
      "\n",
      "Evaluando al final de la época 10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\u001B[A\n",
      "  9%|▉         | 3/34 [00:00<00:01, 23.52it/s]\u001B[A\n",
      " 18%|█▊        | 6/34 [00:00<00:01, 23.21it/s]\u001B[A\n",
      " 26%|██▋       | 9/34 [00:00<00:01, 23.15it/s]\u001B[A\n",
      " 35%|███▌      | 12/34 [00:00<00:00, 23.17it/s]\u001B[A\n",
      " 44%|████▍     | 15/34 [00:00<00:00, 23.19it/s]\u001B[A\n",
      " 53%|█████▎    | 18/34 [00:00<00:00, 23.19it/s]\u001B[A\n",
      " 62%|██████▏   | 21/34 [00:00<00:00, 23.22it/s]\u001B[A\n",
      " 71%|███████   | 24/34 [00:01<00:00, 23.22it/s]\u001B[A\n",
      " 79%|███████▉  | 27/34 [00:01<00:00, 23.21it/s]\u001B[A\n",
      " 88%|████████▊ | 30/34 [00:01<00:00, 23.22it/s]\u001B[A\n",
      "100%|██████████| 34/34 [00:01<00:00, 23.17it/s]\u001B[A\n",
      "Época 10, Batch 803, Loss: 0.6064: 100%|██████████| 8030/8030 [13:27<00:00,  9.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin de Época 10: Pérdida de Validación Promedio = 4.0882\n",
      "Entrenamiento con PEFT completado.\n",
      "PEFT adapter saved to llama3.2-1B-peft-adapter\n",
      "--- Evaluating PEFT Fine-tuned Model ---\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T00:15:01.973508Z",
     "start_time": "2025-05-13T00:15:00.064360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "eval_peft_model = PeftModel.from_pretrained(eval_base_model, peft_adapter_path)\n",
    "eval_peft_model.to(device) #\n",
    "eval_peft_model.eval() #\n",
    "\n",
    "\n",
    "total_correct_peft, total_examples_peft = model_evaluation(eval_peft_model, prompt_for_inference, filtered_test_data) #\n",
    "accuracy_peft = total_correct_peft / total_examples_peft #\n",
    "\n",
    "print(f\"\\n--- PEFT Fine-tuned Model Test Results ---\") #\n",
    "print(f\"Total Examples: {total_examples_peft}\") #\n",
    "print(f\"Correct Predictions: {total_correct_peft}\") #\n",
    "print(f\"Accuracy: {accuracy_peft * 100:.2f}%\") #\n",
    "\n",
    "\n",
    "# ## Optional: Evaluate your Original Manual LoRA Model\n",
    "# If you want to compare directly, you would run your original LoRA fine-tuning\n",
    "# code separately and then evaluate it using the same model_evaluation function.\n",
    "# Make sure to save/load your manually fine-tuned model correctly.\n",
    "\n",
    "# print(\"--- Evaluating Original Manual LoRA Fine-tuned Model ---\")\n",
    "# original_lora_model = torch.load(\"llama3.2-1B_fine-tuned.pt\") # Load your saved model\n",
    "# original_lora_model.to(device) #\n",
    "# total_correct_orig_ft, total_examples_orig_ft = model_evaluation(original_lora_model, prompt_for_inference_ft, filtered_test_data) #\n",
    "# accuracy_orig_ft = total_correct_orig_ft / total_examples_orig_ft #\n",
    "# print(f\"\\n--- Original Manual LoRA Fine-tuned Model Test Results ---\") #\n",
    "# print(f\"Accuracy: {accuracy_orig_ft * 100:.2f}%\") #\n",
    "\n",
    "# ## Results Comparison\n",
    "print(\"\\n--- Final Accuracy Comparison ---\")\n",
    "# print(f\"Base Model Accuracy: {accuracy_base * 100:.2f}%\") # Uncomment if you evaluated base model\n",
    "print(f\"PEFT Library (LoRA) Fine-tuned Accuracy: {accuracy_peft * 100:.2f}%\") #\n",
    "# print(f\"Original Manual LoRA Fine-tuned Accuracy: {accuracy_orig_ft * 100:.2f}%\") # Uncomment if you evaluated original"
   ],
   "id": "b27cfdae8fb9775a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/main/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "100%|██████████| 56/56 [00:01<00:00, 31.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PEFT Fine-tuned Model Test Results ---\n",
      "Total Examples: 56\n",
      "Correct Predictions: 17\n",
      "Accuracy: 30.36%\n",
      "\n",
      "--- Final Accuracy Comparison ---\n",
      "PEFT Library (LoRA) Fine-tuned Accuracy: 30.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c7359a5a9275add2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.9",
   "language": "python",
   "name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
