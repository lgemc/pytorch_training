{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-12T02:29:33.604563Z",
     "start_time": "2025-05-12T02:29:30.431545Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling\n",
    "from tqdm import tqdm\n",
    "import math"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/main/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# LORA: Low Rank Adaptation: an efficient way to fine tune large language models\n",
    "\n",
    "When we have a specific task to perform with large language models we have various options:\n",
    "\n",
    "1. Use the model as it is using prompt engineering\n",
    "2. Fine tune the whole model, updating all its weights\n",
    "3. Fine tune only some layers instead the whole model.\n",
    "\n",
    "Pros and cons of each one:\n",
    "\n",
    "| Approach             | Pros                                                                 | Cons                                                                 |\n",
    "|----------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|\n",
    "| Prompt Engineering   | - Fast and cheap                                                     | - Limited control over behavior                                      |\n",
    "|                      | - No training or infrastructure needed                               | - Performance highly sensitive to prompt wording                     |\n",
    "|                      | - Easily updated or changed                                          | - May hit model limits on specific tasks                             |\n",
    "| Fine-Tune Full Model | - Full control over model behavior                                   | - Very resource-intensive (GPU, time, data)                          |\n",
    "|                      | - Better performance on domain-specific or complex tasks             | - Risk of overfitting or catastrophic forgetting                    |\n",
    "|                      | - Can learn new capabilities                                         | - Requires re-deployment of large models                             |\n",
    "| LoRA Fine-Tuning     | - Much less compute and memory than full fine-tuning                 | - Slightly less flexible than full fine-tuning                      |\n",
    "|                      | - Retains base model unchanged (can swap adapters)                   | - Still needs training pipeline setup                                |\n",
    "|                      | - Modular and efficient for multiple tasks/domains                   | - May not reach full model’s potential on highly specialized tasks   |\n",
    "\n",
    "And important remarks:\n",
    "\n",
    "- If you want to use a high llm from a provider, like GPT from OpenAI or Gemini from google, you simply can't fine tune this model, so prompt engineering is your only available option\n",
    "- If you have low models, like 1B or 8B, them does not perform very well in very specific tasks, but you can perform fine tune over them with limited resources, greatly improving performance\n",
    "\n",
    "So, right now we are going to implement and compare two ways to resolve a specific task: **prompt engineering** leaving the original model as it is, and the second way to test: **lora fine-tuning**."
   ],
   "id": "9341a6b9781e6565"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# The task: Question and answers with RACE\n",
    "\n",
    "We want to train a system with the ability of solve questions and answers where the answer should be picked from a list of options:\n",
    "\n",
    "```text\n",
    "Context: A subject which seems to have been insufficiently studied by doctors and psychologists is the influence of geography and climate on the psychological and physical health of mankind. There seems no doubt that the general character of the landscape, the relative length of day and night, and the climate must all play a big part in determining what kind of people we are.\n",
    "It is true that a few studies have been made. Where all the inhabitants of a particular area enjoy exceptionally good or bad health, scientists have identified contributory factors such as the presence or absence of substances like iodine, fluoride, calcium, or iron in the water supply, or perhaps types of land that provide breeding places for pests like mosquitoes or rats.\n",
    "Moreover, we can all generalize about types of people we have met. Those living in countries with long dark winters are apt to be less talkative and less vivacious than inhabitants of countries where the climate is more equable. And where the olive and the orange grow, there the inhabitants are cheerful, talkative, and spontaneous.\n",
    "But these commonplace generalizations are inadequate: the influence of climate and geography should be studied in depth. Do all mountain dwellers live to a ripe old age? Does the drinking of wine, rather than beer, result in a sunny and open temperament? Is the strength and height of one of the Kenyan tribes due to their habitual drinking of the blood of cows?\n",
    "We are not yet sure of the answers to such questions, but let us hope that something of benefit to mankind may eventually result from such studies.\n",
    "\n",
    "Question: According to the author, research into the influence of geography and climate should  _  .\n",
    "\n",
    "Options:\n",
    "A) focus on some unknown aspects\n",
    "B) be pursued on a larger scale\n",
    "C) be carried out within a larger scope\n",
    "D) go much deeper\n",
    "\n",
    "Answer: D\n",
    "```\n",
    "\n",
    "We are using the `transformers` dataset called `ehvoy/race`, composed by 97k of questions and answers, but\n",
    "to reduce training times we are going to use only subsets with `context.length < 800`, reducing the original\n",
    "dataset to a length of `800` in the train set and `56` items in the test set."
   ],
   "id": "7031b6d3fd0edd26"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T02:32:05.408658Z",
     "start_time": "2025-05-12T02:32:05.398019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EhovyRaceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Ehvoy race is a questions and answer dataset\n",
    "    Variations can get the values all, high, medium and low and depending on this the dataset size may vary\n",
    "    \"\"\"\n",
    "    def __init__(self, variation=\"high\", split=\"train\", max_article_size=None):\n",
    "        self.raw_dataset = load_dataset(\"ehovy/race\", variation, split=split)\n",
    "        if max_article_size is not None:\n",
    "            self.raw_dataset  = self.raw_dataset.filter(\n",
    "                lambda example: len(example['article']) < max_article_size,\n",
    "                desc=f\"Filtrando artículos en {split}\"\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.raw_dataset[idx]\n",
    "\n",
    "def prompt_with_question(example: dict, include_answer=False) -> str:\n",
    "    options_str = \"\\n\".join([f\"{chr(65 + i)}) {opt}\" for i, opt in enumerate(example['options'])])\n",
    "    answer = f\" {example['answer']}\" if include_answer else \"\"\n",
    "    prompt = f\"Context: {example['article']}\\n\\n\" + \\\n",
    "        f\"Question: {example['question']}\\n\\n\" + \\\n",
    "        f\"Options:\\n{options_str}\\n\\n\" + \\\n",
    "        f\"Answer:{answer}\" # The model is expected to fill this part\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "class PromptedEhvoy(Dataset):\n",
    "    \"\"\"\n",
    "    Prompted ehvoy is a dataset that convert the original ehvoy structure composed by a dict of\n",
    "    context, question, options and answer to a single string containing all those information into a single\n",
    "    string\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset: EhovyRaceDataset, build_prompt=prompt_with_question, include_answer=False):\n",
    "        self.build_prompt = build_prompt\n",
    "        self.include_answer = include_answer\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.build_prompt(self.dataset[idx], include_answer=self.include_answer), self.dataset[idx]['answer']\n",
    "\n",
    "def get_dataset(split, max_article_size=800, include_answers=False):\n",
    "    \"\"\"\n",
    "    Build the dataset from original ehvoy and convert it to prompted dataset\n",
    "    \"\"\"\n",
    "    ehovy_dataset = EhovyRaceDataset(variation=\"high\", split=split, max_article_size=max_article_size)\n",
    "    prompted_dataset = PromptedEhvoy(ehovy_dataset, include_answer=include_answers)\n",
    "    return prompted_dataset\n",
    "\n",
    "class TokenizedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Tokenized dataset for text generation tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, data: PromptedEhvoy, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, _ = self.data[idx]\n",
    "\n",
    "        x_tokenized = self.tokenizer(\n",
    "            x,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "\n",
    "        labels = x_tokenized[\"input_ids\"].copy()\n",
    "\n",
    "        answer_start_char_idx = x.find(\"Answer:\")\n",
    "        if answer_start_char_idx != -1:\n",
    "            answer_token_char_start_idx = answer_start_char_idx + len(\"Answer: \")\n",
    "\n",
    "            answer_token_start_index = -1\n",
    "            for i, (start_offset, end_offset) in enumerate(x_tokenized['offset_mapping']):\n",
    "\n",
    "                if start_offset <= answer_token_char_start_idx < end_offset:\n",
    "                    answer_token_start_index = i\n",
    "                    break\n",
    "\n",
    "            if answer_token_start_index != -1:\n",
    "                for i in range(answer_token_start_index):\n",
    "                    labels[i] = -100\n",
    "\n",
    "        x_tokenized[\"labels\"] = labels\n",
    "        # del x_tokenized[\"offset_mapping\"]\n",
    "        return x_tokenized"
   ],
   "id": "9989a677330698c0",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T02:29:38.424501Z",
     "start_time": "2025-05-12T02:29:37.158829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = get_dataset(\"train\", include_answers=False)\n",
    "test_dataset = get_dataset(\"test\", include_answers=False)"
   ],
   "id": "40c2536126929ccf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T02:29:41.603699Z",
     "start_time": "2025-05-12T02:29:41.599978Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Train dataset size: {len(train_dataset)}, Test dataset size: {len(test_dataset)}\")",
   "id": "f503401162b9c549",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 803, Test dataset size: 56\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T02:29:44.050512Z",
     "start_time": "2025-05-12T02:29:44.045972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = train_dataset[0][0]\n",
    "print(x)"
   ],
   "id": "cb4e8c1688fffdb9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: The air hostess   was in a small kitchen at the back of the plane, preparing the plates for lunch, when a little old lady came and spoke to her, \"Could you please tell me,\" she asked, \"where is the ladies' lavatory   in the plane?\"\n",
      "\"Yes, madam,\" said the air hostess and smiled. \"It is right at the other end of the plane---at the front.\"\n",
      "The little lady went too far. She walked all the way to the front of the plane, opened the door in front of her, and saw the captain of the plane and the other officers. They were all busy with their work and did not see her. She went out again, shut the door and returned to the air hostess.\n",
      "\"Oh, didn't you find it, madam?\" the girl asked her. \"Yes, I did,\" said the little lady. \"But there are four men in the ladies' lavatory watching television.\"\n",
      "\n",
      "Question: The story happened  _  .\n",
      "\n",
      "Options:\n",
      "A) in the evening\n",
      "B) in the afternoon\n",
      "C) in the morning\n",
      "D) at midnight\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# The model: Llama 3.2 1B\n",
    "\n",
    "Nowadays, we have a lot of small models with open weights offered by big tech that can be used for free and downloaded\n",
    "from various repositories like hugging face.\n",
    "\n",
    "On this list we can find:\n",
    "- Gemma: A model trained by Google offered in various sizes, included 3B\n",
    "- Phi: A model trained by Microsoft\n",
    "- Llama: A model trained by Meta\n",
    "\n",
    "Special mentions: SmolLM2, a model built by hugging face community, OpenELM, a model built by apple\n",
    "\n",
    "*All this models are based on decoder only architectures, which makes them easier to train*\n",
    "\n",
    "**Our chosen model is Llama 3.2 1B**"
   ],
   "id": "c7a47fbb1bdcdba9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T02:29:53.970113Z",
     "start_time": "2025-05-12T02:29:53.709847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MODEL_NAME=\"meta-llama/Llama-3.2-1B\"\n",
    "MAX_LENGTH=512\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "id": "3e4327311dac11b4",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# In order to reduce the v-ram usage we are going to use float 16\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16).to(device)"
   ],
   "id": "b43c243f1c65693a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T02:29:58.417414Z",
     "start_time": "2025-05-12T02:29:57.933853Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)",
   "id": "ce4c75043ea5d619",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T02:30:09.206375Z",
     "start_time": "2025-05-12T02:30:09.203526Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.pad_token = tokenizer.eos_token",
   "id": "12608dc9e9170d74",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# First experiment: question answering with the model as it is",
   "id": "fd5eb3321834401b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T02:30:20.022510Z",
     "start_time": "2025-05-12T02:30:20.015917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class OptionsPicker(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    OptionsPicker is a class that provides a way to select options based on the provided input.\n",
    "    It uses the Llama32 model and tokenizer to generate predictions and select the best option.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer, options=None, device=\"cuda\"):\n",
    "        \"\"\"\n",
    "        Initialize the OptionsPicker with a model, tokenizer, and options.\n",
    "        Options\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.options = options if options is not None else []\n",
    "\n",
    "    def _get_option_ids(self):\n",
    "        \"\"\"\n",
    "        Convert the options to input IDs using the tokenizer.\n",
    "        Returns:\n",
    "            A list of input IDs for each option.\n",
    "        \"\"\"\n",
    "        option_ids = []\n",
    "        for option in self.options:\n",
    "            inputs = self.tokenizer.encode(option, return_tensors=\"pt\", add_special_tokens=False).to(self.device)\n",
    "            option_ids.append(inputs[\"input_ids\"][0][0].item())\n",
    "        return option_ids\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the model to generate predictions.\n",
    "        Args:\n",
    "            input_ids: Input IDs for the model.\n",
    "            attention_mask: Attention mask for the model.\n",
    "        Returns:\n",
    "            Probabilities for each option.\n",
    "        \"\"\"\n",
    "        logits = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        ).logits\n",
    "\n",
    "        probs = torch.nn.functional.softmax(logits[0, -1], dim=-1)\n",
    "        option_ids = self._get_option_ids()\n",
    "        option_probs = []\n",
    "        for option_id in option_ids:\n",
    "            option_probs.append(probs[option_id].item())\n",
    "        return torch.tensor(option_probs)"
   ],
   "id": "7d29f235d5699cfd",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T02:30:29.233291Z",
     "start_time": "2025-05-12T02:30:29.230438Z"
    }
   },
   "cell_type": "code",
   "source": "options = [\"A\", \"B\", \"C\", \"D\"]",
   "id": "f466b01823021f32",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "options_picker = OptionsPicker(model, tokenizer, options=options, device=\"cuda\")\n",
   "id": "1efd910de028b59f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T01:47:37.000083Z",
     "start_time": "2025-05-12T01:47:36.944417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_tokenized = tokenizer(x, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=MAX_LENGTH)\n",
    "out = options_picker(x_tokenized[\"input_ids\"].to(device), x_tokenized[\"attention_mask\"].to(device))\n",
    "out"
   ],
   "id": "ecdce615b2c0a55c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 0.0000e+00, 1.5497e-06, 8.3447e-07])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T01:58:18.911314Z",
     "start_time": "2025-05-12T01:58:16.092526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "correct_predictions = 0\n",
    "validation_length = len(train_dataset)\n",
    "\n",
    "for i in range(validation_length):\n",
    "    x, y = train_dataset[i]\n",
    "    x = x.replace(\"\\n\\nAnswer:\", \"\\n\\nThe correct answer is:\")\n",
    "    x = f\"\"\"\n",
    "     Read the following context and answer the question by choosing the correct option.\n",
    "     {x}\"\"\"\n",
    "    print(x)\n",
    "    x_tokenized = tokenizer(x, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=800)\n",
    "    out = options_picker(x_tokenized[\"input_ids\"].to(device), x_tokenized[\"attention_mask\"].to(device))\n",
    "    answer = options[torch.argmax(out)]\n",
    "    if answer == y:\n",
    "        correct_predictions += 1\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processed {i} examples, current accuracy: {correct_predictions / (i + 1):.2f}\")\n",
    "\n",
    "accuracy = correct_predictions / validation_length\n",
    "accuracy"
   ],
   "id": "9fec61662a7598bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: The air hostess   was in a small kitchen at the back of the plane, preparing the plates for lunch, when a little old lady came and spoke to her, \"Could you please tell me,\" she asked, \"where is the ladies' lavatory   in the plane?\"\n",
      "\"Yes, madam,\" said the air hostess and smiled. \"It is right at the other end of the plane---at the front.\"\n",
      "The little lady went too far. She walked all the way to the front of the plane, opened the door in front of her, and saw the captain of the plane and the other officers. They were all busy with their work and did not see her. She went out again, shut the door and returned to the air hostess.\n",
      "\"Oh, didn't you find it, madam?\" the girl asked her. \"Yes, I did,\" said the little lady. \"But there are four men in the ladies' lavatory watching television.\"\n",
      "\n",
      "Question: The story happened  _  .\n",
      "\n",
      "Options:\n",
      "A) in the evening\n",
      "B) in the afternoon\n",
      "C) in the morning\n",
      "D) at midnight\n",
      "\n",
      "The correct answer is: C\n",
      "Processed 0 examples, current accuracy: 1.00\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: The air hostess   was in a small kitchen at the back of the plane, preparing the plates for lunch, when a little old lady came and spoke to her, \"Could you please tell me,\" she asked, \"where is the ladies' lavatory   in the plane?\"\n",
      "\"Yes, madam,\" said the air hostess and smiled. \"It is right at the other end of the plane---at the front.\"\n",
      "The little lady went too far. She walked all the way to the front of the plane, opened the door in front of her, and saw the captain of the plane and the other officers. They were all busy with their work and did not see her. She went out again, shut the door and returned to the air hostess.\n",
      "\"Oh, didn't you find it, madam?\" the girl asked her. \"Yes, I did,\" said the little lady. \"But there are four men in the ladies' lavatory watching television.\"\n",
      "\n",
      "Question: What were the four men doing when the little old lady opened the door?\n",
      "\n",
      "Options:\n",
      "A) They were busy working in the control room.\n",
      "B) They were working while watching television.\n",
      "C) They were watching television in the ladies' lavatory.\n",
      "D) They were enjoying themselves by watching television.\n",
      "\n",
      "The correct answer is: A\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: The air hostess   was in a small kitchen at the back of the plane, preparing the plates for lunch, when a little old lady came and spoke to her, \"Could you please tell me,\" she asked, \"where is the ladies' lavatory   in the plane?\"\n",
      "\"Yes, madam,\" said the air hostess and smiled. \"It is right at the other end of the plane---at the front.\"\n",
      "The little lady went too far. She walked all the way to the front of the plane, opened the door in front of her, and saw the captain of the plane and the other officers. They were all busy with their work and did not see her. She went out again, shut the door and returned to the air hostess.\n",
      "\"Oh, didn't you find it, madam?\" the girl asked her. \"Yes, I did,\" said the little lady. \"But there are four men in the ladies' lavatory watching television.\"\n",
      "\n",
      "Question: Which of the following is TRUE according to the passage?\n",
      "\n",
      "Options:\n",
      "A) The air hostess was humorous  .\n",
      "B) The old lady didn't find the ladies' lavatory.\n",
      "C) The old lady was fooled by the air hostess.\n",
      "D) There was no ladies' lavatory in the plane.\n",
      "\n",
      "The correct answer is: B\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Moscow,Russia(Space news)-\"The computer is a better chess player,\"insisted Viktor Prozorov,the loser .\" It seemed as if it were laughing after every good move.I know I should have beaten it for the sake of mankind ,but I just couldn't win,\" he announced and shook his head sadly.\n",
      "Prozorov's disappointment was shared by several grand masters who were present,some of whom were so upset that they shouted at the machine.Many chess players said that this meant the end of chess championships around the world,since the fun had been taken out of the game.\n",
      "The computer walked-or rather,rolled-away with 5,000 dollars in prize money and limited its remarks to a set of noises and light.\n",
      "\n",
      "Question: Which of the following best gives the main idea of this newspaper article?\n",
      "\n",
      "Options:\n",
      "A) 5,000 dollars goes to a computer!\n",
      "B) New invention a laughing computer!\n",
      "C) World's best chess player beaten!\n",
      "D) Computer defeats man in chess!\n",
      "\n",
      "The correct answer is: D\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Moscow,Russia(Space news)-\"The computer is a better chess player,\"insisted Viktor Prozorov,the loser .\" It seemed as if it were laughing after every good move.I know I should have beaten it for the sake of mankind ,but I just couldn't win,\" he announced and shook his head sadly.\n",
      "Prozorov's disappointment was shared by several grand masters who were present,some of whom were so upset that they shouted at the machine.Many chess players said that this meant the end of chess championships around the world,since the fun had been taken out of the game.\n",
      "The computer walked-or rather,rolled-away with 5,000 dollars in prize money and limited its remarks to a set of noises and light.\n",
      "\n",
      "Question: What was it that Prozorov felt most bitter about?\n",
      "\n",
      "Options:\n",
      "A) That he didn't win the$5,000.\n",
      "B) That he hadn't tried his best.\n",
      "C) That he had lost to a machine.\n",
      "D) That this was the end of the chess game.\n",
      "\n",
      "The correct answer is: C\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Moscow,Russia(Space news)-\"The computer is a better chess player,\"insisted Viktor Prozorov,the loser .\" It seemed as if it were laughing after every good move.I know I should have beaten it for the sake of mankind ,but I just couldn't win,\" he announced and shook his head sadly.\n",
      "Prozorov's disappointment was shared by several grand masters who were present,some of whom were so upset that they shouted at the machine.Many chess players said that this meant the end of chess championships around the world,since the fun had been taken out of the game.\n",
      "The computer walked-or rather,rolled-away with 5,000 dollars in prize money and limited its remarks to a set of noises and light.\n",
      "\n",
      "Question: After winning the game,the computer   _   .\n",
      "\n",
      "Options:\n",
      "A) laughed\n",
      "B) walked away\n",
      "C) made some remarks\n",
      "D) gave out some lights and sounds\n",
      "\n",
      "The correct answer is: D\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Moscow,Russia(Space news)-\"The computer is a better chess player,\"insisted Viktor Prozorov,the loser .\" It seemed as if it were laughing after every good move.I know I should have beaten it for the sake of mankind ,but I just couldn't win,\" he announced and shook his head sadly.\n",
      "Prozorov's disappointment was shared by several grand masters who were present,some of whom were so upset that they shouted at the machine.Many chess players said that this meant the end of chess championships around the world,since the fun had been taken out of the game.\n",
      "The computer walked-or rather,rolled-away with 5,000 dollars in prize money and limited its remarks to a set of noises and light.\n",
      "\n",
      "Question: Many chess players felt that playing with a computer would   _   .\n",
      "\n",
      "Options:\n",
      "A) make the game tougher\n",
      "B) make the game less interesting\n",
      "C) make man appear foolish\n",
      "D) make man lose lots of\n",
      "\n",
      "The correct answer is: B\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Read the following directions on a bottle of medicine:\n",
      "    \"Take two tablets with water, followed by one tablet every eight hours, as required, For maximum night-time and early morning relief, take two tablets at bed - time, Do not take more than six tablets in twenty-four hours. \n",
      "    For children six to twelve years old, give half the amount for a grownup. For children under six years old, ask for your doctor's advice.\n",
      "    Reduce the amount if nervousness,  _ , or sleeplessness occurs,\"\n",
      "\n",
      "Question: The directions on this medicine bottle clearly warn the patient not to take more than   _  .\n",
      "\n",
      "Options:\n",
      "A) twenty-four tablets a day.\n",
      "B) eight tablets a day.\n",
      "C) six tablets a day.\n",
      "D) three tablets a day.\n",
      "\n",
      "The correct answer is: C\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Read the following directions on a bottle of medicine:\n",
      "    \"Take two tablets with water, followed by one tablet every eight hours, as required, For maximum night-time and early morning relief, take two tablets at bed - time, Do not take more than six tablets in twenty-four hours. \n",
      "    For children six to twelve years old, give half the amount for a grownup. For children under six years old, ask for your doctor's advice.\n",
      "    Reduce the amount if nervousness,  _ , or sleeplessness occurs,\"\n",
      "\n",
      "Question: We can infer from the directions that   _  .\n",
      "\n",
      "Options:\n",
      "A) the medicine could cause some people to feel nervous.\n",
      "B) children may take the same amount that grown-ups take.\n",
      "C) one may not take this medicine before going to bed.\n",
      "D) the medicine is a liquid.\n",
      "\n",
      "The correct answer is: A\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Read the following directions on a bottle of medicine:\n",
      "    \"Take two tablets with water, followed by one tablet every eight hours, as required, For maximum night-time and early morning relief, take two tablets at bed - time, Do not take more than six tablets in twenty-four hours. \n",
      "    For children six to twelve years old, give half the amount for a grownup. For children under six years old, ask for your doctor's advice.\n",
      "    Reduce the amount if nervousness,  _ , or sleeplessness occurs,\"\n",
      "\n",
      "Question: If one cannot sleep, it is suggested that he   _   .\n",
      "\n",
      "Options:\n",
      "A) take two tablets before going to bed.\n",
      "B) take less than two tablets before going to bed.\n",
      "C) stop taking the medicine.\n",
      "D) ask advice of a doctor.\n",
      "\n",
      "The correct answer is: B\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Read the following directions on a bottle of medicine:\n",
      "    \"Take two tablets with water, followed by one tablet every eight hours, as required, For maximum night-time and early morning relief, take two tablets at bed - time, Do not take more than six tablets in twenty-four hours. \n",
      "    For children six to twelve years old, give half the amount for a grownup. For children under six years old, ask for your doctor's advice.\n",
      "    Reduce the amount if nervousness,  _ , or sleeplessness occurs,\"\n",
      "\n",
      "Question: Obviously the medicine   _   .\n",
      "\n",
      "Options:\n",
      "A) may be dangerous to small children.\n",
      "B) cannot be taken by children under twelve years old.\n",
      "C) may be taken by children but not by grown-ups.\n",
      "D) may be taken by grown-ups but not by children.\n",
      "\n",
      "The correct answer is: A\n",
      "Processed 10 examples, current accuracy: 0.36\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Even if you have been very careful, it is possible that your kids can develop some bad TV habits. It is not too late if the bad habits have already occurred.         56         .Follow the steps below to start breaking your kids'bad habits.\n",
      "Limit the time of watching TV. Limit the TV to at most two hours a day for kids over two years old and no TV at all for kids under the age of two. This does not mean that you have to allow the two hours each day.       57\n",
      "\n",
      "Question: Make it clear that you are firm about not allowing them to watch the program.\n",
      "\n",
      "Options:\n",
      "A) less TV is better.\n",
      "B) Make the TV hard to use.\n",
      "C) Be a good example to your kids.\n",
      "D) Take the TV out of their room or put it away in storage.\n",
      "E. Explain to them why you feel the show is not good for them.\n",
      "F. This is the best way to make sure that they are watching the right programs.\n",
      "G. You just need to be ready to take control of the situation and break those bad habits.\n",
      "\n",
      "The correct answer is: C\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Classes for foreign students at all levels\n",
      "3 months, 6 months, 9 months and one year course Open all year\n",
      "Small class (maximum  12 students)\n",
      "Library, language laboratory and listening center\n",
      "Accommodation  with selected families 25 minutes from London\n",
      "Course fees for English for one year are PS1, 380, with reduction for shorter periods of study.\n",
      "\n",
      "Question: Lincoln College of English   _  .\n",
      "\n",
      "Options:\n",
      "A) is at the centre of London\n",
      "B) lies far away from London\n",
      "C) takes in foreign students, from beginners to the advanced\n",
      "D) accepts students only at the beginning of the year\n",
      "\n",
      "The correct answer is: C\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Classes for foreign students at all levels\n",
      "3 months, 6 months, 9 months and one year course Open all year\n",
      "Small class (maximum  12 students)\n",
      "Library, language laboratory and listening center\n",
      "Accommodation  with selected families 25 minutes from London\n",
      "Course fees for English for one year are PS1, 380, with reduction for shorter periods of study.\n",
      "\n",
      "Question: While you stay there,   _   will take care of you.\n",
      "\n",
      "Options:\n",
      "A) the school where you study\n",
      "B) the family you have chosen\n",
      "C) your classmates\n",
      "D) your own parents\n",
      "\n",
      "The correct answer is: B\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Classes for foreign students at all levels\n",
      "3 months, 6 months, 9 months and one year course Open all year\n",
      "Small class (maximum  12 students)\n",
      "Library, language laboratory and listening center\n",
      "Accommodation  with selected families 25 minutes from London\n",
      "Course fees for English for one year are PS1, 380, with reduction for shorter periods of study.\n",
      "\n",
      "Question: If you go there for a one-term course, you will pay   _   for it.\n",
      "\n",
      "Options:\n",
      "A) PS1, 380\n",
      "B) over PS1, 380\n",
      "C) much less than PS1, 380\n",
      "D) nothing\n",
      "\n",
      "The correct answer is: C\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: A man heard that a certain government wanted a clerk, so wrote and asked for the position. But while he was waiting for an answer, a friend of his introduced him to the head of the department, who gave him the job. Several months later, while the man was working in the department, he got a letter that had been sent to him from the place he used to live in. This letter said,\n",
      "\"Dear sir,\n",
      "We are sorry to have to tell you that we cannot offer work in this department because we do not think that you would be able to do the work successfully.\n",
      "Your faithfully.\"\n",
      "The man laughed, but when he looked at the letter more carefully, he saw that he had signed it himself.\n",
      "\n",
      "Question: At the beginning of the story the man wanted   _  .\n",
      "\n",
      "Options:\n",
      "A) to be a clerk in a certain government\n",
      "B) his friend to introduce him job\n",
      "C) to visit the head of the department\n",
      "D) to get an answer from the department\n",
      "\n",
      "The correct answer is: A\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: A man heard that a certain government wanted a clerk, so wrote and asked for the position. But while he was waiting for an answer, a friend of his introduced him to the head of the department, who gave him the job. Several months later, while the man was working in the department, he got a letter that had been sent to him from the place he used to live in. This letter said,\n",
      "\"Dear sir,\n",
      "We are sorry to have to tell you that we cannot offer work in this department because we do not think that you would be able to do the work successfully.\n",
      "Your faithfully.\"\n",
      "The man laughed, but when he looked at the letter more carefully, he saw that he had signed it himself.\n",
      "\n",
      "Question: He got a letter that had been sent to him, which means   _  .\n",
      "\n",
      "Options:\n",
      "A) he himself received the letter\n",
      "B) someone else got it and then didn't bring it to him\n",
      "C) someone else got it and then brought it to him\n",
      "D) the government sent it to him\n",
      "\n",
      "The correct answer is: C\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: A man heard that a certain government wanted a clerk, so wrote and asked for the position. But while he was waiting for an answer, a friend of his introduced him to the head of the department, who gave him the job. Several months later, while the man was working in the department, he got a letter that had been sent to him from the place he used to live in. This letter said,\n",
      "\"Dear sir,\n",
      "We are sorry to have to tell you that we cannot offer work in this department because we do not think that you would be able to do the work successfully.\n",
      "Your faithfully.\"\n",
      "The man laughed, but when he looked at the letter more carefully, he saw that he had signed it himself.\n",
      "\n",
      "Question: What do you think of the man?\n",
      "\n",
      "Options:\n",
      "A) He was too careless.\n",
      "B) He was very honest.\n",
      "C) He was very clever.\n",
      "D) He was rather stupid.\n",
      "\n",
      "The correct answer is: A\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Do you need friends? I'm sure your answer is \"Yes,of course. Everybody does! \" You need friends when you play and when you work. If you have friends, you will feel happy. If you have no friends. you will feel lonely  . \n",
      "Do you know how to make friends? There is only one good way--You make friends by being friendly. \n",
      "A friendly person is interested in other people. He is always helpful If you want to make friends with a new classmate, you can talk with him, tell him about the other classmates in your class and try your best to be helpful to him.\n",
      "\n",
      "Question: We need friends   _  .\n",
      "\n",
      "Options:\n",
      "A) because we must play with them\n",
      "B) Because we must work with them\n",
      "C) when we play and when we work\n",
      "D) when we talk with them\n",
      "\n",
      "The correct answer is: C\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Do you need friends? I'm sure your answer is \"Yes,of course. Everybody does! \" You need friends when you play and when you work. If you have friends, you will feel happy. If you have no friends. you will feel lonely  . \n",
      "Do you know how to make friends? There is only one good way--You make friends by being friendly. \n",
      "A friendly person is interested in other people. He is always helpful If you want to make friends with a new classmate, you can talk with him, tell him about the other classmates in your class and try your best to be helpful to him.\n",
      "\n",
      "Question: If we want to make friends, we should   _  .\n",
      "\n",
      "Options:\n",
      "A) be politely to them\n",
      "B) be friendly to them.\n",
      "C) be afraid of them\n",
      "D) when we talk with them.\n",
      "\n",
      "The correct answer is: B\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Do you need friends? I'm sure your answer is \"Yes,of course. Everybody does! \" You need friends when you play and when you work. If you have friends, you will feel happy. If you have no friends. you will feel lonely  . \n",
      "Do you know how to make friends? There is only one good way--You make friends by being friendly. \n",
      "A friendly person is interested in other people. He is always helpful If you want to make friends with a new classmate, you can talk with him, tell him about the other classmates in your class and try your best to be helpful to him.\n",
      "\n",
      "Question: A friendly person is   _   other people.\n",
      "\n",
      "Options:\n",
      "A) interested in\n",
      "B) worried about\n",
      "C) surprised at\n",
      "D) like them\n",
      "\n",
      "The correct answer is: A\n",
      "Processed 20 examples, current accuracy: 0.33\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Do you need friends? I'm sure your answer is \"Yes,of course. Everybody does! \" You need friends when you play and when you work. If you have friends, you will feel happy. If you have no friends. you will feel lonely  . \n",
      "Do you know how to make friends? There is only one good way--You make friends by being friendly. \n",
      "A friendly person is interested in other people. He is always helpful If you want to make friends with a new classmate, you can talk with him, tell him about the other classmates in your class and try your best to be helpful to him.\n",
      "\n",
      "Question: If we want to make friends with a new classmate,   _  .\n",
      "\n",
      "Options:\n",
      "A) we can talk with them\n",
      "B) we must try to help him\n",
      "C) we can tell him about the other classmates in our class\n",
      "D) A, B and C\n",
      "\n",
      "The correct answer is: D\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Do you need friends? I'm sure your answer is \"Yes,of course. Everybody does! \" You need friends when you play and when you work. If you have friends, you will feel happy. If you have no friends. you will feel lonely  . \n",
      "Do you know how to make friends? There is only one good way--You make friends by being friendly. \n",
      "A friendly person is interested in other people. He is always helpful If you want to make friends with a new classmate, you can talk with him, tell him about the other classmates in your class and try your best to be helpful to him.\n",
      "\n",
      "Question: Which of the following is true?\n",
      "\n",
      "Options:\n",
      "A) No one needs friends.\n",
      "B) Everyone needs friends.\n",
      "C) Only classmates need friends.\n",
      "D) Someone needs friends.\n",
      "\n",
      "The correct answer is: B\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: People who cannot tell all colors apart are said to be color-blind. Most color-blind people can see yellows and blues, but confuse reds with green. It is very rare for a person to be blind to all colors, but they may see everything in shades of black, white and gray.\n",
      "It is interesting to point out that many color-blind people don't even realize that they are color-blind, they don't know that the colors they are seeing and naming are not the actual colors that people with normal vision can see. This can be dangerous when a color-blind person confuses the red and green of a traffic light.\n",
      "Color blindness is thought to be inherited and although doctors have tested color blindness, there is no cure to treatment for it.\n",
      "\n",
      "Question: There are four cards here, and each has two colors. Which card's colors do you think a color-blind person can tell correctly?\n",
      "\n",
      "Options:\n",
      "A) Red, Green\n",
      "B) Green, Yellow\n",
      "C) Red, Brown\n",
      "D) Brown, Yellow\n",
      "\n",
      "The correct answer is: D\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: People who cannot tell all colors apart are said to be color-blind. Most color-blind people can see yellows and blues, but confuse reds with green. It is very rare for a person to be blind to all colors, but they may see everything in shades of black, white and gray.\n",
      "It is interesting to point out that many color-blind people don't even realize that they are color-blind, they don't know that the colors they are seeing and naming are not the actual colors that people with normal vision can see. This can be dangerous when a color-blind person confuses the red and green of a traffic light.\n",
      "Color blindness is thought to be inherited and although doctors have tested color blindness, there is no cure to treatment for it.\n",
      "\n",
      "Question: A color-blind person  _  .\n",
      "\n",
      "Options:\n",
      "A) always knows how color-blind he is\n",
      "B) often gives the wrong name of colors\n",
      "C) see everything as the same color\n",
      "D) can hardly see something of strong colors\n",
      "\n",
      "The correct answer is: B\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: People who cannot tell all colors apart are said to be color-blind. Most color-blind people can see yellows and blues, but confuse reds with green. It is very rare for a person to be blind to all colors, but they may see everything in shades of black, white and gray.\n",
      "It is interesting to point out that many color-blind people don't even realize that they are color-blind, they don't know that the colors they are seeing and naming are not the actual colors that people with normal vision can see. This can be dangerous when a color-blind person confuses the red and green of a traffic light.\n",
      "Color blindness is thought to be inherited and although doctors have tested color blindness, there is no cure to treatment for it.\n",
      "\n",
      "Question: It's especially dangerous for a color-blind person to cross a street when  _\n",
      "\n",
      "Options:\n",
      "A) it is crowded\n",
      "B) it is a dark night\n",
      "C) there are no traffic lights at the cross of the streets\n",
      "D) the traffic light turns red\n",
      "\n",
      "The correct answer is: D\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: People who cannot tell all colors apart are said to be color-blind. Most color-blind people can see yellows and blues, but confuse reds with green. It is very rare for a person to be blind to all colors, but they may see everything in shades of black, white and gray.\n",
      "It is interesting to point out that many color-blind people don't even realize that they are color-blind, they don't know that the colors they are seeing and naming are not the actual colors that people with normal vision can see. This can be dangerous when a color-blind person confuses the red and green of a traffic light.\n",
      "Color blindness is thought to be inherited and although doctors have tested color blindness, there is no cure to treatment for it.\n",
      "\n",
      "Question: A person who is color-blind is believed to have something to do with  _  .\n",
      "\n",
      "Options:\n",
      "A) his old age\n",
      "B) his poor eyesight\n",
      "C) his parents or grandparents\n",
      "D) his living condition\n",
      "\n",
      "The correct answer is: C\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: People who cannot tell all colors apart are said to be color-blind. Most color-blind people can see yellows and blues, but confuse reds with green. It is very rare for a person to be blind to all colors, but they may see everything in shades of black, white and gray.\n",
      "It is interesting to point out that many color-blind people don't even realize that they are color-blind, they don't know that the colors they are seeing and naming are not the actual colors that people with normal vision can see. This can be dangerous when a color-blind person confuses the red and green of a traffic light.\n",
      "Color blindness is thought to be inherited and although doctors have tested color blindness, there is no cure to treatment for it.\n",
      "\n",
      "Question: Up to now, doctors  _  .\n",
      "\n",
      "Options:\n",
      "A) have found a way to prevent a person from getting color-blind\n",
      "B) have found a way to free a person from his color blindness\n",
      "C) have been able to tell whether a person is color-blind or not\n",
      "D) have made it quite clear the cause of color blindness\n",
      "\n",
      "The correct answer is: C\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Once there was a poor farmer and his farm belonged to  a rich man. One day he brought a basket of apples to the rich man's house. On the doorsteps, he met two monkeys dressed like children. They jumped onto the basket to eat the apples and threw some on the ground. The farmer politely took off his hat and asked the monkeys to get off. They obeyed  and the farmer went into the house. He asked to see the rich man. A servant took him to the room where the rich man was sitting.\n",
      "\"I have brought you the basket of apples you asked for,\" he said.\n",
      "\"But why have you brought a half-empty basket?\" the rich man asked.\n",
      "\"I met your children outside, and they stole  some of the apples.\"\n",
      "\n",
      "Question: Why did the farmer bring apples to the rich man? Because\n",
      "\n",
      "Options:\n",
      "A) his farm belonged to the rich man\n",
      "B) he liked the rich man\n",
      "C) he was poor\n",
      "D) the rich man's children liked apples\n",
      "\n",
      "The correct answer is: C\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Once there was a poor farmer and his farm belonged to  a rich man. One day he brought a basket of apples to the rich man's house. On the doorsteps, he met two monkeys dressed like children. They jumped onto the basket to eat the apples and threw some on the ground. The farmer politely took off his hat and asked the monkeys to get off. They obeyed  and the farmer went into the house. He asked to see the rich man. A servant took him to the room where the rich man was sitting.\n",
      "\"I have brought you the basket of apples you asked for,\" he said.\n",
      "\"But why have you brought a half-empty basket?\" the rich man asked.\n",
      "\"I met your children outside, and they stole  some of the apples.\"\n",
      "\n",
      "Question: What did the monkeys do when the farmer was on the doorsteps?\n",
      "\n",
      "Options:\n",
      "A) They jumped and jumped.\n",
      "B) They played.\n",
      "C) They ate some of the apples.\n",
      "D) They ran away.\n",
      "\n",
      "The correct answer is: D\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Once there was a poor farmer and his farm belonged to  a rich man. One day he brought a basket of apples to the rich man's house. On the doorsteps, he met two monkeys dressed like children. They jumped onto the basket to eat the apples and threw some on the ground. The farmer politely took off his hat and asked the monkeys to get off. They obeyed  and the farmer went into the house. He asked to see the rich man. A servant took him to the room where the rich man was sitting.\n",
      "\"I have brought you the basket of apples you asked for,\" he said.\n",
      "\"But why have you brought a half-empty basket?\" the rich man asked.\n",
      "\"I met your children outside, and they stole  some of the apples.\"\n",
      "\n",
      "Question: The monkeys left the basket because\n",
      "\n",
      "Options:\n",
      "A) they had thrown apples on the ground\n",
      "B) the farmer had politely asked them to get off\n",
      "C) they were afraid of the hat\n",
      "D) the farmer was angry wit h them\n",
      "\n",
      "The correct answer is: D\n",
      "Processed 30 examples, current accuracy: 0.29\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Once there was a poor farmer and his farm belonged to  a rich man. One day he brought a basket of apples to the rich man's house. On the doorsteps, he met two monkeys dressed like children. They jumped onto the basket to eat the apples and threw some on the ground. The farmer politely took off his hat and asked the monkeys to get off. They obeyed  and the farmer went into the house. He asked to see the rich man. A servant took him to the room where the rich man was sitting.\n",
      "\"I have brought you the basket of apples you asked for,\" he said.\n",
      "\"But why have you brought a half-empty basket?\" the rich man asked.\n",
      "\"I met your children outside, and they stole  some of the apples.\"\n",
      "\n",
      "Question: How did the rich man feel when he saw the basket?  He felt_.\n",
      "\n",
      "Options:\n",
      "A) pleased\n",
      "B) unhappy\n",
      "C) excited\n",
      "D) moved\n",
      "\n",
      "The correct answer is: B\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Annealing is a way of making metal softer by heating it and then letting it cool very slowly. If metal is heated and then cooled very quickly , for example by dipping it in water , it will be very hard but also very brittle ---that is , It will break easily. Metal that has been annealed is soft but does not break as easily . It is possible to make metal as hard or as soft as is wished, by annealing it. The metal is heated, and allowed to cool slowly , for a certain length of time. The longer the heated metal takes to cool slowly , the softer it becomes . Annealing can also be used on other materials, such as glass.\n",
      "\n",
      "Question: Annealing can make metal   _  .\n",
      "\n",
      "Options:\n",
      "A) hard and tough\n",
      "B) hard but brittle\n",
      "C) soft but tough\n",
      "D) soft and brittle\n",
      "\n",
      "The correct answer is: C\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Annealing is a way of making metal softer by heating it and then letting it cool very slowly. If metal is heated and then cooled very quickly , for example by dipping it in water , it will be very hard but also very brittle ---that is , It will break easily. Metal that has been annealed is soft but does not break as easily . It is possible to make metal as hard or as soft as is wished, by annealing it. The metal is heated, and allowed to cool slowly , for a certain length of time. The longer the heated metal takes to cool slowly , the softer it becomes . Annealing can also be used on other materials, such as glass.\n",
      "\n",
      "Question: Why do people put hot metal in water ?   _\n",
      "\n",
      "Options:\n",
      "A) To make it hard\n",
      "B) To make it soft\n",
      "C) To make it cool\n",
      "D) To make it brittle\n",
      "\n",
      "The correct answer is: A\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Annealing is a way of making metal softer by heating it and then letting it cool very slowly. If metal is heated and then cooled very quickly , for example by dipping it in water , it will be very hard but also very brittle ---that is , It will break easily. Metal that has been annealed is soft but does not break as easily . It is possible to make metal as hard or as soft as is wished, by annealing it. The metal is heated, and allowed to cool slowly , for a certain length of time. The longer the heated metal takes to cool slowly , the softer it becomes . Annealing can also be used on other materials, such as glass.\n",
      "\n",
      "Question: As suggested by the text , how can glass be made less brittle ?  _\n",
      "\n",
      "Options:\n",
      "A) It can be heated and then cooled quickly\n",
      "B) It can be cooled and then heated slowly\n",
      "C) It can be heated and then cooled slowly\n",
      "D) It can be cooled and then heated quickly\n",
      "\n",
      "The correct answer is: C\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: A fad diet is a diet that suddenly becomes popular, usually because it promises people that they will lose a lot of weight overnight. Although dieters may really show weight loss from a fad diet, they will almost certainly get back the weight if their usual eating habits   remain unchanged. Moreover, fad diets continuously call for special products of questionable value or for \"health foods\" that may cost twice as much as supermarket  foods. Worse yet, certain fad diets don't include nutrients good for health. People have been known to become very ill as a result of following medically unhealthy diets.\n",
      "The worse fact of fad diets, though, is that they keep people who are over-weight or sick from receiving the medical care that they truly need.\n",
      "\n",
      "Question: The word \"fad\" in the first sentence means   _  .\n",
      "\n",
      "Options:\n",
      "A) new\n",
      "B) wonderful\n",
      "C) popular quickly\n",
      "D) loved by people\n",
      "\n",
      "The correct answer is: A\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: A fad diet is a diet that suddenly becomes popular, usually because it promises people that they will lose a lot of weight overnight. Although dieters may really show weight loss from a fad diet, they will almost certainly get back the weight if their usual eating habits   remain unchanged. Moreover, fad diets continuously call for special products of questionable value or for \"health foods\" that may cost twice as much as supermarket  foods. Worse yet, certain fad diets don't include nutrients good for health. People have been known to become very ill as a result of following medically unhealthy diets.\n",
      "The worse fact of fad diets, though, is that they keep people who are over-weight or sick from receiving the medical care that they truly need.\n",
      "\n",
      "Question: From the text we know that   _  .\n",
      "\n",
      "Options:\n",
      "A) people refuse to accept fad diets\n",
      "B) fad diets can help people to lose weight\n",
      "C) a man can lose weight after eating a fad diet\n",
      "D) fad diets are now welcomed by people\n",
      "\n",
      "The correct answer is: B\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: A fad diet is a diet that suddenly becomes popular, usually because it promises people that they will lose a lot of weight overnight. Although dieters may really show weight loss from a fad diet, they will almost certainly get back the weight if their usual eating habits   remain unchanged. Moreover, fad diets continuously call for special products of questionable value or for \"health foods\" that may cost twice as much as supermarket  foods. Worse yet, certain fad diets don't include nutrients good for health. People have been known to become very ill as a result of following medically unhealthy diets.\n",
      "The worse fact of fad diets, though, is that they keep people who are over-weight or sick from receiving the medical care that they truly need.\n",
      "\n",
      "Question: In the writer's opinion, fad diets  _  .\n",
      "\n",
      "Options:\n",
      "A) are very helpful\n",
      "B) are not healthy diet\n",
      "C) won't cost much\n",
      "D) include lots of nutrients\n",
      "\n",
      "The correct answer is: B\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: The following notice is posted in the bus station.\n",
      "Time Table:\n",
      "*Buses leave the Railway Station, New York City, from7:00 a.m. and every half-hour thereafter, until 11:30 p.m. (7 days a week)\n",
      "*Buses leave Brennan Station 20 minutes before and after every hour from 6:20 a.m. to 11:40 p.m. (7 days a week)\n",
      "*Evening rush hours (5:00 p.m. to 7:00 p.m.); Buses leave the Railway Station, New York City every 15 minutes.(Monday--Friday)\n",
      "*Holidays Buses leave every hour on the hour, each direction.(Trip time:30minutes each way)\n",
      "*All tickets must be bought at Window 12, the Railway Station, New York City, or at the Brennan Station Window BEFORE boarding buses.\n",
      "\n",
      "Question: If you want to take a bus in the evening rush hour, you should take the   _                         in the Railway Station, New York City on Monday.\n",
      "\n",
      "Options:\n",
      "A) 6:20 p.m.\n",
      "B) 5:45 p.m.\n",
      "C) 8:00 p.m.\n",
      "D) 7:15 p.m.\n",
      "\n",
      "The correct answer is: B\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: The following notice is posted in the bus station.\n",
      "Time Table:\n",
      "*Buses leave the Railway Station, New York City, from7:00 a.m. and every half-hour thereafter, until 11:30 p.m. (7 days a week)\n",
      "*Buses leave Brennan Station 20 minutes before and after every hour from 6:20 a.m. to 11:40 p.m. (7 days a week)\n",
      "*Evening rush hours (5:00 p.m. to 7:00 p.m.); Buses leave the Railway Station, New York City every 15 minutes.(Monday--Friday)\n",
      "*Holidays Buses leave every hour on the hour, each direction.(Trip time:30minutes each way)\n",
      "*All tickets must be bought at Window 12, the Railway Station, New York City, or at the Brennan Station Window BEFORE boarding buses.\n",
      "\n",
      "Question: You'll go back home from work in Brennan. Which of the following bus will you take?\n",
      "\n",
      "Options:\n",
      "A) 7:30 p.m.\n",
      "B) 6:00 a.m.\n",
      "C) 5:45 p.m.\n",
      "D) 6:20 p.m.\n",
      "\n",
      "The correct answer is: D\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: The following notice is posted in the bus station.\n",
      "Time Table:\n",
      "*Buses leave the Railway Station, New York City, from7:00 a.m. and every half-hour thereafter, until 11:30 p.m. (7 days a week)\n",
      "*Buses leave Brennan Station 20 minutes before and after every hour from 6:20 a.m. to 11:40 p.m. (7 days a week)\n",
      "*Evening rush hours (5:00 p.m. to 7:00 p.m.); Buses leave the Railway Station, New York City every 15 minutes.(Monday--Friday)\n",
      "*Holidays Buses leave every hour on the hour, each direction.(Trip time:30minutes each way)\n",
      "*All tickets must be bought at Window 12, the Railway Station, New York City, or at the Brennan Station Window BEFORE boarding buses.\n",
      "\n",
      "Question: Where should passengers buy their tickets?\n",
      "\n",
      "Options:\n",
      "A) From the bus driver.\n",
      "B) On the bus after getting on it.\n",
      "C) From the conductor.\n",
      "D) At the station before boarding.\n",
      "\n",
      "The correct answer is: D\n",
      "Processed 40 examples, current accuracy: 0.27\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Have you ever heard your own voice? \"Of course,\" you say.\n",
      "Has anyone else ever heard your voice? Again you say, \"Of course.\"\n",
      "But that's not quite true. Nobody else has ever heard your voice-the way you hear it. When you talk, you set up sound waves. The air outside your head carries the sound waves to your outer ears. But, of course, the sound of your voice begins inside your head. The bones of your head pick up the sound waves, too. They carry the sound waves straight to your inner ears. You get the sound from the outside and the inside too. Other people get just the sound waves from the outside. That is why they don't hear your voice the way you do.\n",
      "\n",
      "Question: You   _  hear your voice the same way others hear it.\n",
      "\n",
      "Options:\n",
      "A) can't\n",
      "B) seldom\n",
      "C) sometimes\n",
      "D) always\n",
      "\n",
      "The correct answer is: A\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Have you ever heard your own voice? \"Of course,\" you say.\n",
      "Has anyone else ever heard your voice? Again you say, \"Of course.\"\n",
      "But that's not quite true. Nobody else has ever heard your voice-the way you hear it. When you talk, you set up sound waves. The air outside your head carries the sound waves to your outer ears. But, of course, the sound of your voice begins inside your head. The bones of your head pick up the sound waves, too. They carry the sound waves straight to your inner ears. You get the sound from the outside and the inside too. Other people get just the sound waves from the outside. That is why they don't hear your voice the way you do.\n",
      "\n",
      "Question: The passage is mainly about  _  .\n",
      "\n",
      "Options:\n",
      "A) waves in the air\n",
      "B) the way you hear your own voice\n",
      "C) voice spreading far and wide\n",
      "D) the different way people hear their voices\n",
      "\n",
      "The correct answer is: B\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: The horsepower was first used two hundred years ago. James Watt had made the world's first widely used steam engine. He had no way of telling people exactly how powerful it was, for at that time there were no units for measuring power.\n",
      "    Watt decided to find out how much work one strong horse could do in one minute. He called that init one horsepower. With this unit he could measure the work his steam engine could do.\n",
      "    He discovered that a horse could lift a 3,300-pound weight 10 feet into the air in one minute. His engine could lift a 3,300 pound weight 100 feet in one minute.\n",
      "    Because his engine did ten times as much work as the horse, Watt called it a ten-horsepower engine.\n",
      "\n",
      "Question: Watt made the world's first   _  .\n",
      "\n",
      "Options:\n",
      "A) train\n",
      "B) engine\n",
      "C) steam engine\n",
      "D) bus\n",
      "\n",
      "The correct answer is: C\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: The horsepower was first used two hundred years ago. James Watt had made the world's first widely used steam engine. He had no way of telling people exactly how powerful it was, for at that time there were no units for measuring power.\n",
      "    Watt decided to find out how much work one strong horse could do in one minute. He called that init one horsepower. With this unit he could measure the work his steam engine could do.\n",
      "    He discovered that a horse could lift a 3,300-pound weight 10 feet into the air in one minute. His engine could lift a 3,300 pound weight 100 feet in one minute.\n",
      "    Because his engine did ten times as much work as the horse, Watt called it a ten-horsepower engine.\n",
      "\n",
      "Question: Watt wanted to find a way   _  .\n",
      "\n",
      "Options:\n",
      "A) to lift a 3,300-pound weight\n",
      "B) to show how useful his steam engine was\n",
      "C) to tell people exactly how powerful his steam engine was\n",
      "D) to measure the weight of his steam engine\n",
      "\n",
      "The correct answer is: C\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: The horsepower was first used two hundred years ago. James Watt had made the world's first widely used steam engine. He had no way of telling people exactly how powerful it was, for at that time there were no units for measuring power.\n",
      "    Watt decided to find out how much work one strong horse could do in one minute. He called that init one horsepower. With this unit he could measure the work his steam engine could do.\n",
      "    He discovered that a horse could lift a 3,300-pound weight 10 feet into the air in one minute. His engine could lift a 3,300 pound weight 100 feet in one minute.\n",
      "    Because his engine did ten times as much work as the horse, Watt called it a ten-horsepower engine.\n",
      "\n",
      "Question: What does one horsepower mean? It means   _  .\n",
      "\n",
      "Options:\n",
      "A) one horse's power\n",
      "B) what one strong horse can do in one minute\n",
      "C) what one horse can do in a day\n",
      "D) what work one horse can do as much as possible\n",
      "\n",
      "The correct answer is: B\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: The horsepower was first used two hundred years ago. James Watt had made the world's first widely used steam engine. He had no way of telling people exactly how powerful it was, for at that time there were no units for measuring power.\n",
      "    Watt decided to find out how much work one strong horse could do in one minute. He called that init one horsepower. With this unit he could measure the work his steam engine could do.\n",
      "    He discovered that a horse could lift a 3,300-pound weight 10 feet into the air in one minute. His engine could lift a 3,300 pound weight 100 feet in one minute.\n",
      "    Because his engine did ten times as much work as the horse, Watt called it a ten-horsepower engine.\n",
      "\n",
      "Question: Which is not true?   _  .\n",
      "\n",
      "Options:\n",
      "A) Watt decided to find out how much work one strong horse could do in one minute\n",
      "B) Watt decided to make the world's first widely used horse engine\n",
      "C) He wanted to find a way to tell people exactly how powerful his engine was\n",
      "D) He wanted to measure the work his engine could do\n",
      "\n",
      "The correct answer is: B\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: The horsepower was first used two hundred years ago. James Watt had made the world's first widely used steam engine. He had no way of telling people exactly how powerful it was, for at that time there were no units for measuring power.\n",
      "    Watt decided to find out how much work one strong horse could do in one minute. He called that init one horsepower. With this unit he could measure the work his steam engine could do.\n",
      "    He discovered that a horse could lift a 3,300-pound weight 10 feet into the air in one minute. His engine could lift a 3,300 pound weight 100 feet in one minute.\n",
      "    Because his engine did ten times as much work as the horse, Watt called it a ten-horsepower engine.\n",
      "\n",
      "Question: The best headline for the article is   _  .\n",
      "\n",
      "Options:\n",
      "A) Horsepower\n",
      "B) Watt's steam engine\n",
      "C) A ten-horsepower engine\n",
      "D) The beginning of horsepower\n",
      "\n",
      "The correct answer is: D\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Bet Winner\n",
      "MILLBURN, New Jersey-- An 11-year-old boy, he gave up television for a year in a bet with his mother, says he will use some of the money to buy himself an astronaut's suit. The bet ended at 9:01 on Monday morning, but Benjamin waited until his mother, Roslyn, handed him five 100-dollar bills in front of a gathering of newspapermen in the afternoon before switching on the TV. During the past year, he has filled his time reading and his grades have improved from ''satisfactory\" to ''very good.\"\n",
      "CHINA DAILY, Wednesday, March 9, 2011  ( 94 words )\n",
      "\n",
      "Question: Who lost a bet to whom?\n",
      "\n",
      "Options:\n",
      "A) The boy's mother to his father\n",
      "B) Benjamin's father to Roslyn\n",
      "C) Benjamin to his mother\n",
      "D) Roslyn to Benjamin\n",
      "\n",
      "The correct answer is: D\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Bet Winner\n",
      "MILLBURN, New Jersey-- An 11-year-old boy, he gave up television for a year in a bet with his mother, says he will use some of the money to buy himself an astronaut's suit. The bet ended at 9:01 on Monday morning, but Benjamin waited until his mother, Roslyn, handed him five 100-dollar bills in front of a gathering of newspapermen in the afternoon before switching on the TV. During the past year, he has filled his time reading and his grades have improved from ''satisfactory\" to ''very good.\"\n",
      "CHINA DAILY, Wednesday, March 9, 2011  ( 94 words )\n",
      "\n",
      "Question: For how long had the boy kept himself from turning on the TV?\n",
      "\n",
      "Options:\n",
      "A) From Monday morning till afternoon\n",
      "B) For one year and several hours\n",
      "C) For one year and a day\n",
      "D) For one year\n",
      "\n",
      "The correct answer is: D\n",
      "\n",
      "     Read the following context and answer the question by choosing the correct option.\n",
      "     Context: Bet Winner\n",
      "MILLBURN, New Jersey-- An 11-year-old boy, he gave up television for a year in a bet with his mother, says he will use some of the money to buy himself an astronaut's suit. The bet ended at 9:01 on Monday morning, but Benjamin waited until his mother, Roslyn, handed him five 100-dollar bills in front of a gathering of newspapermen in the afternoon before switching on the TV. During the past year, he has filled his time reading and his grades have improved from ''satisfactory\" to ''very good.\"\n",
      "CHINA DAILY, Wednesday, March 9, 2011  ( 94 words )\n",
      "\n",
      "Question: Why did the mother hand the bills to the boy?\n",
      "\n",
      "Options:\n",
      "A) Because his grades had improved\n",
      "B) Because he had won some money\n",
      "C) Because he wanted to buy an astronaut's suit\n",
      "D) Because she had given him her promise\n",
      "\n",
      "The correct answer is: D\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(x)\n\u001B[1;32m     11\u001B[0m x_tokenized \u001B[38;5;241m=\u001B[39m tokenizer(x, padding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_length\u001B[39m\u001B[38;5;124m\"\u001B[39m, truncation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m, max_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m800\u001B[39m)\n\u001B[0;32m---> 12\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43moptions_picker\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx_tokenized\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minput_ids\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_tokenized\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mattention_mask\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m answer \u001B[38;5;241m=\u001B[39m options[torch\u001B[38;5;241m.\u001B[39margmax(out)]\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer \u001B[38;5;241m==\u001B[39m y:\n",
      "File \u001B[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[7], line 39\u001B[0m, in \u001B[0;36mOptionsPicker.forward\u001B[0;34m(self, input_ids, attention_mask)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, input_ids, attention_mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m     31\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;124;03m    Forward pass through the model to generate predictions.\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;124;03m        Probabilities for each option.\u001B[39;00m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 39\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     41\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     42\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mlogits\n\u001B[1;32m     44\u001B[0m     probs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39msoftmax(logits[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     45\u001B[0m     option_ids \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_option_ids()\n",
      "File \u001B[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001B[0m, in \u001B[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action\u001B[38;5;241m.\u001B[39mNOTIFY, Action\u001B[38;5;241m.\u001B[39mNOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[1;32m    170\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m--> 172\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:842\u001B[0m, in \u001B[0;36mLlamaForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001B[0m\n\u001B[1;32m    839\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m    841\u001B[0m \u001B[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001B[39;00m\n\u001B[0;32m--> 842\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    843\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    844\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    845\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    846\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    847\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    848\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    849\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    850\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    851\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    852\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    853\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    854\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    856\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    857\u001B[0m \u001B[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:594\u001B[0m, in \u001B[0;36mLlamaModel.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001B[0m\n\u001B[1;32m    582\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[1;32m    583\u001B[0m         decoder_layer\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[1;32m    584\u001B[0m         hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    591\u001B[0m         position_embeddings,\n\u001B[1;32m    592\u001B[0m     )\n\u001B[1;32m    593\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 594\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    595\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    596\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    597\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    598\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    599\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    600\u001B[0m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    601\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    602\u001B[0m \u001B[43m        \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    603\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mflash_attn_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    604\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    606\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    608\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "File \u001B[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:336\u001B[0m, in \u001B[0;36mLlamaDecoderLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001B[0m\n\u001B[1;32m    333\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_layernorm(hidden_states)\n\u001B[1;32m    335\u001B[0m \u001B[38;5;66;03m# Self Attention\u001B[39;00m\n\u001B[0;32m--> 336\u001B[0m hidden_states, self_attn_weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    337\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    338\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    339\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    340\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    341\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    343\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    345\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    346\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    347\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m hidden_states\n\u001B[1;32m    349\u001B[0m \u001B[38;5;66;03m# Fully Connected\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:275\u001B[0m, in \u001B[0;36mLlamaAttention.forward\u001B[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001B[0m\n\u001B[1;32m    272\u001B[0m value_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mv_proj(hidden_states)\u001B[38;5;241m.\u001B[39mview(hidden_shape)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    274\u001B[0m cos, sin \u001B[38;5;241m=\u001B[39m position_embeddings\n\u001B[0;32m--> 275\u001B[0m query_states, key_states \u001B[38;5;241m=\u001B[39m \u001B[43mapply_rotary_pos_emb\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msin\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;66;03m# sin and cos are specific to RoPE models; cache_position needed for the static cache\u001B[39;00m\n\u001B[1;32m    279\u001B[0m     cache_kwargs \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msin\u001B[39m\u001B[38;5;124m\"\u001B[39m: sin, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcos\u001B[39m\u001B[38;5;124m\"\u001B[39m: cos, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcache_position\u001B[39m\u001B[38;5;124m\"\u001B[39m: cache_position}\n",
      "File \u001B[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:174\u001B[0m, in \u001B[0;36mapply_rotary_pos_emb\u001B[0;34m(q, k, cos, sin, position_ids, unsqueeze_dim)\u001B[0m\n\u001B[1;32m    172\u001B[0m sin \u001B[38;5;241m=\u001B[39m sin\u001B[38;5;241m.\u001B[39munsqueeze(unsqueeze_dim)\n\u001B[1;32m    173\u001B[0m q_embed \u001B[38;5;241m=\u001B[39m (q \u001B[38;5;241m*\u001B[39m cos) \u001B[38;5;241m+\u001B[39m (rotate_half(q) \u001B[38;5;241m*\u001B[39m sin)\n\u001B[0;32m--> 174\u001B[0m k_embed \u001B[38;5;241m=\u001B[39m (k \u001B[38;5;241m*\u001B[39m cos) \u001B[38;5;241m+\u001B[39m (\u001B[43mrotate_half\u001B[49m\u001B[43m(\u001B[49m\u001B[43mk\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m*\u001B[39m sin)\n\u001B[1;32m    175\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m q_embed, k_embed\n",
      "File \u001B[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:144\u001B[0m, in \u001B[0;36mrotate_half\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m    139\u001B[0m         sin \u001B[38;5;241m=\u001B[39m sin \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattention_scaling\n\u001B[1;32m    141\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m cos\u001B[38;5;241m.\u001B[39mto(dtype\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdtype), sin\u001B[38;5;241m.\u001B[39mto(dtype\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdtype)\n\u001B[0;32m--> 144\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mrotate_half\u001B[39m(x):\n\u001B[1;32m    145\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Rotates half the hidden dims of the input.\"\"\"\u001B[39;00m\n\u001B[1;32m    146\u001B[0m     x1 \u001B[38;5;241m=\u001B[39m x[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, : x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e5aa54642a46f9a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Lets apply lora",
   "id": "c0d02a80c9916e1b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T02:32:50.082081Z",
     "start_time": "2025-05-12T02:32:48.818602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "model_lora = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16).to(device)"
   ],
   "id": "e2ab4b826c61503a",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T02:32:51.158736Z",
     "start_time": "2025-05-12T02:32:51.153164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LoraLinear(torch.nn.Module):\n",
    "    def __init__(self, linear_layer, alpha = 1, r = 1, device = \"cuda\"):\n",
    "        super().__init__()\n",
    "        self.linear_layer = linear_layer.to(torch.float32) # Se cambia el tipo de la capa a float32 para evitar errores durante el entrenamiento\n",
    "        self.r = r\n",
    "        fan_in = self.linear_layer.in_features\n",
    "        fan_out = self.linear_layer.out_features\n",
    "        self.lora_A = torch.nn.Parameter(torch.zeros((fan_in, r), device=device))\n",
    "        self.lora_B = torch.nn.Parameter(torch.zeros((r, fan_out), device=device))\n",
    "        torch.nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        self.linear_layer.weight.requires_grad = False\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        self.training = mode\n",
    "        if not mode:\n",
    "            self.merged_weight = (self.linear_layer.weight.transpose(0,1) + self.lora_A @ self.lora_B).to(torch.float16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            x = x.to(torch.float32)\n",
    "            output = self.linear_layer(x)\n",
    "            output += x @ self.lora_A @ self.lora_B\n",
    "            output = output.to(torch.float16)\n",
    "        else:\n",
    "            output = x @ self.merged_weight\n",
    "        return output"
   ],
   "id": "b8159710a8521623",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T02:37:28.464391Z",
     "start_time": "2025-05-12T02:37:28.453260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoints_folder = \"checkpoints\"\n",
    "def train(\n",
    "    model,\n",
    "    data_collator,\n",
    "    train_dataset,\n",
    "    eval_dataset,\n",
    "    num_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    batch_size=8,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    print(\"Iniciando entrenamiento...\")\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    model.to(device)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for layer in model.model.layers:\n",
    "        if hasattr(layer, 'self_attn'):\n",
    "            layer.self_attn.q_proj = LoraLinear(layer.self_attn.q_proj, r=16)\n",
    "            layer.self_attn.k_proj = LoraLinear(layer.self_attn.k_proj, r=16)\n",
    "            layer.self_attn.v_proj = LoraLinear(layer.self_attn.v_proj, r=16)\n",
    "            layer.self_attn.o_proj = LoraLinear(layer.self_attn.o_proj, r=16)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        print(f\"\\n--- Época {epoch + 1}/{num_epochs} ---\")\n",
    "\n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            x = batch\n",
    "            print(x)\n",
    "            x = {k: v.to(device) for k, v in x.items()}\n",
    "            outputs = model(**x)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_train_loss += loss.item()\n",
    "            progress_bar.update(1)\n",
    "            progress_bar.set_description(f\"Época {epoch + 1}, Batch {batch_idx + 1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        print(f\"Fin de Época {epoch + 1}: Pérdida de Entrenamiento Promedio = {avg_train_loss:.4f}\")\n",
    "\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        print(f\"\\nEvaluando al final de la época {epoch + 1}...\")\n",
    "        with torch.no_grad():\n",
    "            for eval_batch in tqdm(eval_dataloader, desc=\"Evaluación\"):\n",
    "                eval_batch = {k: v.to(device) for k, v in eval_batch.items()}\n",
    "                outputs = model(**eval_batch)\n",
    "                total_eval_loss += outputs.loss.item()\n",
    "        avg_eval_loss = total_eval_loss / len(eval_dataloader)\n",
    "        print(f\"Fin de Época {epoch + 1}: Pérdida de Validación Promedio = {avg_eval_loss:.4f}\")\n",
    "        # Guardar el modelo\n",
    "        model.save_pretrained(f\"{checkpoints_folder}/lora_model_epoch_{epoch + 1}.ckpt\")\n",
    "\n",
    "    progress_bar.close()\n",
    "    print(\"Entrenamiento completado.\")"
   ],
   "id": "2fad4b44ba5398ef",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T02:32:53.364395Z",
     "start_time": "2025-05-12T02:32:53.262970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset_tokenized = TokenizedDataset(train_dataset, tokenizer, max_length=MAX_LENGTH)\n",
    "test_dataset_tokenized = TokenizedDataset(test_dataset, tokenizer, max_length=MAX_LENGTH)\n",
    "train(\n",
    "    model_lora, data_collator, train_dataset_tokenized, test_dataset_tokenized,\n",
    "    num_epochs=2, batch_size=8, learning_rate=5e-5,\n",
    "    device=device\n",
    ")"
   ],
   "id": "2f5441046e225f75",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/202 [00:00<?, ?it/s]\u001B[A\u001B[A\u001B[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Época 1/2 ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'offset_mapping'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[23], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m train_dataset_tokenized \u001B[38;5;241m=\u001B[39m TokenizedDataset(train_dataset, tokenizer, max_length\u001B[38;5;241m=\u001B[39mMAX_LENGTH)\n\u001B[1;32m      2\u001B[0m test_dataset_tokenized \u001B[38;5;241m=\u001B[39m TokenizedDataset(test_dataset, tokenizer, max_length\u001B[38;5;241m=\u001B[39mMAX_LENGTH)\n\u001B[0;32m----> 3\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_lora\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_collator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataset_tokenized\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_dataset_tokenized\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5e-5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[22], line 35\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, data_collator, train_dataset, eval_dataset, num_epochs, learning_rate, batch_size, device)\u001B[0m\n\u001B[1;32m     32\u001B[0m total_train_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m--- Época \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnum_epochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ---\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 35\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, batch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_dataloader):\n\u001B[1;32m     36\u001B[0m     x \u001B[38;5;241m=\u001B[39m batch\n\u001B[1;32m     37\u001B[0m     x \u001B[38;5;241m=\u001B[39m {k: v\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m x\u001B[38;5;241m.\u001B[39mitems()}\n",
      "File \u001B[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 708\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    709\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    710\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    711\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[1;32m    712\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    713\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[1;32m    714\u001B[0m ):\n",
      "File \u001B[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    762\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    763\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 764\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    765\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    766\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[0;32mIn[15], line 85\u001B[0m, in \u001B[0;36mTokenizedDataset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     82\u001B[0m answer_token_char_start_idx \u001B[38;5;241m=\u001B[39m answer_start_char_idx \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnswer: \u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     84\u001B[0m answer_token_start_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 85\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (start_offset, end_offset) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[43mx_tokenized\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moffset_mapping\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m):\n\u001B[1;32m     87\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m start_offset \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m answer_token_char_start_idx \u001B[38;5;241m<\u001B[39m end_offset:\n\u001B[1;32m     88\u001B[0m         answer_token_start_index \u001B[38;5;241m=\u001B[39m i\n",
      "File \u001B[0;32m~/miniconda3/envs/main/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:271\u001B[0m, in \u001B[0;36mBatchEncoding.__getitem__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m    261\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;124;03mIf the key is a string, returns the value of the dict associated to `key` ('input_ids', 'attention_mask',\u001B[39;00m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;124;03metc.).\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    268\u001B[0m \u001B[38;5;124;03mwith the constraint of slice.\u001B[39;00m\n\u001B[1;32m    269\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    270\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(item, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m--> 271\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_encodings \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    273\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_encodings[item]\n",
      "\u001B[0;31mKeyError\u001B[0m: 'offset_mapping'"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "options_picker_lora = OptionsPicker(model_lora, tokenizer, options, device=device)",
   "id": "53e6be48669303f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e07b84e4bda87879"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
